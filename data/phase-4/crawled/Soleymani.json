[{"id": "b9464b492f6638035d25b42f32ff3d51cb6d1e30", "url": "https://www.semanticscholar.org/paper/MG-BERT%3A-Multi-Graph-Augmented-BERT-for-Masked-BehnamGhader-Zakerinia/b9464b492f6638035d25b42f32ff3d51cb6d1e30", "title": "MG-BERT: Multi-Graph Augmented BERT for Masked Language Modeling", "abstract": "Multi-Graph augmented BERT (MG-BERT) model that is based on BERT embeds tokens while taking advantage of a static multi-graph containing global word co-occurrences in the text corpus beside global real-world facts about words in knowledge graphs is proposed. Pre-trained models like Bidirectional Encoder Representations from Transformers (BERT), have recently made a big leap forward in Natural Language Processing (NLP) tasks. However, there are still some shortcomings in the Masked Language Modeling (MLM) task performed by these models. In this paper, we first introduce a multi-graph including different types of relations between words. Then, we propose Multi-Graph augmented BERT (MG-BERT) model that is based on BERT. MG-BERT embeds tokens while taking advantage of a static multi-graph containing global word co-occurrences in the text corpus beside global real-world facts about words in knowledge graphs. The proposed model also employs a dynamic sentence graph to capture local context effectively. Experimental results demonstrate that our model can considerably enhance the performance in the MLM task.", "publication_year": "2021", "authors": ["Parishad BehnamGhader", "Hossein Zakerinia", "Mahdieh Soleymani Baghshah"], "related_topics": ["Computer Science"], "references": ["5f994dc8cae24ca9d1ed629e517fcc652660ddde", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "31184789ef4c3084af930b1e0dede3215b4a9240", "d6a13d8d168936a8947101d76fe060704d2f26ec", "b36b2914f16c78b1bf88ee720342d893d8a9fc46", "56cafbac34f2bb3f6a9828cd228ff281b810d6bb", "cd8a9914d50b0ac63315872530274d158d6aff09", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "2cab7f5d64a427cb59fb21112fe8dc28fb753b56", "1fa9ed2bea208511ae698a967875e943049f16b6"], "references_count": 29, "citations_count": 0}, {"id": "b9e98f630e8eaf77ddcd0f80d1360b611ae61e70", "url": "https://www.semanticscholar.org/paper/Deep-Learning-Based-Proarrhythmia-Analysis-Using-Golgooni-Mirsadeghi/b9e98f630e8eaf77ddcd0f80d1360b611ae61e70", "title": "Deep Learning-Based Proarrhythmia Analysis Using Field Potentials Recorded From Human Pluripotent Stem Cells Derived Cardiomyocytes", "abstract": "A novel method for automated analysis of \u201cirregularity\u201d in an in vitro model of cardiotoxicity experiments is introduced, which may overcome the drawbacks of using predesigned features that restricts the classification performance to the comprehensiveness and the quality of the designed features. An early characterization of drug-induced cardiotoxicity may be possible by combining comprehensive in vitro proarrhythmia assay and deep learning techniques. We aimed to develop a method to automatically detect irregular beating rhythm of field potentials recorded from human pluripotent stem cells (hPSC) derived cardiomyocytes (hPSC-CM) by multi-electrode array (MEA) system. We included field potentials from 380 experiments, which were labeled as normal or arrhythmic by electrophysiology experts. Convolutional and recurrent neural networks (CNN and RNN) were employed for automatic classification of field potential recordings. A preparation phase was initially applied to split 60-s long recordings into a series of 5-s windows. Subsequently, the classification phase comprising of two main steps was designed and applied. The first step included the classification of 5-s windows by using a designated CNN. While, the results of 5-s window assessments were used as the input sequence to an RNN that aggregates these results in the second step. The output was then compared to electrophysiologist-level arrhythmia detection, resulting in 0.83 accuracy, 0.93 sensitivity, 0.70 specificity, and 0.80 precision. In summary, this paper introduces a novel method for automated analysis of \u201cirregularity\u201d in an in vitro model of cardiotoxicity experiments. Thus, our method may overcome the drawbacks of using predesigned features that restricts the classification performance to the comprehensiveness and the quality of the designed features. Furthermore, automated analysis may facilitate the quality control experiments through the procedure of drug development with respect to cardiotoxicity and avoid late drug attrition from market.", "publication_year": "2019", "authors": ["Zeinab Golgooni", "Sara Mirsadeghi", "Mahdieh Soleymani Baghshah", "Pedram Ataee", "Hossein Baharvand", "Sara Pahlavan", "Hamid R. Rabiee"], "related_topics": ["Biology"], "references": ["50afa4fa74b0475ca0264461c79f7bd42fcc494c", "307ff8f512098497e2c69b79c00fbb7b3cc9650e", "7a373d7dbd44ad99e5287f78b0e168e33498b44d", "04232aeef8343cfdf85fe9b5d0164ea186029ed1", "e6f337f871168ec891b3f0fc1b060005e8e4de01", "4ef11d0b2d5bd02eab3f8113601370fc7183cc30", "b2e9ab6f182579d75fa0a61d266252b258e61746", "04aecf353a9d854d4ce2b602a6d5920af7f07b2b", "89a125d1a89bcd0c18df6810786f92d27ee4e17f", "c3abc2d4b3cb86d6e3606f225c671fff3b334c9b"], "references_count": 46, "citations_count": 8}, {"id": "1eae26fe1ca566f17468080c3aecab1c3f9efb66", "url": "https://www.semanticscholar.org/paper/A-Deep-Learning-Framework-for-Viable-Tumor-Burden-Jahromi-Khani/1eae26fe1ca566f17468080c3aecab1c3f9efb66", "title": "A Deep Learning Framework for Viable Tumor Burden Estimation", "abstract": "This paper proposes a deep learning framework for the segmentation of whole and viable tumor areas of liver cancer from whole-slide images (WSIs) using Fast Segmentation Convolutional Neural Network (Fast-SCNN) as the network. Liver masses have become a common clinical challenge since they require to be defined and accurately categorized as neoplastic or nonneoplastic lesions. Hepatocellular carcinoma (HCC), the most common histologic type of primary liver malignancy, is a global health concern being the fifth most common cancer and the second cause of cancer mortality worldwide. Accurate diagnosis, which in some circumstances requires histopathology results, is necessary for appropriate management. Also, some tumor characteristics help in predicting tumor behavior and patient response to therapy. In this paper, we propose a deep learning framework for the segmentation of whole and viable tumor areas of liver cancer from whole-slide images (WSIs). To this end, we use Fast Segmentation Convolutional Neural Network (Fast-SCNN) as our network. We use the dataset from PAIP 2019 challenge. After data-augmentation on the training subset, we train the network with a multi-term loss function and SWA technique. Our model achieves 0.80 for the median of the Jaccard Index for the task of Viable Tumor Segmentation and 0.77 for the median of Weighted Absolute Accuracy for the task of Viable Tumor Burden Estimation on the whole-slide images of the test subset.", "publication_year": "2020", "authors": ["Seyed Alireza Fatemi Jahromi", "Ali Asghar Khani", "Hatef Otroshi Shahreza", "Mahdieh Soleymani Baghshah", "Hamid Behroozi"], "related_topics": ["Computer Science", "Medicine"], "references": ["d779b87172306c37c2c711512e84bc8112adf21e", "915adc7d9aacc46b6b8575f4a8be4b7cb4a1caf7", "769149c0dc0ed308eca8bc916f4326b2e2f57a1f", "21ba757bf394720e0b66b86e7638ae28742d6570", "47a0dd130fbf397c554cfcbfdedda121c017c4ca", "6048de9749a1f31ac70e5c30030ceb1dc5d3f2b0", "ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba", "188f8f6f70947215a9dfeebb0b577155e0d3d339", "ae1c89817a3a239e5344293138bdd80293983460", "57a892b9576baeba70277179712d5b09e19224b9"], "references_count": 62, "citations_count": 2}, {"id": "5a5f7d39433d68059e513b947a9fde62b5d4d3fe", "url": "https://www.semanticscholar.org/paper/An-attribute-learning-method-for-zero-shot-Yazdanian-Shojaee/5a5f7d39433d68059e513b947a9fde62b5d4d3fe", "title": "An attribute learning method for zero-shot recognition", "abstract": "Experimental results show that the learned attributes by the proposed attribute learning method can improve the accuracy of the state-of-the-art zero-shot learning methods. Recently, the problem of integrating side information about classes has emerged in the learning settings like zero-shot learning. Although using multiple sources of information about the input space has been investigated in the last decade and many multi-view and multi-modal learning methods have already been introduced, the attribute learning for classes (output space) is a new problem that has been attended in the last few years. In this paper, we propose an attribute learning method that can use different sources of descriptions for classes to find new attributes that are more proper to be used as class signatures. Experimental results show that the learned attributes by the proposed method can improve the accuracy of the state-of-the-art zero-shot learning methods.", "publication_year": "2017", "authors": ["Ramtin Yazdanian", "Seyed Mohsen Shojaee", "Mahdieh Soleymani Baghshah"], "related_topics": ["Computer Science"], "references": ["5fd80e47d53c64512a0b85a4c7a0beb24bc35766", "a6b8cd5f34b438f487679b1166ea03e56eb14c9e", "b29227f8dde62a5cd21678b4bc429206615485a2", "846946cd21413211a4701f309c3927d67363cd30", "ac98259064e86f643f2cd11e5417b43bf28daa91", "caa632d101a41a7860562e4399a5eaa9a4088b55", "755e9f43ce398ae8737366720c5f82685b0c253e", "244ae156ba2aaa91b2fa443c8ceb74ee13c6c6fa", "018e730f8947173e1140210d4d1760d05c9d3854", "6cd5fc1f0a63df570d3bccb33bf300791574e06f"], "references_count": 29, "citations_count": 0}, {"id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "url": "https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks. We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "publication_year": "2019", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "related_topics": ["Computer Science"], "references": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "8c1b00128e74f1cd92aede3959690615695d5101", "93b8da28d006415866bf48f9a6e06b5242129195", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "421fc2556836a6b441de806d7b393a35b6eaea58"], "references_count": 59, "citations_count": 53929}, {"id": "31184789ef4c3084af930b1e0dede3215b4a9240", "url": "https://www.semanticscholar.org/paper/KG-BERT%3A-BERT-for-Knowledge-Graph-Completion-Yao-Mao/31184789ef4c3084af930b1e0dede3215b4a9240", "title": "KG-BERT: BERT for Knowledge Graph Completion", "abstract": "This work treats triples in knowledge graphs as textual sequences and proposes a novel framework named Knowledge Graph Bidirectional Encoder Representations from Transformer (KG-BERT) to model these triples. Knowledge graphs are important resources for many artificial intelligence tasks but often suffer from incompleteness. In this work, we propose to use pre-trained language models for knowledge graph completion. We treat triples in knowledge graphs as textual sequences and propose a novel framework named Knowledge Graph Bidirectional Encoder Representations from Transformer (KG-BERT) to model these triples. Our method takes entity and relation descriptions of a triple as input and computes scoring function of the triple with the KG-BERT language model. Experimental results on multiple benchmark knowledge graphs show that our method can achieve state-of-the-art performance in triple classification, link prediction and relation prediction tasks.", "publication_year": "2019", "authors": ["Liang Yao", "Chengsheng Mao", "Yuan Luo"], "related_topics": ["Computer Science"], "references": ["cd8a9914d50b0ac63315872530274d158d6aff09", "96acb1c882ad655c6b8459c2cd331803801446ca", "cab46caf83a9e0390c6ca4d8603187969c9a53ad", "bd345877856dc83c2c10c125dbf0f41e2bde38b1", "17a1e5d78bffb17979ac55aa792698727fe25a21", "3ce14b7a3c1b89c717eba10229d9d80d80bd0e04", "e379f7c85441df5d8ddc1565cabf4b4290c22f1f", "67cab3bafc8fa9e1ae3ff89791ad43c81441d271", "aa1b05e8449eb5ee93b114453d9c946ae00459b1", "5f994dc8cae24ca9d1ed629e517fcc652660ddde"], "references_count": 44, "citations_count": 248}, {"id": "d6a13d8d168936a8947101d76fe060704d2f26ec", "url": "https://www.semanticscholar.org/paper/Barack%E2%80%99s-Wife-Hillary%3A-Using-Knowledge-Graphs-for-RobertL.Logan-Liu/d6a13d8d168936a8947101d76fe060704d2f26ec", "title": "Barack\u2019s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling", "abstract": "This work introduces the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context that enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model\u2019s ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts.", "publication_year": "2019", "authors": ["IV RobertL.Logan", "Nelson F. Liu", "Matthew E. Peters", "Matt Gardner", "Sameer Singh"], "related_topics": ["Computer Science"], "references": ["0fa5142f908afc94c923ca2adbe14a5673bc76eb", "8cb592fa5e30e6fa5abe7041767768964f1f8cf4", "9405cc0d6169988371b2755e573cc28650d14dfe", "26e9eb44ed8065122d37b0c429a8d341bfeea9a5", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "ef9ddbc35676ce8ffc2a8067044473727839dbac", "efbd381493bb9636f489b965a2034d529cd56bcd", "58c6f890a1ae372958b7decf56132fe258152722", "2927dfc481446568fc9108795570eb4d416be021", "604764133befe7a0aaa692919545846197e6e065"], "references_count": 29, "citations_count": 146}, {"id": "b36b2914f16c78b1bf88ee720342d893d8a9fc46", "url": "https://www.semanticscholar.org/paper/Learning-beyond-Datasets%3A-Knowledge-Graph-Augmented-Annervaz-Chowdhury/b36b2914f16c78b1bf88ee720342d893d8a9fc46", "title": "Learning beyond Datasets: Knowledge Graph Augmented Neural Networks for Natural Language Processing", "abstract": "This work proposes to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks by introducing a convolution-based model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. Machine Learning has been the quintessential solution for many AI problems, but learning models are heavily dependent on specific training data. Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand. In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks. Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism. We introduce a convolution-based model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task. Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) & DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset. We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.", "publication_year": "2018", "authors": ["K. M. Annervaz", "Somnath Basu Roy Chowdhury", "Ambedkar Dukkipati"], "related_topics": ["Computer Science"], "references": ["96acb1c882ad655c6b8459c2cd331803801446ca", "e3274206b36a603abc4a335af91273ecba5e73cc", "6fba4968f1b39d490bf95fe4030e3d385f167074", "033f25ad905ef2ed32a8331cf38b83953ff15922", "e379f7c85441df5d8ddc1565cabf4b4290c22f1f", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "79baf8cf6be6510f69be8c515516136138678cf5", "d77de3a4ddfa62f8105c0591fd41e549edcfd95f", "18bd7cd489874ed9976b4f87a6a558f9533316e0", "2582ab7c70c9e7fcb84545944eba8f3a7f253248"], "references_count": 40, "citations_count": 59}, {"id": "56cafbac34f2bb3f6a9828cd228ff281b810d6bb", "url": "https://www.semanticscholar.org/paper/KEPLER%3A-A-Unified-Model-for-Knowledge-Embedding-and-Wang-Gao/56cafbac34f2bb3f6a9828cd228ff281b810d6bb", "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation", "abstract": "A unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs is proposed. Abstract Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.", "publication_year": "2019", "authors": ["Xiaozhi Wang", "Tianyu Gao", "Zhaocheng Zhu", "Zhiyuan Liu", "Juan-Zi Li", "Jian Tang"], "related_topics": ["Computer Science"], "references": ["5f994dc8cae24ca9d1ed629e517fcc652660ddde", "06a73ad09664435f8b3cd90293f4e05a047cf375", "bfeb827d06c1a3583b5cc6d25241203a81f6af09", "96acb1c882ad655c6b8459c2cd331803801446ca", "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "70af3ee98c53441d9090119f7b76efb1b6d03edd", "6dd3b79f34a8b40320d1d745b9abf2d70e1d4db8", "d6a13d8d168936a8947101d76fe060704d2f26ec", "994afdf0db0cb0456f4f76468380822c2f532726", "f7b0d94fd4a32c4c9be472b4e8d6c5bc308f0dfa"], "references_count": 74, "citations_count": 317}, {"id": "cd8a9914d50b0ac63315872530274d158d6aff09", "url": "https://www.semanticscholar.org/paper/Modeling-Relational-Data-with-Graph-Convolutional-Schlichtkrull-Kipf/cd8a9914d50b0ac63315872530274d158d6aff09", "title": "Modeling Relational Data with Graph Convolutional Networks", "abstract": "It is shown that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline. Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.", "publication_year": "2017", "authors": ["M. Schlichtkrull", "Thomas Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling"], "related_topics": ["Computer Science"], "references": ["9697d32ed0a16da167f2bdba05ef96d0da066eb5", "822f1ed9a76a57cc19d8fda7745365b97130b97a", "af2e6165b68e75c911dfdb8f81f9ab6627722ab7", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "86412306b777ee35aba71d4795b02915cb8a04c3", "1ef01e7bfab2041bc0c0a56a57906964df9fc985", "e745b0506f4133263633eb05e5006a8cff4129f0", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "6b7d6e6416343b2a122f8416e69059ce919026ef", "97f7ef7a5332218e0e9ce75ad5cf77048466ca83"], "references_count": 54, "citations_count": 2958}, {"id": "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "url": "https://www.semanticscholar.org/paper/Translating-Embeddings-for-Modeling-Data-Bordes-Usunier/2582ab7c70c9e7fcb84545944eba8f3a7f253248", "title": "Translating Embeddings for Modeling Multi-relational Data", "abstract": "TransE is proposed, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities, which proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.", "publication_year": "2013", "authors": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garc{\\'i"], "related_topics": ["Computer Science"], "references": ["473b3f2cc2c942c0116d980fe5b36a338f6017de", "f6764d853a14b0c34df1d2283e76277aead40fde", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "834cb8e1e738b8d2c6d24e652ac966d6e7089a46", "8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092", "498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "1f4a4769e4d2fb846e59c2f185e0377190739f18", "fec691d09b564986ad27162ce15344604c840ff9", "81bbe42e3ec09c28b8864956148e58f4cb5aa860", "4e07791ee0872401215f12aefde342bd843240cc"], "references_count": 18, "citations_count": 5286}, {"id": "2cab7f5d64a427cb59fb21112fe8dc28fb753b56", "url": "https://www.semanticscholar.org/paper/Enriching-BERT-with-Knowledge-Graph-Embeddings-for-Ostendorff-Bourgonje/2cab7f5d64a427cb59fb21112fe8dc28fb753b56", "title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification", "abstract": "Building upon BERT, a deep neural language model, it is demonstrated how to combine text representations with metadata and knowledge graph embeddings, which encode author information. In this paper, we focus on the classification of books using short descriptive texts (cover blurbs) and additional metadata. Building upon BERT, a deep neural language model, we demonstrate how to combine text representations with metadata and knowledge graph embeddings, which encode author information. Compared to the standard BERT approach we achieve considerably better results for the classification task. For a more coarse-grained classification using eight labels we achieve an F1- score of 87.20, while a detailed classification using 343 labels yields an F1-score of 64.70. We make the source code and trained models of our experiments publicly available", "publication_year": "2019", "authors": ["Malte Ostendorff", "Peter Bourgonje", "Maria Berger", "Juli{\\'a"], "related_topics": ["Computer Science"], "references": ["1a9954d86466a7e4de6f98ddee452ceb50e15d86", "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "58203813610b866483ffc2bd1181f616ae38107c", "64c5f7055b2e6982b6b95e069b22230d13a134bb", "fc1d23d2f9167d13ef1bce098ef55d1b40894dd4", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "455afd748e8834ef521e4b67c7c056d3c33429e2", "160563abbd75265b19afc8b4169bab9e1eb33d97", "7ac58400e5063bed9b7c35f87e44ddb917ccf357", "031e4e43aaffd7a479738dcea69a2d5be7957aa3"], "references_count": 18, "citations_count": 62}, {"id": "1fa9ed2bea208511ae698a967875e943049f16b6", "url": "https://www.semanticscholar.org/paper/HuggingFace's-Transformers%3A-State-of-the-art-Wolf-Debut/1fa9ed2bea208511ae698a967875e943049f16b6", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "abstract": "The \\\\textit{Transformers} library is an open-source library that consists of carefully engineered state-of-the art Transformer architectures under a unified API and a curated collection of pretrained models made by and available for the community. Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\\\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\\\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\\\url{this https URL}.", "publication_year": "2019", "authors": ["Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R{\\'e"], "related_topics": ["Computer Science"], "references": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992", "93b4cc549a1bc4bc112189da36c318193d05d806", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "a54b56af24bb4873ed0163b77df63b92bd018ddc", "7a064df1aeada7e69e5173f7d4c8606f4470365b", "d9f6ada77448664b71128bb19df15765336974a6", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "327d7e55d64cb34d55bd3a3fe58233c238a312cd", "157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3"], "references_count": 78, "citations_count": 3838}, {"id": "50afa4fa74b0475ca0264461c79f7bd42fcc494c", "url": "https://www.semanticscholar.org/paper/In-vitro-electrophysiological-drug-testing-using-Caspi-Itzhaki/50afa4fa74b0475ca0264461c79f7bd42fcc494c", "title": "In vitro electrophysiological drug testing using human embryonic stem cell derived cardiomyocytes.", "abstract": "It is hypothesized that human embryonic stem cell-derived cardiomyocytes assessed with a combination of single cell electrophysiology and microelectrode array (MEA) mapping can serve as a novel model for Electrophysiological drug screening. Pro-arrhythmia (development of cardiac arrhythmias as a pharmacological side effect) has become the single most common cause of the withdrawal or restrictions of previously marketed drugs. The development of new medications, free from these side effects, is hampered by the lack of an in vitro assay for human cardiac tissue. We hypothesized that human embryonic stem cell-derived cardiomyocytes (hESC-CMs) assessed with a combination of single cell electrophysiology and microelectrode array (MEA) mapping can serve as a novel model for electrophysiological drug screening. Current-clamp studies revealed that E-4031 and Sotalol (IKr blockers) significantly increased hESC-CM's action potential duration and also induced after-depolarizations (the in vitro correlates of increased arrhythmogenic potential). Multicellular aggregates of hESC-CMs were then analyzed with the MEA technique. Application of class I (Quinidine, Procaineamide) and class III (Sotalol) antiarrhythmic agents, E-4031, and Cisapride (a noncardiogenic agent known to lengthen QT) resulted in dose-dependent prolongation of the corrected field potential duration (cFPD). We next utilized the MEA technique to also assess pharmacological effects on conduction. Activation maps demonstrated significant conduction slowing following administration of Na channel blockers (Quinidine and Propafenone) and of the gap junction blocker (1-heptanol). While most attention has been focused on the prospects of using hESC-derived cardiomyocytes for regenerative medicine, this study highlights the possible utilization of these unique cells also for cardiac electrophysiological studies, drug screening, and target validation.", "publication_year": "2009", "authors": ["Oren Caspi", "Ilanit Itzhaki", "Izhak Kehat", "Amira Gepstein", "Gil Arbel", "Irit Huber", "Jonathan Satin", "Lior Gepstein"], "related_topics": ["Biology", "Medicine"], "references": ["2269e63b969cb7b62d51c86f2c58ec645dad2303", "0e01651f92b8aa685df569856997f2ddff12c1e5", "65b5a7936849b6f1278cdf6b5b9eaf9e126799c4", "cf06c03542b1407e3da9d8fdb8ce7ace9a8ba07c", "00edea7e3c0f45492baea5ffa637b311857a6383", "997aeac7640fdf10adc7486a02173d144a767aee", "bc0e497fd8d8ed4ecf91075373af29cf0b049d4f", "84d07718fa5452e3899d6070d17b4bbdcb87bd61", "ad421e55153d9e844c2be6db43d2a450076c3b73", "7b6a64f775cb23f200581bd7ec087bf31e3e104a"], "references_count": 58, "citations_count": 236}, {"id": "307ff8f512098497e2c69b79c00fbb7b3cc9650e", "url": "https://www.semanticscholar.org/paper/Estimating-the-risk-of-drug-induced-proarrhythmia-Guo-Abrams/307ff8f512098497e2c69b79c00fbb7b3cc9650e", "title": "Estimating the risk of drug-induced proarrhythmia using human induced pluripotent stem cell-derived cardiomyocytes.", "abstract": "The first published report of a high-throughput functional assay employing a monolayer of beating human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) is described, detailing a model that accurately detects drug-induced cardiac abnormalities. Improved in vitro systems for predicting drug-induced toxicity are needed in the pharmaceutical and biotechnology industries to decrease late-stage drug attrition. One unmet need is an early screen for cardiotoxicity, which accounts for about one third of safety-based withdrawn pharmaceuticals. Herein, the first published report of a high-throughput functional assay employing a monolayer of beating human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) is described, detailing a model that accurately detects drug-induced cardiac abnormalities. Using 96-well plates with interdigitated electrode arrays that assess impedance, the rhythmic, synchronous contractions of the iPSC-CMs were detected. Treatment of the iPSC-CMs with 28 different compounds with known cardiac effects resulted in compound-specific changes in the beat rate and/or the amplitude of the impedance measurement. Changes in impedance for the compounds tested were comparable with the results from a related technology, electric field potential assessment obtained from microelectrode arrays. Using the results from the set of compounds, an index of drug-induced arrhythmias was calculated, enabling the determination of a drug's proarrhythmic potential. This system of interrogating human cardiac function in vitro opens new opportunities for predicting cardiac toxicity and studying cardiac biology.", "publication_year": "2011", "authors": ["Liang Guo", "Rory M. C. Abrams", "Joshua Babiarz", "Jennifer D. Cohen", "Seiji Kameoka", "Martin J. Sanders", "Eric T Chiao", "Kyle L Kolaja"], "related_topics": ["Biology", "Medicine"], "references": ["0e01651f92b8aa685df569856997f2ddff12c1e5", "65b5a7936849b6f1278cdf6b5b9eaf9e126799c4", "c1cb043d6cc44f9c661a45c1daba623d41b36bf1", "2b552d462e7cc16f40d1e8ef2765c9e81f8c54db", "22a81a3583b6993c46a6e006a684529668239b89", "bd1aafb7364cf9ba364260f8384677820d160dbf", "cf313169a9639e7b837404546589aeb24224e863", "cca805487ff41c849517930de4e53c00f2116213", "dd9b8885d1f5dca9cb9f80060dac1352642f260f", "6aca4840e2643a1e5ffb269599cfbd3255810b33"], "references_count": 43, "citations_count": 270}, {"id": "7a373d7dbd44ad99e5287f78b0e168e33498b44d", "url": "https://www.semanticscholar.org/paper/Interpretation-of-field-potentials-measured-on-a-in-Tertoolen-Braam/7a373d7dbd44ad99e5287f78b0e168e33498b44d", "title": "Interpretation of field potentials measured on a multi electrode array in pharmacological toxicity screening on primary and human pluripotent stem cell-derived cardiomyocytes", "abstract": "Semantic Scholar extracted view of \\\"Interpretation of field potentials measured on a multi electrode array in pharmacological toxicity screening on primary and human pluripotent stem cell-derived cardiomyocytes\\\" by L. Tertoolen et al.", "publication_year": "2017", "authors": ["Leon G. J. Tertoolen", "Stefan R. Braam", "Berend J van Meer", "Robert Passier", "Christine L. Mummery"], "related_topics": ["Biology"], "references": ["0e01651f92b8aa685df569856997f2ddff12c1e5", "88499b49a830a39dd16381e224d8880e47d868ce", "bf613c2aa1d514e099cff15eeb751653321cbcb6", "04b17d30b1cee5eba210e40150e80a51b196dc7a", "98fb5773e149e1ef59e9612098209d2ac44cdd8a", "f8317fe10a44714ed9c1d6df54c50bd075896f01", "74062c98a4c16981407a0c181abadc85dd03eb97", "0ed904b68234575f6109403eb7a228da2f1b0faa", "9e4ba0fc9b6103fdd5509e8cdf370971648a6e2f", "de63e783210fe097f6164e66aa125ed014d44dff"], "references_count": 13, "citations_count": 54}, {"id": "04232aeef8343cfdf85fe9b5d0164ea186029ed1", "url": "https://www.semanticscholar.org/paper/Electrophysiological-Analysis-of-human-Pluripotent-Sala-Oostwaard/04232aeef8343cfdf85fe9b5d0164ea186029ed1", "title": "Electrophysiological Analysis of human Pluripotent Stem Cell-derived Cardiomyocytes (hPSC-CMs) Using Multi-electrode Arrays (MEAs)", "abstract": "This protocol describes how to dissociate 2D cell cultures of hPSC-CMs to small aggregates and single cells and plate them on MEAs to record their spontaneous electrical activity as field potential and methods for analyzing the recorded data to extract specific parameters, such as the QT and the RR intervals. Cardiomyocytes can now be derived with high efficiency from both human embryonic and human induced-Pluripotent Stem Cells (hPSC). hPSC-derived cardiomyocytes (hPSC-CMs) are increasingly recognized as having great value for modeling cardiovascular diseases in humans, especially arrhythmia syndromes. They have also demonstrated relevance as in vitro systems for predicting drug responses, which makes them potentially useful for drug-screening and discovery, safety pharmacology and perhaps eventually for personalized medicine. This would be facilitated by deriving hPSC-CMs from patients or susceptible individuals as hiPSCs. For all applications, however, precise measurement and analysis of hPSC-CM electrical properties are essential for identifying changes due to cardiac ion channel mutations and/or drugs that target ion channels and can cause sudden cardiac death. Compared with manual patch-clamp, multi-electrode array (MEA) devices offer the advantage of allowing medium- to high-throughput recordings. This protocol describes how to dissociate 2D cell cultures of hPSC-CMs to small aggregates and single cells and plate them on MEAs to record their spontaneous electrical activity as field potential. Methods for analyzing the recorded data to extract specific parameters, such as the QT and the RR intervals, are also described here. Changes in these parameters would be expected in hPSC-CMs carrying mutations responsible for cardiac arrhythmias and following addition of specific drugs, allowing detection of those that carry a cardiotoxic risk.", "publication_year": "2017", "authors": ["Luca Sala", "Dorien Ward-van Oostwaard", "Leon G. J. Tertoolen", "Christine L. Mummery", "Milena Bellin"], "related_topics": ["Biology"], "references": ["8fdcc7c7765f06a1836db3647aec32fddbed0b7f", "878ba1facdba4e27058a664860fdaee5b02a4ff0", "8b65176229dde32cd4c1cf2b9d04807f0dab29e8", "5d3185baf131f5f08e55667a475d15abb629efda", "184177e938c9176f5fee59eefa2ab959dffa5351", "7a373d7dbd44ad99e5287f78b0e168e33498b44d", "127615ea4a38e98be0bdcc8b76a326c338f30535", "e57d812c931e2f2d775f9b472c3b61c895bddbaf", "cbfac1f03e8a04ec04ac2d5368ece85676edf5e9", "95183f57aea770611a0847ad2f73c7c85a48269d"], "references_count": 63, "citations_count": 34}, {"id": "e6f337f871168ec891b3f0fc1b060005e8e4de01", "url": "https://www.semanticscholar.org/paper/Screening-Drug-Induced-Arrhythmia-Using-Human-Stem-Navarrete-Liang/e6f337f871168ec891b3f0fc1b060005e8e4de01", "title": "Screening Drug-Induced Arrhythmia Using Human Induced Pluripotent Stem Cell\u2013Derived Cardiomyocytes and Low-Impedance Microelectrode Arrays", "abstract": "The data indicate that the MEA/hiPSC-CM assay is a sensitive, robust, and efficient platform for testing drug effectiveness and for arrhythmia screening and may provide significant advantages over current industry standard assays that use immortalized cell lines or animal models. Background\u2014 Drug-induced arrhythmia is one of the most common causes of drug development failure and withdrawal from market. This study tested whether human induced pluripotent stem cell\u2013derived cardiomyocytes (hiPSC-CMs) combined with a low-impedance microelectrode array (MEA) system could improve on industry-standard preclinical cardiotoxicity screening methods, identify the effects of well-characterized drugs, and elucidate underlying risk factors for drug-induced arrhythmia. hiPSC-CMs may be advantageous over immortalized cell lines because they possess similar functional characteristics as primary human cardiomyocytes and can be generated in unlimited quantities. Methods and Results\u2014 Pharmacological responses of beating embryoid bodies exposed to a comprehensive panel of drugs at 65 to 95 days postinduction were determined. Responses of hiPSC-CMs to drugs were qualitatively and quantitatively consistent with the reported drug effects in literature. Torsadogenic hERG blockers, such as sotalol and quinidine, produced statistically and physiologically significant effects, consistent with patch-clamp studies, on human embryonic stem cell\u2013derived cardiomyocytes hESC-CMs. False-negative and false-positive hERG blockers were identified accurately. Consistent with published studies using animal models, early afterdepolarizations and ectopic beats were observed in 33% and 40% of embryoid bodies treated with sotalol and quinidine, respectively, compared with negligible early afterdepolarizations and ectopic beats in untreated controls. Conclusions\u2014 We found that drug-induced arrhythmias can be recapitulated in hiPSC-CMs and documented with low impedance MEA. Our data indicate that the MEA/hiPSC-CM assay is a sensitive, robust, and efficient platform for testing drug effectiveness and for arrhythmia screening. This system may hold great potential for reducing drug development costs and may provide significant advantages over current industry standard assays that use immortalized cell lines or animal models.", "publication_year": "2013", "authors": ["Enrique G. Navarrete", "Ping Liang", "Feng Lan", "Veronica Sanchez-Freire", "Chelsey S. Simmons", "Tingyu Gong", "Arun Sharma", "Paul W. Burridge", "Bhagat Patlolla", "Andrew Stephen Lee", "Haodi Wu", "Ramin E. Beygui", "Sean M. Wu", "Robert C. Robbins", "Donald M. Bers", "Joseph C. Wu"], "related_topics": ["Biology", "Medicine"], "references": ["50afa4fa74b0475ca0264461c79f7bd42fcc494c", "307ff8f512098497e2c69b79c00fbb7b3cc9650e", "a610242d1e177bb04a3d6e35b5d34afd8648bfb0", "878ba1facdba4e27058a664860fdaee5b02a4ff0", "70d2bd197d72935a93bc486a9345992051f6ad69", "0e01651f92b8aa685df569856997f2ddff12c1e5", "d1c166e8edf5e672ccc08dc65b1979aaeaafa775", "88499b49a830a39dd16381e224d8880e47d868ce", "de9817a662077f9f9b3b77107590685584bd89e0", "c1cb043d6cc44f9c661a45c1daba623d41b36bf1"], "references_count": 74, "citations_count": 279}, {"id": "4ef11d0b2d5bd02eab3f8113601370fc7183cc30", "url": "https://www.semanticscholar.org/paper/Cardiomyocyte-MEA-Data-Analysis-(CardioMDA)-%E2%80%93-A-for-Pradhapan-Kuusela/4ef11d0b2d5bd02eab3f8113601370fc7183cc30", "title": "Cardiomyocyte MEA Data Analysis (CardioMDA) \u2013 A Novel Field Potential Data Analysis Software for Pluripotent Stem Cell Derived Cardiomyocytes", "abstract": "An offline, semi-automatic data analysis software equipped with correlation analysis and ensemble averaging techniques to improve the accuracy, reliability and throughput rate of analysing human pluripotent stem cell derived cardiomyocyte field potentials, and will facilitate the analysis of CM MEA signals in semi-automated way. Cardiac safety pharmacology requires in-vitro testing of all drug candidates before clinical trials in order to ensure they are screened for cardio-toxic effects which may result in severe arrhythmias. Micro-electrode arrays (MEA) serve as a complement to current in-vitro methods for drug safety testing. However, MEA recordings produce huge volumes of data and manual analysis forms a bottleneck for high-throughput screening. To overcome this issue, we have developed an offline, semi-automatic data analysis software, \u2018Cardiomyocyte MEA Data Analysis (CardioMDA)\u2019, equipped with correlation analysis and ensemble averaging techniques to improve the accuracy, reliability and throughput rate of analysing human pluripotent stem cell derived cardiomyocyte (CM) field potentials. With the program, true field potential and arrhythmogenic complexes can be distinguished from one another. The averaged field potential complexes, analysed using our software to determine the field potential duration, were compared with the analogous values obtained from manual analysis. The reliability of the correlation analysis algorithm, evaluated using various arrhythmogenic and morphology changing signals, revealed a mean sensitivity and specificity of 99.27% and 94.49% respectively, in determining true field potential complexes. The field potential duration of the averaged waveforms corresponded well to the manually analysed data, thus demonstrating the reliability of the software. The software has also the capability to create overlay plots for signals recorded under different drug concentrations in order to visualize and compare the magnitude of response on different ion channels as a result of drug treatment. Our novel field potential analysis platform will facilitate the analysis of CM MEA signals in semi-automated way and provide a reliable means of efficient and swift analysis for cardiomyocyte drug or disease model studies.", "publication_year": "2013", "authors": ["Paruthi Pradhapan", "Jukka Kuusela", "Jari Viik", "Katriina Aalto-Set{\\\"a"], "related_topics": ["Biology"], "references": ["50afa4fa74b0475ca0264461c79f7bd42fcc494c", "42c550a012171047707cd12afb10ef6d2c3e661c", "0e01651f92b8aa685df569856997f2ddff12c1e5", "2269e63b969cb7b62d51c86f2c58ec645dad2303", "09c77cd329a36c22f62d6bc3204499e6a9794c26", "881744b7a097ab9804d0dd3d1456a0c9e0337759", "63f4067d18d872fa045d3b2b7ba1fa251bb01dcf", "b4427af5b7f8d0d230a1a3f3c519fca87e896d6b", "e0ba942b9ccdb5448da8738ae005fe7c3fd05951", "a80930d3b42106fd4bbe5715e3e95155a97b0fd8"], "references_count": 53, "citations_count": 31}, {"id": "b2e9ab6f182579d75fa0a61d266252b258e61746", "url": "https://www.semanticscholar.org/paper/Effects-of-cardioactive-drugs-on-human-induced-stem-Kuusela-Kujala/b2e9ab6f182579d75fa0a61d266252b258e61746", "title": "Effects of cardioactive drugs on human induced pluripotent stem cell derived long QT syndrome cardiomyocytes", "abstract": "The drug effects on these patient-specific cardiomyocytes appear to recapitulate clinical observations and provide further evidence that these cells can be applied for in vitro drug testing to probe their vulnerability to arrhythmia. Human induced pluripotent stem cells (hiPSC) have enabled a major step forward in pathophysiologic studies of inherited diseases and may also prove to be valuable in in vitro drug testing. Long QT syndrome (LQTS), characterized by prolonged cardiac repolarization and risk of sudden death, may be inherited or result from adverse drug effects. Using a microelectrode array platform, we investigated the effects of six different drugs on the electrophysiological characteristics of human embryonic stem cell-derived cardiomyocytes as well as hiPSC-derived cardiomyocytes from control subjects and from patients with type 1 (LQT1) and type 2 (LQT2) of LQTS. At baseline the repolarization time was significantly longer in LQTS cells compared to controls. Isoprenaline increased the beating rate of all cell lines by 10\u201373\u00a0% but did not show any arrhythmic effects in any cell type. Different QT-interval prolonging drugs caused prolongation of cardiac repolarization by 3\u201313\u00a0% (cisapride), 10\u201320\u00a0% (erythromycin), 8\u201323\u00a0% (sotalol), 16\u201342\u00a0% (quinidine) and 12\u201327\u00a0% (E-4031), but we did not find any systematic differences in sensitivity between the control, LQT1 and LQT2 cell lines. Sotalol, quinidine and E-4031 also caused arrhythmic beats and beating arrests in some cases. In summary, the drug effects on these patient-specific cardiomyocytes appear to recapitulate clinical observations and provide further evidence that these cells can be applied for in vitro drug testing to probe their vulnerability to arrhythmia.", "publication_year": "2016", "authors": ["Jukka Kuusela", "Ville J. Kujala", "Annamari Kiviaho", "Marisa Ojala", "Heikki Swan", "Kimmo K. Kontula", "Katriina Aalto-Set{\\\"a"], "related_topics": ["Biology", "Medicine"], "references": ["d1c166e8edf5e672ccc08dc65b1979aaeaafa775", "50afa4fa74b0475ca0264461c79f7bd42fcc494c", "70d2bd197d72935a93bc486a9345992051f6ad69", "0e01651f92b8aa685df569856997f2ddff12c1e5", "8b5f6a39faf2b5586012d9fccb19c0f43120b642", "a80930d3b42106fd4bbe5715e3e95155a97b0fd8", "09f4b7dd63927fcfadd5fec2cc9de065ae83d911", "2fd46267aaab85b200d3a95fb32539a76b19255b", "c1b24375b2292fb40e0a444803183e19cfbe6347", "e6f337f871168ec891b3f0fc1b060005e8e4de01"], "references_count": 47, "citations_count": 25}, {"id": "04aecf353a9d854d4ce2b602a6d5920af7f07b2b", "url": "https://www.semanticscholar.org/paper/High-throughput-multi-parameter-profiling-of-drug-Clements-Thomas/04aecf353a9d854d4ce2b602a6d5920af7f07b2b", "title": "High-throughput multi-parameter profiling of electrophysiological drug effects in human embryonic stem cell derived cardiomyocytes using multi-electrode arrays.", "abstract": "This study is the first to apply multi-parameter phenotypic profiling and clustering techniques commonly used for high-content imaging and microarray data to the analysis of electrophysiology data obtained by multi-electrode array (MEA) analysis of hESC-CM. Human stem cell derived cardiomyocytes (hESC-CM) provide a potential model for development of improved assays for pre-clinical predictive drug safety screening. We have used multi-electrode array (MEA) analysis of hESC-CM to generate multi-parameter data to profile drug impact on cardiomyocyte electrophysiology using a panel of 21 compounds active against key cardiac ion channels. Our study is the first to apply multi-parameter phenotypic profiling and clustering techniques commonly used for high-content imaging and microarray data to the analysis of electrophysiology data obtained by MEA analysis. Our data show good correlations with previous studies in stem cell derived cardiomyocytes and demonstrate improved specificity in compound risk assignment over convention single-parametric approaches. These analyses indicate great potential for multi-parameter MEA data acquired from hESC-CM to enable drug electrophysiological liabilities to be assessed in pre-clinical cardiotoxicity assays, facilitating informed decision making and liability management at the optimum point in drug development.", "publication_year": "2014", "authors": ["Mike Clements", "Nick Thomas"], "related_topics": ["Biology"], "references": ["50afa4fa74b0475ca0264461c79f7bd42fcc494c", "307ff8f512098497e2c69b79c00fbb7b3cc9650e", "b704c18b49757a3f43ebc7f29c6bbe27702dcfaa", "e0ba942b9ccdb5448da8738ae005fe7c3fd05951", "0e01651f92b8aa685df569856997f2ddff12c1e5", "cf79baec2b4441d2d87d473d18697b17c452c0ec", "e5a18de385cb61dd3cd5f98fffe48a0f17a1cfb1", "3a5d9f656bbed7d7a9e9930626a78e02c0fa29f3", "89a125d1a89bcd0c18df6810786f92d27ee4e17f", "6ccf3818d0a2380430f85c514703705832a9752e"], "references_count": 33, "citations_count": 132}, {"id": "89a125d1a89bcd0c18df6810786f92d27ee4e17f", "url": "https://www.semanticscholar.org/paper/Refining-the-human-iPSC-cardiomyocyte-arrhythmic-Guo-Coyle/89a125d1a89bcd0c18df6810786f92d27ee4e17f", "title": "Refining the human iPSC-cardiomyocyte arrhythmic risk assessment model.", "abstract": "This hCAR assay showed increased performance over existing preclinical tools in predicting clinical QT prolongation, arrhythmia, and TdP, and hiPS-CMs are a relevant cell system to improve evaluating cardiac safety liabilities of drug candidates. Human induced pluripotent stem cell-derived cardiomyocytes (hiPS-CMs) are capable of detecting drug-induced clinical arrhythmia, Torsade de Pointes (TdP), and QT prolongation. Efforts herein employ a broad set of structurally diverse drugs to optimize the predictive algorithm for applications in discovery toxicology and cardiac safety screening. The changes in the beat rhythm and rate of a confluent monolayer of hiPS-CMs by 88 marketed and 30 internal discovery compounds were detected with real-time cellular impedance measurement and quantified by measures of arrhythmic beating (IB20, lowest concentration inducing \u2265 20% arrhythmic [irregular, atypical] beats in 3 consecutive 20-s sweeps, and predicted proarrhythmic score [PPS]-IB20) or changes in beat rate (BR20, the lowest concentration inducing a reduction in beat rate of \u2265 20% at 3 consecutive sweeps compared with the time-matched vehicle control group, and PPS-BR20). Drug-induced arrhythmic beats and reductions in beat rates are predictive of clinical arrhythmia and QT prolongation, respectively. A threshold of \u2264 10 \u03bcM for class determination results in 82% in vitro-in vivo concordance for TdP prediction and 91% sensitivity for non-TdP arrhythmia detection, or 83% and 91% if clinically efficacious plasma (effective serum therapeutic concentration [C eff]) values are incorporated. This human cardiomyocyte arrhythmic risk (hCAR) model provides greater predictivity for torsadogenicity in humans compared with either human ether-a-go-go-related gene (hERG) inhibition (75%) or QT prolongation (81%). The concordance of beat rate reductions to predict clinical QT prolongation is 86%, or 87% when C eff is considered, which is greater than a hERG signal (80%). Further, arrhythmic beats resulting from cytotoxicity were identified by a distinct arrhythmic beating pattern, which occurred after the onset of cytolethality. This hCAR assay showed increased performance over existing preclinical tools in predicting clinical QT prolongation, arrhythmia, and TdP. Thus, hiPS-CMs are a relevant cell system to improve evaluating cardiac safety liabilities of drug candidates.", "publication_year": "2013", "authors": ["Liang Guo", "Luke A. Coyle", "Rory M. C. Abrams", "Raymond A. Kemper", "Eric T Chiao", "Kyle L Kolaja"], "related_topics": ["Biology", "Medicine"], "references": ["307ff8f512098497e2c69b79c00fbb7b3cc9650e", "d33fc88776aa3cafaee9e6cb40738c3d19bd9ac1", "dfce4451e73ab7108517c0b020a4fc2cc4e63e1b", "de9817a662077f9f9b3b77107590685584bd89e0", "c1cb043d6cc44f9c661a45c1daba623d41b36bf1", "c53bbacfe5f720a9cb464ed0a94349a69de1d8d2", "1cbee9a44469d434efd9ed0d55347e4b2c80e2f8", "88499b49a830a39dd16381e224d8880e47d868ce", "f5ddb94efb7cd13f6fa7f1ba1f0d12a387837e55", "41e1e4d6154c0e9193e808b9ac815a9c6024b52e"], "references_count": 40, "citations_count": 116}, {"id": "c3abc2d4b3cb86d6e3606f225c671fff3b334c9b", "url": "https://www.semanticscholar.org/paper/In-vitro-Modeling-of-Ryanodine-Receptor-2-Using-Fatima-Xu/c3abc2d4b3cb86d6e3606f225c671fff3b334c9b", "title": "In vitro Modeling of Ryanodine Receptor 2 Dysfunction Using Human Induced Pluripotent Stem Cells", "abstract": "This study demonstrates the suitability of iPS cells in modeling RYR2-related cardiac disorders in vitro and opens new opportunities for investigating the disease mechanism in vitro, developing new drugs, predicting their toxicity, and optimizing current treatment strategies. Background/Aims: Induced pluripotent stem (iPS) cells generated from accessible adult cells of patients with genetic diseases open unprecedented opportunities for exploring the pathophysiology of human diseases in vitro. Catecholaminergic polymorphic ventricular tachycardia type 1 (CPVT1) is an inherited cardiac disorder that is caused by mutations in the cardiac ryanodine receptor type 2 gene (RYR2) and is characterized by stress-induced ventricular arrhythmia that can lead to sudden cardiac death in young individuals. The aim of this study was to generate iPS cells from a patient with CPVT1 and determine whether iPS cell-derived cardiomyocytes carrying patient specific RYR2 mutation recapitulate the disease phenotype in vitro. Methods: iPS cells were derived from dermal fibroblasts of healthy donors and a patient with CPVT1 carrying the novel heterozygous autosomal dominant mutation p.F2483I in the RYR2. Functional properties of iPS cell derived-cardiomyocytes were analyzed by using whole-cell current and voltage clamp and calcium imaging techniques. Results: Patch-clamp recordings revealed arrhythmias and delayed afterdepolarizations (DADs) after catecholaminergic stimulation of CPVT1-iPS cell-derived cardiomyocytes. Calcium imaging studies showed that, compared to healthy cardiomyocytes, CPVT1-cardiomyocytes exhibit higher amplitudes and longer durations of spontaneous Ca2+ release events at basal state. In addition, in CPVT1-cardiomyocytes the Ca2+-induced Ca2+-release events continued after repolarization and were abolished by increasing the cytosolic cAMP levels with forskolin. Conclusion: This study demonstrates the suitability of iPS cells in modeling RYR2-related cardiac disorders in vitro and opens new opportunities for investigating the disease mechanism in vitro, developing new drugs, predicting their toxicity, and optimizing current treatment strategies.", "publication_year": "2011", "authors": ["Azra Fatima", "Guoxing Xu", "Kaifeng Shao", "Symeon Papadopoulos", "Martin Lehmann", "Juan Jose Arnaiz-Cot", "Angelo Oscar Rosa", "Filomain Nguemo", "Matthias Matzkies", "Sven Dittmann", "Susannah L. Stone", "M Linke", "Ulrich Zechner", "Vera Beyer", "Hans Christian Hennies", "Stephan Rosenkranz", "Baerbel Klauke", "Abdul Shokor Parwani", "Wilhelm Haverkamp", "Gabriele Pfitzer", "Martin Farr", "Lars Nilausen Cleemann", "Martin Morad", "Hendrik Milting", "J{\\\"u"], "related_topics": ["Biology", "Medicine"], "references": ["d1c166e8edf5e672ccc08dc65b1979aaeaafa775", "0f75886a110eaa92b4073534b7b661404d06adee", "42c550a012171047707cd12afb10ef6d2c3e661c", "70d2bd197d72935a93bc486a9345992051f6ad69", "c1b24375b2292fb40e0a444803183e19cfbe6347", "78518cbc7679530967d8ad894e244db8b27a0988", "9c3fe9272d32260f3b8308c21fb9ee5365e74412", "fb176b6504e797fcff1571abf9ea83d8ff66785f", "4724c49eb2ff897876c2464d9970fd21746526ad", "b23ccb4b1492d21a8794ceec3ae6ecf7cc718e49"], "references_count": 70, "citations_count": 197}, {"id": "d779b87172306c37c2c711512e84bc8112adf21e", "url": "https://www.semanticscholar.org/paper/Towards-Automatic-Prostate-Gleason-Grading-Via-Deep-Khani-Jahromi/d779b87172306c37c2c711512e84bc8112adf21e", "title": "Towards Automatic Prostate Gleason Grading Via Deep Convolutional Neural Networks", "abstract": "This paper uses the DeepLabV3+ model with MobileNetV2 backbone and train it with the newly released dataset from Gleason 2019 challenge and achieves the mean Cohen's quadratic kappa score of 0.56 with the pathologists' annotations on the test subset which is higher than the inter-pathologists' score. Prostate Cancer has become one of the deadliest cancers among males in many nations. Pathologists use various approaches for the detection and the staging of prostate cancer. Microscopic inspection of biopsy tissues is the most accurate approach among them. The Gleason grading system is used to evaluate the stage of Prostate Cancer using prostate biopsy samples. The task of assigning a grade to each region in a tissue is a time-consuming task. Furthermore, this task often has several challenges since it has considerable inter-observer variability even among expert pathologists. In this paper, we propose an automatic method for this task using a deep learningbased approach. For this purpose, we use the DeepLabV3+ model with MobileNetV2 backbone and train it with the newly released dataset from Gleason 2019 challenge. Our model achieves the mean Cohen's quadratic kappa score of 0.56 with the pathologists' annotations on the test subset which is higher than the inter-pathologists' score (0.55).", "publication_year": "2019", "authors": ["Ali Asghar Khani", "Seyed Alireza Fatemi Jahromi", "Hatef Otroshi Shahreza", "Hamid Behroozi", "Mahdieh Soleymani Baghshah"], "related_topics": ["Computer Science"], "references": ["2c2f461af47644c0c05a62da0739c4bb48b99cdf", "c310babca20446de2ee7d8857abe239e0fe261a0", "47262a72c9c7bf5070b97e70b55c6190d1079260", "d5b91f292c611dea61f6e95b007ae53c2766a5f9", "d20dff7103c3da6753ef108dc825d9fe44bd00d2", "50004c086ffd6a201a4b782281aaa930fbfe6ecf", "a6876ea89e677a7cc42dd43f27165ff6fd414de5", "6364fdaa0a0eccd823a779fcdd489173f938e91a", "8f7bb9b751da9ede977395630b4482df634c38be", "84c1717345dd451e7a61fe89807b4c017754fc4e"], "references_count": 36, "citations_count": 6}, {"id": "915adc7d9aacc46b6b8575f4a8be4b7cb4a1caf7", "url": "https://www.semanticscholar.org/paper/Detecting-Cancer-Metastases-on-Gigapixel-Pathology-Liu-Gadepalli/915adc7d9aacc46b6b8575f4a8be4b7cb4a1caf7", "title": "Detecting Cancer Metastases on Gigapixel Pathology Images", "abstract": "This work presents a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x100,000 pixels and achieves image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.", "publication_year": "2017", "authors": ["Yun Liu", "Krishna Gadepalli", "Mohammad Norouzi", "George E. Dahl", "Timo Kohlberger", "Aleksey Boyko", "Subhashini Venugopalan", "Aleksei Timofeev", "Phil Q. Nelson", "Greg S Corrado", "Jason D. Hipp", "Lily H. Peng", "Martin C. Stumpe"], "related_topics": ["Computer Science"], "references": ["21ba757bf394720e0b66b86e7638ae28742d6570", "2f11f86fd805807076b22317738c819484a8e21b", "47262a72c9c7bf5070b97e70b55c6190d1079260", "80b0a281c520581e474d178e4020721c61ab5667", "94087ad5ed11555c260a42f2f9ca9da183c6f87e", "d847ee63fe234f9cc2a8be851ed511b7d1a8da36", "485f0c988c7a4f9bc0e976c65a5055837091fd39", "4661b82606512f03a2f3fcc1d2587152b89f8e73", "2729d2918978d5ed602aa843fbdd027d83e0036f", "2f4df08d9072fc2ac181b7fced6a245315ce05c8"], "references_count": 25, "citations_count": 566}, {"id": "769149c0dc0ed308eca8bc916f4326b2e2f57a1f", "url": "https://www.semanticscholar.org/paper/Classification-and-mutation-prediction-from-cell-Coudray-Ocampo/769149c0dc0ed308eca8bc916f4326b2e2f57a1f", "title": "Classification and mutation prediction from non\u2013small cell lung cancer histopathology images using deep learning", "abstract": "A deep convolutional neural network model is trained on whole-slide images obtained from The Cancer Genome Atlas to accurately and automatically classify them into LUAD, LUSC or normal lung tissue and predicts the ten most commonly mutated genes in LUAD. Visual inspection of histopathology slides is one of the main methods used by pathologists to assess the stage, type and subtype of lung tumors. Adenocarcinoma (LUAD) and squamous cell carcinoma (LUSC) are the most prevalent subtypes of lung cancer, and their distinction requires visual inspection by an experienced pathologist. In this study, we trained a deep convolutional neural network (inception v3) on whole-slide images obtained from The Cancer Genome Atlas to accurately and automatically classify them into LUAD, LUSC or normal lung tissue. The performance of our method is comparable to that of pathologists, with an average area under the curve (AUC) of 0.97. Our model was validated on independent datasets of frozen tissues, formalin-fixed paraffin-embedded tissues and biopsies. Furthermore, we trained the network to predict the ten most commonly mutated genes in LUAD. We found that six of them\u2014STK11, EGFR, FAT1, SETBP1, KRAS and TP53\u2014can be predicted from pathology images, with AUCs from 0.733 to 0.856 as measured on a held-out population. These findings suggest that deep-learning models can assist pathologists in the detection of cancer subtype or gene mutations. Our approach can be applied to any cancer type, and the code is available at https://github.com/ncoudray/DeepPATH.A convolutional neural network model using feature extraction and machine-learning techniques provides a tool for classification of lung cancer histopathology images and predicting mutational status of driver oncogenes", "publication_year": "2018", "authors": ["Nicolas Coudray", "Paolo Santiago Ocampo", "Theodore Sakellaropoulos", "Navneet Narula", "Matija Snuderl", "David Feny{\\\"o"], "related_topics": ["Biology"], "references": ["94087ad5ed11555c260a42f2f9ca9da183c6f87e", "bd6f783022ebd6704ff34f6bf824ef1cb1ad0cee", "add18010e1af63998bae7573f4cd5d2843eeb5bb", "abb569b5b79365f57e7d20150b31bf65da89f275", "d5b91f292c611dea61f6e95b007ae53c2766a5f9", "7256088eece603df2e5675025e8bed90c0f21171", "bcc9db4c560ea48d1b205acfc8ec77568d913503", "e1ec11a1cb3d9745fb18d3bf74247f95a6663d08", "e1e27b29318ff47de91619940019b10fd584c231", "30027db82f4eb242a2ee05973cabb06c8f02bd73"], "references_count": 68, "citations_count": 1472}, {"id": "21ba757bf394720e0b66b86e7638ae28742d6570", "url": "https://www.semanticscholar.org/paper/Deep-Learning-for-Identifying-Metastatic-Breast-Wang-Khosla/21ba757bf394720e0b66b86e7638ae28742d6570", "title": "Deep Learning for Identifying Metastatic Breast Cancer", "abstract": "The power of using deep learning to produce significant improvements in the accuracy of pathological diagnoses is demonstrated, by combining the deep learning system's predictions with the human pathologist's diagnoses. The International Symposium on Biomedical Imaging (ISBI) held a grand challenge to evaluate computational systems for the automated detection of metastatic breast cancer in whole slide images of sentinel lymph node biopsies. Our team won both competitions in the grand challenge, obtaining an area under the receiver operating curve (AUC) of 0.925 for the task of whole slide image classification and a score of 0.7051 for the tumor localization task. A pathologist independently reviewed the same images, obtaining a whole slide image classification AUC of 0.966 and a tumor localization score of 0.733. Combining our deep learning system's predictions with the human pathologist's diagnoses increased the pathologist's AUC to 0.995, representing an approximately 85 percent reduction in human error rate. These results demonstrate the power of using deep learning to produce significant improvements in the accuracy of pathological diagnoses.", "publication_year": "2016", "authors": ["Dayong Wang", "Aditya Khosla", "Rishab Gargeya", "Humayun Irshad", "Andrew H. Beck"], "related_topics": ["Medicine"], "references": ["077592c2b76318a15562cf9f962f515988c011fb", "13de33ee941f1ebf3ed185c20fb4453a07302c30", "bd898f483476e3dcacf83cd85efc64e6319da0e1", "64aec645896bb6b444f6d81620fd5c9a1e3d6d6c", "56145dd68267bcef288f072e155fa617b325d33a", "485f0c988c7a4f9bc0e976c65a5055837091fd39", "ab9dce8b2af5e11985736be6bd73ec0968b2bb27", "a601a1ea75bde37f4dffd4f4c6025e91d2ae9a29", "ef6a1baa9441a4ebc4a5fb90f8c64ff67a61b288", "7651dc7f8e73be39aec686542bdc418de69a8b31"], "references_count": 25, "citations_count": 783}, {"id": "6048de9749a1f31ac70e5c30030ceb1dc5d3f2b0", "url": "https://www.semanticscholar.org/paper/PFA-ScanNet%3A-Pyramidal-Feature-Aggregation-with-for-Zhao-Lin/6048de9749a1f31ac70e5c30030ceb1dc5d3f2b0", "title": "PFA-ScanNet: Pyramidal Feature Aggregation with Synergistic Learning for Breast Cancer Metastasis Analysis", "abstract": "A novel Pyramidal Feature Aggregation ScanNet (PFA-ScanNet) for robust and fast analysis of breast cancer metastasis, which mainly benefits from the aggregation of extracted local-to-global features with diverse receptive fields and a high-efficiency inference mechanism designed with dense pooling layers. Automatic detection of cancer metastasis from whole slide images (WSIs) is a crucial step for following patient staging and prognosis. Recent convolutional neural network based approaches are struggling with the trade-off between accuracy and computational efficiency due to the difficulty in processing large-scale gigapixel WSIs. To meet this challenge, we propose a novel Pyramidal Feature Aggregation ScanNet (PFA-ScanNet) for robust and fast analysis of breast cancer metastasis. Our method mainly benefits from the aggregation of extracted local-to-global features with diverse receptive fields, as well as the proposed synergistic learning for training the main detector and extra decoder with semantic guidance. Furthermore, a high-efficiency inference mechanism is designed with dense pooling layers, which allows dense and fast scanning for gigapixel WSI analysis. As a result, the proposed PFA-ScanNet achieved the state-of-the-art FROC of 90.2% on the Camelyon16 dataset, as well as competitive kappa score of 0.905 on the Camelyon17 leaderboard. In addition, our method shows leading speed advantage over other methods, about 7.2 min per WSI with a single GPU, making automatic analysis of breast cancer metastasis more applicable in the clinical usage.", "publication_year": "2019", "authors": ["Zixu Zhao", "Huangjing Lin", "Hao Chen", "Pheng-Ann Heng"], "related_topics": ["Computer Science"], "references": ["e0a711d111eb0373a06d46bbe26b710f7c924ccb", "c1555c566a2628f0485b16b2ae4371aff617d7f7", "7642c455f69a2fe21d8f03679d3f6df7fcf0e9a5", "915adc7d9aacc46b6b8575f4a8be4b7cb4a1caf7", "4bfddce2f6356166be52eb7044864812df9c646b", "21ba757bf394720e0b66b86e7638ae28742d6570", "cfa3d45a6fcf704fe7ab6953424481d1698055a4", "5fd490e5ceed129a83d16dbda29ab61fe4aa1acb", "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36", "3617ccfec4bed2d8ac15d0ad1a35b589d9b270cb"], "references_count": 13, "citations_count": 20}, {"id": "ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba", "url": "https://www.semanticscholar.org/paper/Diagnostic-Assessment-of-Deep-Learning-Algorithms-Bejnordi-Veta/ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba", "title": "Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer", "abstract": "In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Importance Application of deep learning algorithms to whole-slide pathology images can potentially improve diagnostic accuracy and efficiency. Objective Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin\u2013stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists\u2019 diagnoses in a diagnostic setting. Design, Setting, and Participants Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n\u2009=\u2009110) and without (n\u2009=\u2009160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC). Exposures Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation. Main Outcomes and Measures The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor. Results The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4% [95% CI, 64.3%-80.4%]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95% CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884]; P\u2009&lt;\u2009.001). The top 5 algorithms had a mean AUC that was comparable with the pathologist interpreting the slides in the absence of time constraints (mean AUC, 0.960 [range, 0.923-0.994] for the top 5 algorithms vs 0.966 [95% CI, 0.927-0.998] for the pathologist WOTC). Conclusions and Relevance In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting.", "publication_year": "2017", "authors": ["Babak Ehteshami Bejnordi", "Mitko Veta", "Paul Johannes van Diest", "Bram van Ginneken", "Nico Karssemeijer", "Geert J. S. Litjens", "Jeroen A. van der Laak", "Meyke Hermsen", "Quirine F. Manson", "Maschenka C. A. Balkenhol", "Oscar G. F. Geessink", "Nikolaos Stathonikos", "Marcory Crf van Dijk", "Peter Bult", "Francisco Beca", "Andrew H. Beck", "Dayong Wang", "Aditya Khosla", "Rishab Gargeya", "Humayun Irshad", "Aoxiao Zhong", "Qi Dou", "Quanzheng Li", "Hao Chen", "Huangjing Lin", "Pheng-Ann Heng", "Christian Hass", "Elia Bruni", "Quincy Kwan-Sut Wong", "Ugur Halici", "Mustafa {\\\"U"], "related_topics": ["Medicine"], "references": ["47262a72c9c7bf5070b97e70b55c6190d1079260", "a8916917d2e8bf88a63e27c2ecbe6e3294882667", "bed1bcdf96bdd3bc56e80fe769e87f12ad6b2e6b", "b23a7d485ee8f60f33119c27acf43607caee3cd3", "e1ec11a1cb3d9745fb18d3bf74247f95a6663d08", "2729d2918978d5ed602aa843fbdd027d83e0036f", "c767fbf94ae063f91fbf14b511bbb21664a394bf", "c912e250ba2703d06e4399c7103e84457ecc39d8", "8d7bc6a0af5c063c457a88561bcf8f895c9f7392", "63cb9d55b52c92a75f54d1d5f66153a8a18261f9"], "references_count": 66, "citations_count": 1919}, {"id": "188f8f6f70947215a9dfeebb0b577155e0d3d339", "url": "https://www.semanticscholar.org/paper/1399-H%26E-stained-sentinel-lymph-node-sections-of-Litjens-B%C3%A1ndi/188f8f6f70947215a9dfeebb0b577155e0d3d339", "title": "1399 H&E-stained sentinel lymph node sections of breast cancer patients: the CAMELYON dataset", "abstract": "A unique dataset of annotated, whole-slide digital histopathology images has been provided with high potential for re-use, in 3 terabytes of data in the context of the CAMELYON16 and CAMELYon17 Grand Challenges. Abstract Background The presence of lymph node metastases is one of the most important factors in breast cancer prognosis. The most common way to assess regional lymph node status is the sentinel lymph node procedure. The sentinel lymph node is the most likely lymph node to contain metastasized cancer cells and is excised, histopathologically processed, and examined by a pathologist. This tedious examination process is time-consuming and can lead to small metastases being missed. However, recent advances in whole-slide imaging and machine learning have opened an avenue for analysis of digitized lymph node sections with computer algorithms. For example, convolutional neural networks, a type of machine-learning algorithm, can be used to automatically detect cancer metastases in lymph nodes with high accuracy. To train machine-learning models, large, well-curated datasets are needed. Results We released a dataset of 1,399 annotated whole-slide images (WSIs) of lymph nodes, both with and without metastases, in 3 terabytes of data in the context of the CAMELYON16 and CAMELYON17 Grand Challenges. Slides were collected from five medical centers to cover a broad range of image appearance and staining variations. Each WSI has a slide-level label indicating whether it contains no metastases, macro-metastases, micro-metastases, or isolated tumor cells. Furthermore, for 209 WSIs, detailed hand-drawn contours for all metastases are provided. Last, open-source software tools to visualize and interact with the data have been made available. Conclusions A unique dataset of annotated, whole-slide digital histopathology images has been provided with high potential for re-use.", "publication_year": "2018", "authors": ["Geert J. S. Litjens", "P{\\'e"], "related_topics": ["Computer Science", "Medicine"], "references": ["ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba", "2d422a474e5d346ac73a386c7e7fcbab2805db5d", "b23a7d485ee8f60f33119c27acf43607caee3cd3", "47262a72c9c7bf5070b97e70b55c6190d1079260", "3bc56f7c5807e2dafcf39eb704051e2ce25eceeb", "fe132ff1a013c626f21122165af2065428491825", "67f2a9f2de5090d137a2e0b7984ecc32c4f094f3", "d6f5b61f6a24009d1ed741f49a024b7eb5c5357e", "d0ad4142e4e7fb052935b349dc67bac288c83ff3", "ed5a48e55cb801b83434b9529998f6226046f8df"], "references_count": 31, "citations_count": 230}, {"id": "ae1c89817a3a239e5344293138bdd80293983460", "url": "https://www.semanticscholar.org/paper/Attention-U-Net%3A-Learning-Where-to-Look-for-the-Oktay-Schlemper/ae1c89817a3a239e5344293138bdd80293983460", "title": "Attention U-Net: Learning Where to Look for the Pancreas", "abstract": "A novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes is proposed to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.", "publication_year": "2018", "authors": ["Ozan Oktay", "Jo Schlemper", "Lo{\\\"i"], "related_topics": ["Computer Science"], "references": ["aac368016f2540683ad2f611eb0cd889d350ff72", "50004c086ffd6a201a4b782281aaa930fbfe6ecf", "6364fdaa0a0eccd823a779fcdd489173f938e91a", "3d44a1a97b6c768ae3080adf717326167457b0ad", "65147d0652741e886243549123dab142699e07eb", "c70218603f0af1be5d063056cbe629e042141a86", "1acdfdf6e24dc6c89f61aa7600648b38870bbc9b", "01556b9fbade335e0b58812fa75023a6dd409ce1", "b1135c3ba94839082c91c5b2600181d251b2634b", "569977bdb3f31d4b7c78ab3834fd34b370330e4e"], "references_count": 41, "citations_count": 2391}, {"id": "57a892b9576baeba70277179712d5b09e19224b9", "url": "https://www.semanticscholar.org/paper/Automated-Gleason-grading-of-prostate-cancer-tissue-Arvaniti-Fricker/57a892b9576baeba70277179712d5b09e19224b9", "title": "Automated Gleason grading of prostate cancer tissue microarrays via deep learning", "abstract": "A deep learning approach for automated Gleason grading of prostate cancer tissue microarrays with Hematoxylin and Eosin (H&E) staining achieves pathology expert-level stratification of patients into prognostically distinct groups, on the basis of disease-specific survival data available for the test cohort. The Gleason grading system remains the most powerful prognostic predictor for patients with prostate cancer since the 1960s. Its application requires highly-trained pathologists, is tedious and yet suffers from limited inter-pathologist reproducibility, especially for the intermediate Gleason score 7. Automated annotation procedures constitute a viable solution to remedy these limitations. In this study, we present a deep learning approach for automated Gleason grading of prostate cancer tissue microarrays with Hematoxylin and Eosin (H&E) staining. Our system was trained using detailed Gleason annotations on a discovery cohort of 641 patients and was then evaluated on an independent test cohort of 245 patients annotated by two pathologists. On the test cohort, the inter-annotator agreements between the model and each pathologist, quantified via Cohen\u2019s quadratic kappa statistic, were 0.75 and 0.71 respectively, comparable with the inter-pathologist agreement (kappa\u2009=\u20090.71). Furthermore, the model\u2019s Gleason score assignments achieved pathology expert-level stratification of patients into prognostically distinct groups, on the basis of disease-specific survival data available for the test cohort. Overall, our study shows promising results regarding the applicability of deep learning-based solutions towards more objective and reproducible prostate cancer grading, especially for cases with heterogeneous Gleason patterns.", "publication_year": "2018", "authors": ["Eirini Arvaniti", "Kim S. Fricker", "Michael Moret", "Niels J. Rupp", "Thomas Hermanns", "Christian Daniel Fankhauser", "Norbert Wey", "Peter J. Wild", "Jan Hendrik R{\\\"u"], "related_topics": ["Biology"], "references": ["e37ab4e114de7b8ab60ca74e6a89e4fbbed3a625", "47262a72c9c7bf5070b97e70b55c6190d1079260", "5faf07decd896237a82b89e4e4fd42739a3eea1b", "561a83fb8e86461e6d2432e382ffc0b32575cb54", "d20dff7103c3da6753ef108dc825d9fe44bd00d2", "8cb8c1b719f27b7a22c4e9bf5211f7403d22c749", "915adc7d9aacc46b6b8575f4a8be4b7cb4a1caf7", "abb569b5b79365f57e7d20150b31bf65da89f275", "b2b6dc48ba97c26d92c8a772213284a0eb546bdd", "077592c2b76318a15562cf9f962f515988c011fb"], "references_count": 42, "citations_count": 266}, {"id": "a6b8cd5f34b438f487679b1166ea03e56eb14c9e", "url": "https://www.semanticscholar.org/paper/Semi-Supervised-Zero-Shot-Classification-with-Label-Li-Guo/a6b8cd5f34b438f487679b1166ea03e56eb14c9e", "title": "Semi-Supervised Zero-Shot Classification with Label Representation Learning", "abstract": "A novel zero-shot classification approach that automatically learns label embeddings from the input data in a semi-supervised large-margin learning framework that tackles the target prediction problem directly without introducing intermediate prediction problems. Given the challenge of gathering labeled training data, zero-shot classification, which transfers information from observed classes to recognize unseen classes, has become increasingly popular in the computer vision community. Most existing zero-shot learning methods require a user to first provide a set of semantic visual attributes for each class as side information before applying a two-step prediction procedure that introduces an intermediate attribute prediction problem. In this paper, we propose a novel zero-shot classification approach that automatically learns label embeddings from the input data in a semi-supervised large-margin learning framework. The proposed framework jointly considers multi-class classification over all classes (observed and unseen) and tackles the target prediction problem directly without introducing intermediate prediction problems. It also has the capacity to incorporate semantic label information from different sources when available. To evaluate the proposed approach, we conduct experiments on standard zero-shot data sets. The empirical results show the proposed approach outperforms existing state-of-the-art zero-shot learning methods.", "publication_year": "2015", "authors": ["X. Li", "Yuhong Guo", "Dale Schuurmans"], "related_topics": ["Computer Science"], "references": ["b29227f8dde62a5cd21678b4bc429206615485a2", "ab50e0fba1e7964d2686e90f9bed66a06ed6ff42", "ccbc09d498cad330c37f94e15b77bf220b10ccb4", "755e9f43ce398ae8737366720c5f82685b0c253e", "d6714ee0a3c3c5ead3d681d4bec8e60f042928ef", "caa632d101a41a7860562e4399a5eaa9a4088b55", "018e730f8947173e1140210d4d1760d05c9d3854", "c30b9fb837e912ccf3919fdb64e9543fca57799e", "f038e8c3656f5c7a4846a7eca731eb567255adcb", "88e090ffc1f75eed720b5afb167523eb2e316f7f"], "references_count": 34, "citations_count": 114}, {"id": "b29227f8dde62a5cd21678b4bc429206615485a2", "url": "https://www.semanticscholar.org/paper/Max-Margin-Zero-Shot-Learning-for-Multi-class-Li-Guo/b29227f8dde62a5cd21678b4bc429206615485a2", "title": "Max-Margin Zero-Shot Learning for Multi-class Classification", "abstract": "A semi-supervised max-margin learning framework that integrates the semisupervised classification problem over observed classes and the unsupervised clustering problem over unseen classes together to tackle zero-shot multi-class classification is proposed. Due to the dramatic expanse of data categories and the lack of labeled instances, zero-shot learning, which transfers knowledge from observed classes to recognize unseen classes, has started drawing a lot of attention from the research community. In this paper, we propose a semi-supervised max-margin learning framework that integrates the semisupervised classification problem over observed classes and the unsupervised clustering problem over unseen classes together to tackle zero-shot multi-class classification. By further integrating label embedding into this framework, we produce a dual formulation that permits convenient incorporation of auxiliary label semantic knowledge to improve zero-shot learning. We conduct extensive experiments on three standard image data sets to evaluate the proposed approach by comparing to two state-of-the-art methods. Our results demonstrate the efficacy of the proposed framework.", "publication_year": "2015", "authors": ["X. Li", "Yuhong Guo"], "related_topics": ["Computer Science"], "references": ["d6714ee0a3c3c5ead3d681d4bec8e60f042928ef", "ab50e0fba1e7964d2686e90f9bed66a06ed6ff42", "caa632d101a41a7860562e4399a5eaa9a4088b55", "c30b9fb837e912ccf3919fdb64e9543fca57799e", "516b1eda00a955043fbcf037f128b117c9d9b10c", "be2f5d8a7e6b415f1e22cee7dfd9be56b1afd8be", "9bc0295460089592d04e754a5fd427060b7bfa8c", "2750dbc60d5ccc8fbe5e4babae6cfab543940f1a", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "88e090ffc1f75eed720b5afb167523eb2e316f7f"], "references_count": 28, "citations_count": 40}, {"id": "846946cd21413211a4701f309c3927d67363cd30", "url": "https://www.semanticscholar.org/paper/Synthesized-Classifiers-for-Zero-Shot-Learning-Changpinyo-Chao/846946cd21413211a4701f309c3927d67363cd30", "title": "Synthesized Classifiers for Zero-Shot Learning", "abstract": "This work introduces a set of \\\"phantom\\\" object classes whose coordinates live in both the semantic space and the model space and demonstrates superior accuracy of this approach over the state of the art on four benchmark datasets for zero-shot learning. Given semantic descriptions of object classes, zero-shot learning aims to accurately recognize objects of the unseen classes, from which no examples are available at the training stage, by associating them to the seen classes, from which labeled examples are provided. We propose to tackle this problem from the perspective of manifold learning. Our main idea is to align the semantic space that is derived from external information to the model space that concerns itself with recognizing visual features. To this end, we introduce a set of \\\"phantom\\\" object classes whose coordinates live in both the semantic space and the model space. Serving as bases in a dictionary, they can be optimized from labeled data such that the synthesized real object classifiers achieve optimal discriminative performance. We demonstrate superior accuracy of our approach over the state of the art on four benchmark datasets for zero-shot learning, including the full ImageNet Fall 2011 dataset with more than 20,000 unseen classes.", "publication_year": "2016", "authors": ["Soravit Changpinyo", "Wei-Lun Chao", "Boqing Gong", "Fei Sha"], "related_topics": ["Computer Science"], "references": ["755e9f43ce398ae8737366720c5f82685b0c253e", "a6b8cd5f34b438f487679b1166ea03e56eb14c9e", "ac98259064e86f643f2cd11e5417b43bf28daa91", "b2a5c3744eea40c76d0359e517026e8ed6c922ff", "d302b1fde88a6859a3bdaadcb940748b6debaf20", "7c0773c7578433a2277e919ac824f142d5de351c", "0e49be4280379b77a561b89f5b049837f48d1ecb", "ccbc09d498cad330c37f94e15b77bf220b10ccb4", "9bc0295460089592d04e754a5fd427060b7bfa8c", "b0d08e25e46c28423b60668836d382fdf245e7d9"], "references_count": 56, "citations_count": 682}, {"id": "ac98259064e86f643f2cd11e5417b43bf28daa91", "url": "https://www.semanticscholar.org/paper/Zero-Shot-Learning-via-Semantic-Similarity-Zhang-Saligrama/ac98259064e86f643f2cd11e5417b43bf28daa91", "title": "Zero-Shot Learning via Semantic Similarity Embedding", "abstract": "A version of the zero-shot learning problem where seen class source and target domain data are provided and the goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information for unseen classes. In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information (e.g. attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.", "publication_year": "2015", "authors": ["Ziming Zhang", "Venkatesh Saligrama"], "related_topics": ["Computer Science"], "references": ["ccbc09d498cad330c37f94e15b77bf220b10ccb4", "755e9f43ce398ae8737366720c5f82685b0c253e", "caa632d101a41a7860562e4399a5eaa9a4088b55", "be2f5d8a7e6b415f1e22cee7dfd9be56b1afd8be", "d6714ee0a3c3c5ead3d681d4bec8e60f042928ef", "ab50e0fba1e7964d2686e90f9bed66a06ed6ff42", "4aa4069693bee00d1b0759ca3df35e59284e9845", "018e730f8947173e1140210d4d1760d05c9d3854", "c30b9fb837e912ccf3919fdb64e9543fca57799e", "caccc069e658ea397c9faf673e74c959c734ff53"], "references_count": 48, "citations_count": 564}, {"id": "caa632d101a41a7860562e4399a5eaa9a4088b55", "url": "https://www.semanticscholar.org/paper/Label-Embedding-for-Attribute-Based-Classification-Akata-Perronnin/caa632d101a41a7860562e4399a5eaa9a4088b55", "title": "Label-Embedding for Attribute-Based Classification", "abstract": "This work proposes to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors, and introduces a function which measures the compatibility between an image and a label embedding. Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.", "publication_year": "2013", "authors": ["Zeynep Akata", "Florent Perronnin", "Za{\\\"i"], "related_topics": ["Computer Science"], "references": ["54aacc196ffe49b3450059fccdf7cd3bb6f6f3c3", "88e090ffc1f75eed720b5afb167523eb2e316f7f", "a3ea706f6604a1e6e87c33d7a3b4b97b1bb338ef", "16a3c2c3e2bfbac65cc89a031b340a5951526183", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "3a4a53fe47036ac89dad070ab87a9d8795b139b1", "9d69a7ab54c717df44f152c617a8cc76218437ff", "3089e6745b7dd50e41a3a50c6ff831415fe22739", "fc23a386c2189f221b25dbd0bb34fcd26ccf60fa", "c30b9fb837e912ccf3919fdb64e9543fca57799e"], "references_count": 46, "citations_count": 595}, {"id": "755e9f43ce398ae8737366720c5f82685b0c253e", "url": "https://www.semanticscholar.org/paper/Zero-Shot-Learning-Through-Cross-Modal-Transfer-Socher-Ganjoo/755e9f43ce398ae8737366720c5f82685b0c253e", "title": "Zero-Shot Learning Through Cross-Modal Transfer", "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the object class, and uses novelty detection methods to differentiate unseen classes from seen classes. This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.", "publication_year": "2013", "authors": ["Richard Socher", "Milind Ganjoo", "Christopher D. Manning", "A. Ng"], "related_topics": ["Computer Science"], "references": ["0566bf06a0368b518b8b474166f7b1dfef3f9283", "5d90f06bb70a0a3dced62413346235c02b1aa086", "812355cec91fa30bb50e9e992a3549af39e4f6eb", "5726c7b40fcc454b77d989656c085520bf6c15fa", "6eb3a15108dfdec25b46522ed94b866aeb156de9", "100a038fdf29b4b20801887f0ec40e3f10d9a4f9", "80e9e3fc3670482c1fee16b2542061b779f47c4f", "0f6911bc1e6abee8bbf9dd3f8d54d40466429da7", "0d8ec0d3ac8c8e0f6dcda6e0b1845d29e985e58b", "67fdc1e0d878675e9ac765830f85b461777e49ec"], "references_count": 36, "citations_count": 1318}, {"id": "244ae156ba2aaa91b2fa443c8ceb74ee13c6c6fa", "url": "https://www.semanticscholar.org/paper/Multi-cue-Zero-Shot-Learning-with-Strong-Akata-Malinowski/244ae156ba2aaa91b2fa443c8ceb74ee13c6c6fa", "title": "Multi-cue Zero-Shot Learning with Strong Supervision", "abstract": "This work introduces a joint embedding framework that maps multiple text parts as well as multiple semantic parts into a common space that consistently and significantly improves on the state-of-the-art in zero-short recognition and retrieval. Scaling up visual category recognition to large numbers of classes remains challenging. A promising research direction is zero-shot learning, which does not require any training data to recognize new classes, but rather relies on some form of auxiliary information describing the new classes. Ultimately, this may allow to use textbook knowledge that humans employ to learn about new classes by transferring knowledge from classes they know well. The most successful zero-shot learning approaches currently require a particular type of auxiliary information - namely attribute annotations performed by humans - that is not readily available for most classes. Our goal is to circumvent this bottleneck by substituting such annotations by extracting multiple pieces of information from multiple unstructured text sources readily available on the web. To compensate for the weaker form of auxiliary information, we incorporate stronger supervision in the form of semantic part annotations on the classes from which we transfer knowledge. We achieve our goal by a joint embedding framework that maps multiple text parts as well as multiple semantic parts into a common space. Our results consistently and significantly improve on the state-of-the-art in zero-short recognition and retrieval.", "publication_year": "2016", "authors": ["Zeynep Akata", "Mateusz Malinowski", "Mario Fritz", "Bernt Schiele"], "related_topics": ["Computer Science"], "references": ["ccbc09d498cad330c37f94e15b77bf220b10ccb4", "c30b9fb837e912ccf3919fdb64e9543fca57799e", "4aa4069693bee00d1b0759ca3df35e59284e9845", "6540cb7971d1a9d72562d465172e010fbb729bc3", "be2f5d8a7e6b415f1e22cee7dfd9be56b1afd8be", "caccc069e658ea397c9faf673e74c959c734ff53", "6cd5fc1f0a63df570d3bccb33bf300791574e06f", "9bc0295460089592d04e754a5fd427060b7bfa8c", "aea0f946e8dcddb65cc2e907456c42453f246a50", "88e090ffc1f75eed720b5afb167523eb2e316f7f"], "references_count": 55, "citations_count": 131}, {"id": "018e730f8947173e1140210d4d1760d05c9d3854", "url": "https://www.semanticscholar.org/paper/Zero-shot-recognition-with-unreliable-attributes-Jayaraman-Grauman/018e730f8947173e1140210d4d1760d05c9d3854", "title": "Zero-shot recognition with unreliable attributes", "abstract": "This work proposes a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions, and obtains more robust discriminative models for the unseen classes by leveraging statistics about each attribute's error tendencies. In principle, zero-shot learning makes it possible to train a recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses\u2014even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute's error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.", "publication_year": "2014", "authors": ["Dinesh Jayaraman", "Kristen Grauman"], "related_topics": ["Computer Science"], "references": ["9bc0295460089592d04e754a5fd427060b7bfa8c", "88e090ffc1f75eed720b5afb167523eb2e316f7f", "23e568fcf0192e4ff5e6bed7507ee5b9e6c43598", "d6714ee0a3c3c5ead3d681d4bec8e60f042928ef", "2198f4130c4850ffebe52d5eeaefc61e15b60426", "5e470f5320ae9920b597422dfae5d5e1eadbf55e", "caa632d101a41a7860562e4399a5eaa9a4088b55", "4aa4069693bee00d1b0759ca3df35e59284e9845", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "54aacc196ffe49b3450059fccdf7cd3bb6f6f3c3"], "references_count": 32, "citations_count": 281}, {"id": "6cd5fc1f0a63df570d3bccb33bf300791574e06f", "url": "https://www.semanticscholar.org/paper/Label-Embedding-for-Image-Classification-Akata-Perronnin/6cd5fc1f0a63df570d3bccb33bf300791574e06f", "title": "Label-Embedding for Image Classification", "abstract": "This work proposes to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors, and introduces a function that measures the compatibility between an image and a label embedding. Attributes act as intermediate representations that enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function that measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes, such as, e.g., class hierarchies or textual descriptions. Moreover, label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples.", "publication_year": "2015", "authors": ["Zeynep Akata", "Florent Perronnin", "Za{\\\"i"], "related_topics": ["Computer Science"], "references": ["caa632d101a41a7860562e4399a5eaa9a4088b55", "caccc069e658ea397c9faf673e74c959c734ff53", "54aacc196ffe49b3450059fccdf7cd3bb6f6f3c3", "88e090ffc1f75eed720b5afb167523eb2e316f7f", "755e9f43ce398ae8737366720c5f82685b0c253e", "4aa4069693bee00d1b0759ca3df35e59284e9845", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "a3ea706f6604a1e6e87c33d7a3b4b97b1bb338ef", "3a4a53fe47036ac89dad070ab87a9d8795b139b1", "2198f4130c4850ffebe52d5eeaefc61e15b60426"], "references_count": 74, "citations_count": 585}, {"id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "url": "https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need", "abstract": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "publication_year": "2017", "authors": ["Ashish Vaswani", "Noam M. Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "related_topics": ["Computer Science"], "references": ["b60abe57bc195616063be10638c6437358c81d1e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "98445f4172659ec5e891e031d8202c102135c644", "032274e57f7d8b456bd255fe76b909b2c1d7458e", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "13d9323a8716131911bfda048a40e2cde1a76a46", "d76c07211479e233f7c6a6f32d5346c983c5598f", "43428880d75b3a14257c3ee9bda054e61eb869c0", "510e26733aaff585d65701b9f1be7ca9d5afc586"], "references_count": 39, "citations_count": 60168}, {"id": "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "url": "https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59", "title": "Dissecting Contextual Word Embeddings: Architecture and Representation", "abstract": "There is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks, suggesting that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated. Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.", "publication_year": "2018", "authors": ["Matthew E. Peters", "Mark Neumann", "Luke Zettlemoyer", "Wen-tau Yih"], "related_topics": ["Computer Science"], "references": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "efef34c1caef102ad5cc052642d75beaaf5adcaf", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "82364428995c29b3dcb60c1835548eeff4adcd20", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "7cdb46dd8ba4440a8e3859a001fd38da93fbba4a", "3aa52436575cf6768a0a1a476601825f6a62e58f"], "references_count": 56, "citations_count": 307}, {"id": "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "url": "https://www.semanticscholar.org/paper/Semi-Supervised-Sequence-Modeling-with-Cross-View-Clark-Luong/0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "title": "Semi-Supervised Sequence Modeling with Cross-View Training", "abstract": "Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data, is proposed and evaluated, achieving state-of-the-art results. Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.", "publication_year": "2018", "authors": ["Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc V. Le"], "related_topics": ["Computer Science"], "references": ["7647a06965d868a4f6451bef0818994100a142e8", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "85f94d8098322f8130512b4c6c4627548ce4a6cc", "afc2850945a871e72c245818f9bc141bd659b453", "ac17cfa150d802750b46220084d850cfdb64d1c1", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "cea967b59209c6be22829699f05b8b1ac4dc092d", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "d76c07211479e233f7c6a6f32d5346c983c5598f"], "references_count": 89, "citations_count": 296}, {"id": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "url": "https://www.semanticscholar.org/paper/Semi-supervised-sequence-tagging-with-bidirectional-Peters-Ammar/0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "title": "Semi-supervised sequence tagging with bidirectional language models", "abstract": "A general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers. Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "publication_year": "2017", "authors": ["Matthew E. Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power"], "related_topics": ["Computer Science"], "references": ["59761abc736397539bdd01ad7f9d91c8607c0457", "7ece4e8d31f872d928369ac2cf58a616a7182112", "b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "bc1022b031dc6c7019696492e8116598097a8c12", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "26e743d5bd465f49b9538deaf116c15e61b7951f", "ade0c116120b54b57a91da51235108b75c28375a", "189e6bb7523733c4e524214b9e6ae92d4ed50dac", "2c821e2ec8ef976d3abb36fb0dc1946f04208512"], "references_count": 46, "citations_count": 568}, {"id": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "url": "https://www.semanticscholar.org/paper/Character-Level-Language-Modeling-with-Deeper-Al-Rfou-Choe/b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention", "abstract": "This paper shows that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.", "publication_year": "2018", "authors": ["Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones"], "related_topics": ["Computer Science"], "references": ["88caa4a0253a8b0076176745ebc072864eab66e1", "58c6f890a1ae372958b7decf56132fe258152722", "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "f9a1b3850dfd837793743565a8af95973d395a4e", "4db8cd9117254d21c9c828b8ba2aea58e57ee2c4", "27981998aaef92952eabef2c1490b926f9150c4f", "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105"], "references_count": 53, "citations_count": 284}, {"id": "8c1b00128e74f1cd92aede3959690615695d5101", "url": "https://www.semanticscholar.org/paper/QANet%3A-Combining-Local-Convolution-with-Global-for-Yu-Dohan/8c1b00128e74f1cd92aede3959690615695d5101", "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension", "abstract": "A new Q\\\\&A architecture called QANet is proposed, which does not require recurrent networks, and its encoder consists exclusively of convolution and self-attention, where convolution models local interactions andSelf-att attention models global interactions. Current end-to-end machine reading and question answering (Q\\\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\\\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.", "publication_year": "2018", "authors": ["Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V. Le"], "related_topics": ["Computer Science"], "references": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "e94697b98b707f557436e025bdc8498fa261d3bc", "93499a7c7f699b6630a86fad964536f9423bb6d0", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "adc276e6eae7051a027a4c269fb21dae43cadfed", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "de0c30321b22c56d637e7c29cb59180f157272a8", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "12e20e4ea572dbe476fd894c5c9a9930cf250dd2", "c25a67ad7e8629a9d12b9e2fc356cd73af99a060"], "references_count": 49, "citations_count": 949}, {"id": "93b8da28d006415866bf48f9a6e06b5242129195", "url": "https://www.semanticscholar.org/paper/GLUE%3A-A-Multi-Task-Benchmark-and-Analysis-Platform-Wang-Singh/93b8da28d006415866bf48f9a6e06b5242129195", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.", "publication_year": "2018", "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "related_topics": ["Computer Science"], "references": ["ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc", "ade0c116120b54b57a91da51235108b75c28375a", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "e242ba1a62eb2595d89afbec2657f33d9ab4abe3", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "93b4cc549a1bc4bc112189da36c318193d05d806", "afc2850945a871e72c245818f9bc141bd659b453", "4840754f50492e722f897a0aec178e9d3f8a3719"], "references_count": 63, "citations_count": 3970}, {"id": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "url": "https://www.semanticscholar.org/paper/Skip-Thought-Vectors-Kiros-Zhu/6e795c6e9916174ae12349f5dc3f516570c17ce8", "title": "Skip-Thought Vectors", "abstract": "The approach for unsupervised learning of a generic, distributed sentence encoder is described, using the continuity of text from books to train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.", "publication_year": "2015", "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "related_topics": ["Computer Science"], "references": ["27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "cea967b59209c6be22829699f05b8b1ac4dc092d", "687bac2d3320083eb4530bf18bb8f8f721477600", "d41cfe9b2ada4e09d53262bc75c473d8043936fc", "0b544dfe355a5070b60986319a3f51fb45d1348e", "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "944a1cfd79dbfb6fef460360a0765ba790f4027a"], "references_count": 43, "citations_count": 2122}, {"id": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "url": "https://www.semanticscholar.org/paper/MaskGAN%3A-Better-Text-Generation-via-Filling-in-the-Fedus-Goodfellow/7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "title": "MaskGAN: Better Text Generation via Filling in the ______", "abstract": "This work introduces an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context and shows qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model. Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.", "publication_year": "2018", "authors": ["William Fedus", "Ian J. Goodfellow", "Andrew M. Dai"], "related_topics": ["Computer Science"], "references": ["a8176a160777bfe82b1c67506835c60073e6fbe8", "bad429f1fff54bff3d20cde79651fec2eb805a7c", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "2966ecd82505ecd55ead0e6a327a304c8f9868e3", "0d16298285eb347bf951b302e6f2c8e4dc472253", "cea967b59209c6be22829699f05b8b1ac4dc092d", "db38edba294b7d2fd8ca3aad65721bd9dce32619", "424aef7340ee618132cc3314669400e23ad910ba", "67d968c7450878190e45ac7886746de867bf673d"], "references_count": 36, "citations_count": 428}, {"id": "421fc2556836a6b441de806d7b393a35b6eaea58", "url": "https://www.semanticscholar.org/paper/Contextual-String-Embeddings-for-Sequence-Labeling-Akbik-Blythe/421fc2556836a6b441de806d7b393a35b6eaea58", "title": "Contextual String Embeddings for Sequence Labeling", "abstract": "This paper proposes to leverage the internal states of a trained character language model to produce a novel type of word embedding which they refer to as contextual string embeddings, which are fundamentally model words as sequences of characters and are contextualized by their surrounding text. Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair", "publication_year": "2018", "authors": ["A. Akbik", "Duncan A. J. Blythe", "Roland Vollgraf"], "related_topics": ["Computer Science"], "references": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "32ece3fe025b43d44086ebf4141e09786ceecb7e", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "7647a06965d868a4f6451bef0818994100a142e8", "94a178bc81d045bbc7ff6bb83738c2491c3c9985", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "664ec878de4b7170712baae4a7821fc2602bba25", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa"], "references_count": 33, "citations_count": 1082}, {"id": "96acb1c882ad655c6b8459c2cd331803801446ca", "url": "https://www.semanticscholar.org/paper/Representation-Learning-of-Knowledge-Graphs-with-Xie-Liu/96acb1c882ad655c6b8459c2cd331803801446ca", "title": "Representation Learning of Knowledge Graphs with Entity Descriptions", "abstract": "Experimental results on real-world datasets show that, the proposed novel RL method for knowledge graphs outperforms other baselines on the two tasks, especially under the zero-shot setting, which indicates that the method is capable of building representations for novel entities according to their descriptions. \\n \\n Representation learning (RL) of knowledge graphs aims to project both entities and relations into a continuous low-dimensional space. Most methods concentrate on learning representations with knowledge triples indicating relations between entities. In fact, in most knowledge graphs there are usually concise descriptions for entities, which cannot be well utilized by existing methods. In this paper, we propose a novel RL method for knowledge graphs taking advantages of entity descriptions. More specifically, we explore two encoders, including continuous bag-of-words and deep convolutional neural models to encode semantics of entity descriptions. We further learn knowledge representations with both triples and descriptions. We evaluate our method on two tasks, including knowledge graph completion and entity classification. Experimental results on real-world datasets show that, our method outperforms other baselines on the two tasks, especially under the zero-shot setting, which indicates that our method is capable of building representations for novel entities according to their descriptions. The source code of this paper can be obtained from https://github.com/xrb92/DKRL.\\n \\n", "publication_year": "2016", "authors": ["Ruobing Xie", "Zhiyuan Liu", "Jia Jia", "Huanbo Luan", "Maosong Sun"], "related_topics": ["Computer Science"], "references": ["994afdf0db0cb0456f4f76468380822c2f532726", "86412306b777ee35aba71d4795b02915cb8a04c3", "aa1b05e8449eb5ee93b114453d9c946ae00459b1", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "085e4ab0164e13464b183d3430021f74a9df673a", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "af2e6165b68e75c911dfdb8f81f9ab6627722ab7", "318b558717ff9a4a996e45368b26a1233f03d1d7", "4e278a0fe9fbfeceb29acde435706aa790aeda56"], "references_count": 18, "citations_count": 550}, {"id": "cab46caf83a9e0390c6ca4d8603187969c9a53ad", "url": "https://www.semanticscholar.org/paper/DOLORES%3A-Deep-Contextualized-Knowledge-Graph-Wang-Kulkarni/cab46caf83a9e0390c6ca4d8603187969c9a53ad", "title": "DOLORES: Deep Contextualized Knowledge Graph Embeddings", "abstract": "This work introduces a new method DOLORES for learning knowledge graph embeddings that effectively captures contextual cues and dependencies among entities and relations and shows that these representations can very easily be incorporated into existing models to significantly advance the state of the art on several knowledge graph prediction tasks. We introduce a new method DOLORES for learning knowledge graph embeddings that effectively captures contextual cues and dependencies among entities and relations. First, we note that short paths on knowledge graphs comprising of chains of entities and relations can encode valuable information regarding their contextual usage. We operationalize this notion by representing knowledge graphs not as a collection of triples but as a collection of entity-relation chains, and learn embeddings for entities and relations using deep neural models that capture such contextual usage. In particular, our model is based on Bi-Directional LSTMs and learn deep representations of entities and relations from constructed entity-relation chains. We show that these representations can very easily be incorporated into existing models to significantly advance the state of the art on several knowledge graph prediction tasks like link prediction, triple classification, and missing relation type prediction (in some cases by at least 9.5%).", "publication_year": "2018", "authors": ["Haoyu Wang", "Vivek Kulkarni", "William Yang Wang"], "related_topics": ["Computer Science"], "references": ["994afdf0db0cb0456f4f76468380822c2f532726", "aa1b05e8449eb5ee93b114453d9c946ae00459b1", "2b828ff9ab979bfd3b5558d15b95d1c56a70012c", "e379f7c85441df5d8ddc1565cabf4b4290c22f1f", "ae64d2b8deec906a8ae4138a696388032efa9e3b", "af2e6165b68e75c911dfdb8f81f9ab6627722ab7", "86412306b777ee35aba71d4795b02915cb8a04c3", "9697d32ed0a16da167f2bdba05ef96d0da066eb5", "1522df73ddd64a308ac2a900b29c6e3dd1c16941", "2e61ddfa317a197e27ed90d4eab3a19882fe3e8e"], "references_count": 39, "citations_count": 23}, {"id": "bd345877856dc83c2c10c125dbf0f41e2bde38b1", "url": "https://www.semanticscholar.org/paper/Knowledge-Graph-Representation-with-Jointly-and-Xu-Qiu/bd345877856dc83c2c10c125dbf0f41e2bde38b1", "title": "Knowledge Graph Representation with Jointly Structural and Textual Encoding", "abstract": "This paper introduces three neural models to encode the valuable information from text description of entity, among which an attentive model can select related information as needed, and proposes a novel deep architecture to utilize both structural and textual information of entities. The objective of knowledge graph embedding is to encode both entities and relations of knowledge graphs into continuous low-dimensional vector spaces. Previously, most works focused on symbolic representation of knowledge graph with structure information, which can not handle new entities or entities with few facts well. In this paper, we propose a novel deep architecture to utilize both structural and textual information of entities. Specifically, we introduce three neural models to encode the valuable information from text description of entity, among which an attentive model can select related information as needed. Then, a gating mechanism is applied to integrate representations of structure and text into a unified architecture. Experiments show that our models outperform baseline and obtain state-of-the-art results on link prediction and triplet classification tasks.", "publication_year": "2016", "authors": ["Jiacheng Xu", "Xipeng Qiu", "Kan Chen", "Xuanjing Huang"], "related_topics": ["Computer Science"], "references": ["6dd3b79f34a8b40320d1d745b9abf2d70e1d4db8", "96acb1c882ad655c6b8459c2cd331803801446ca", "c0fb4f62c39ad91ae6a884a6ad5ebe79517646a1", "7e928ef936c2815d7522c5176163d6ab7309a8b7", "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "d6e5c0cabb07081e750d6426b649978584918216", "085e4ab0164e13464b183d3430021f74a9df673a", "318b558717ff9a4a996e45368b26a1233f03d1d7", "994afdf0db0cb0456f4f76468380822c2f532726", "18bd7cd489874ed9976b4f87a6a558f9533316e0"], "references_count": 49, "citations_count": 111}, {"id": "17a1e5d78bffb17979ac55aa792698727fe25a21", "url": "https://www.semanticscholar.org/paper/Representation-Learning-of-Knowledge-Graphs-with-Xie-Liu/17a1e5d78bffb17979ac55aa792698727fe25a21", "title": "Representation Learning of Knowledge Graphs with Hierarchical Types", "abstract": "Experimental results show that the proposed Type-embodied Knowledge Representation Learning models significantly outperform all baselines on both tasks, especially with long-tail distribution, and indicates that the models are capable of capturing hierarchical type information which is significant when constructing representations of knowledge graphs. Representation learning of knowledge graphs aims to encode both entities and relations into a continuous low-dimensional vector space. Most existing methods only concentrate on learning representations with structured information located in triples, regardless of the rich information located in hierarchical types of entities, which could be collected in most knowledge graphs. In this paper, we propose a novel method named Type-embodied Knowledge Representation Learning (TKRL) to take advantages of hierarchical entity types. We suggest that entities should have multiple representations in different types. More specifically, we consider hierarchical types as projection matrices for entities, with two type encoders designed to model hierarchical structures. Meanwhile, type information is also utilized as relation-specific type constraints. We evaluate our models on two tasks including knowledge graph completion and triple classification, and further explore the performances on long-tail dataset. Experimental results show that our models significantly outperform all baselines on both tasks, especially with long-tail distribution. It indicates that our models are capable of capturing hierarchical type information which is significant when constructing representations of knowledge graphs. The source code of this paper can be obtained from https://github.com/thunlp/TKRL.", "publication_year": "2016", "authors": ["Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"], "related_topics": ["Computer Science"], "references": ["96acb1c882ad655c6b8459c2cd331803801446ca", "f86e65797301b7e35aec66672a320a1697018924", "aa1b05e8449eb5ee93b114453d9c946ae00459b1", "86412306b777ee35aba71d4795b02915cb8a04c3", "994afdf0db0cb0456f4f76468380822c2f532726", "18bd7cd489874ed9976b4f87a6a558f9533316e0", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "62754e946c454af793085e8b761e21c4fc68827b", "e745b0506f4133263633eb05e5006a8cff4129f0", "473b3f2cc2c942c0116d980fe5b36a338f6017de"], "references_count": 27, "citations_count": 248}, {"id": "3ce14b7a3c1b89c717eba10229d9d80d80bd0e04", "url": "https://www.semanticscholar.org/paper/Accurate-Text-Enhanced-Knowledge-Graph-Learning-An-Chen/3ce14b7a3c1b89c717eba10229d9d80d80bd0e04", "title": "Accurate Text-Enhanced Knowledge Graph Representation Learning", "abstract": "This work proposes an accurate text-enhanced knowledge graph representation learning method, which can represent a relation/entity with different representations in different triples by exploiting additional textual information. Previous representation learning techniques for knowledge graph representation usually represent the same entity or relation in different triples with the same representation, without considering the ambiguity of relations and entities. To appropriately handle the semantic variety of entities/relations in distinct triples, we propose an accurate text-enhanced knowledge graph representation learning method, which can represent a relation/entity with different representations in different triples by exploiting additional textual information. Specifically, our method enhances representations by exploiting the entity descriptions and triple-specific relation mention. And a mutual attention mechanism between relation mention and entity description is proposed to learn more accurate textual representations for further improving knowledge graph representation. Experimental results show that our method achieves the state-of-the-art performance on both link prediction and triple classification tasks, and significantly outperforms previous text-enhanced knowledge representation models.", "publication_year": "2018", "authors": ["Bo An", "Bo Chen", "Xianpei Han", "Le Sun"], "related_topics": ["Computer Science"], "references": ["bd345877856dc83c2c10c125dbf0f41e2bde38b1", "aa1b05e8449eb5ee93b114453d9c946ae00459b1", "ea5907c9b0742baa2593d3abf99b7d0084a902a9", "96acb1c882ad655c6b8459c2cd331803801446ca", "7e928ef936c2815d7522c5176163d6ab7309a8b7", "994afdf0db0cb0456f4f76468380822c2f532726", "67cab3bafc8fa9e1ae3ff89791ad43c81441d271", "0dddf37145689e5f2899f8081d9971882e6ff1e9", "d2072e4bc03c82697be667c265d728045712bc46", "318b558717ff9a4a996e45368b26a1233f03d1d7"], "references_count": 35, "citations_count": 57}, {"id": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f", "url": "https://www.semanticscholar.org/paper/SSP%3A-Semantic-Space-Projection-for-Knowledge-Graph-Xiao-Huang/e379f7c85441df5d8ddc1565cabf4b4290c22f1f", "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions", "abstract": "This paper proposes the semantic space projection (SSP) model, a model which jointly learns from the symbolic triples and textual descriptions to discover semantic relevance and offer precise semantic embedding. \\n \\n Knowledge graph embedding represents entities and relations in knowledge graph as low-dimensional, continuous vectors, and thus enables knowledge graph compatible with machine learning models. Though there have been a variety of models for knowledge graph embedding, most methods merely concentrate on the fact triples, while supplementary textual descriptions of entities and relations have not been fully employed. To this end, this paper proposes the semantic space projection (SSP) model which jointly learns from the symbolic triples and textual descriptions. Our model builds interaction between the two information sources, and employs textual descriptions to discover semantic relevance and offer precise semantic embedding. Extensive experiments show that our method achieves substantial improvements against baselines on the tasks of knowledge graph completion and entity classification.\\n \\n", "publication_year": "2016", "authors": ["Han Xiao", "Minlie Huang", "Lian Meng", "Xiaoyan Zhu"], "related_topics": ["Computer Science"], "references": ["d1a525c16a53b94200029df1037f2c9c7c244d7b", "67cab3bafc8fa9e1ae3ff89791ad43c81441d271", "96acb1c882ad655c6b8459c2cd331803801446ca", "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "d42d2a4112e0a751424624ac0b78980fa9fe9d96", "0dddf37145689e5f2899f8081d9971882e6ff1e9", "18bd7cd489874ed9976b4f87a6a558f9533316e0", "994afdf0db0cb0456f4f76468380822c2f532726", "aa1b05e8449eb5ee93b114453d9c946ae00459b1", "69418ff5d4eac106c72130e152b807004e2b979c"], "references_count": 33, "citations_count": 167}, {"id": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271", "url": "https://www.semanticscholar.org/paper/TransG-%3A-A-Generative-Model-for-Knowledge-Graph-Xiao-Huang/67cab3bafc8fa9e1ae3ff89791ad43c81441d271", "title": "TransG : A Generative Model for Knowledge Graph Embedding", "abstract": "This paper proposes a novel generative model (TransG) to address the issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples. Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper proposes a novel generative model (TransG) to address the issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples. The new model can discover latent semantics for a relation and leverage a mixture of relationspecific component vectors to embed a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, and at the first time, the issue of multiple relation semantics is formally discussed. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.", "publication_year": "2016", "authors": ["Han Xiao", "Minlie Huang", "Xiaoyan Zhu"], "related_topics": ["Computer Science"], "references": ["0dddf37145689e5f2899f8081d9971882e6ff1e9", "994afdf0db0cb0456f4f76468380822c2f532726", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "aa1b05e8449eb5ee93b114453d9c946ae00459b1", "991b64748dfeecf026a27030c16fe1743aa20167", "02e2059c328bd9fad4e676266435199663bed804", "1bb60eff4965b9dca7917808395769b952806017", "2a3f862199883ceff5e3c74126f0c80770653e05", "69418ff5d4eac106c72130e152b807004e2b979c", "473b3f2cc2c942c0116d980fe5b36a338f6017de"], "references_count": 31, "citations_count": 241}, {"id": "aa1b05e8449eb5ee93b114453d9c946ae00459b1", "url": "https://www.semanticscholar.org/paper/Modeling-Relation-Paths-for-Representation-Learning-Lin-Liu/aa1b05e8449eb5ee93b114453d9c946ae00459b1", "title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "abstract": "This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, it design a path-constraint resource allocation algorithm to measure the reliability of relation paths and (2) represents relation paths via semantic composition of relation embeddings. Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns between entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allocation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as compared with baselines, our model achieves significant and consistent improvements on knowledge base completion and relation extraction from text. The source code of this paper can be obtained from https://github.com/mrlyk423/ relation_extraction.", "publication_year": "2015", "authors": ["Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "related_topics": ["Computer Science"], "references": ["8c68094a59dd2f24415082c53464abf45387f0bb", "994afdf0db0cb0456f4f76468380822c2f532726", "834cb8e1e738b8d2c6d24e652ac966d6e7089a46", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "974d1048fa45227a3ef9f71efe5501f79683dfdf"], "references_count": 33, "citations_count": 539}, {"id": "0fa5142f908afc94c923ca2adbe14a5673bc76eb", "url": "https://www.semanticscholar.org/paper/A-Neural-Knowledge-Language-Model-Ahn-Choi/0fa5142f908afc94c923ca2adbe14a5673bc76eb", "title": "A Neural Knowledge Language Model", "abstract": "A Neural Knowledge Language Model (NKLM) which combines symbolic knowledge provided by a knowledge graph with the RNN language model, and shows that the NKLM significantly improves the perplexity while generating a much smaller number of unknown words. Current language models have significant limitations in their ability to encode and decode knowledge. This is mainly because they acquire knowledge based on statistical co-occurrences, even if most of the knowledge words are rarely observed named entities. In this paper, we propose a Neural Knowledge Language Model (NKLM) which combines symbolic knowledge provided by a knowledge graph with the RNN language model. At each time step, the model predicts a fact on which the observed word is to be based. Then, a word is either generated from the vocabulary or copied from the knowledge graph. We train and test the model on a new dataset, WikiFacts. In experiments, we show that the NKLM significantly improves the perplexity while generating a much smaller number of unknown words. In addition, we demonstrate that the sampled descriptions include named entities which were used to be the unknown words in RNN language models.", "publication_year": "2016", "authors": ["Sungjin Ahn", "Heeyoul Choi", "Tanel P{\\\"a"], "related_topics": ["Computer Science"], "references": ["aa5b35dcf8b024f5352db73cc3944e8fad4f3793", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "1f4a4769e4d2fb846e59c2f185e0377190739f18", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "efbd381493bb9636f489b965a2034d529cd56bcd", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "71ae756c75ac89e2d731c9c79649562b5768ff39", "50d53cc562225549457cbc782546bfbe1ac6f0cf"], "references_count": 51, "citations_count": 117}, {"id": "8cb592fa5e30e6fa5abe7041767768964f1f8cf4", "url": "https://www.semanticscholar.org/paper/Do-Language-Models-Have-Common-Sense-Trinh-Le/8cb592fa5e30e6fa5abe7041767768964f1f8cf4", "title": "Do Language Models Have Common Sense", "abstract": "Surprising evidence is shown that language models can already learn to capture certain common sense knowledge and that a language model can compute the probability of any statement, and this probability can be used to evaluate the truthfulness of that statement. It has been argued that current machine learning models do not have common sense, and therefore must be hard-coded with prior knowledge (Marcus, 2018). Here we show surprising evidence that language models can already learn to capture certain common sense knowledge. Our key observation is that a language model can compute the probability of any statement, and this probability can be used to evaluate the truthfulness of that statement. On the Winograd Schema Challenge (Levesque et al., 2011), language models are 11% higher in accuracy than previous state-of-the-art supervised methods. Language models can also be fine-tuned for the task of Mining Commonsense Knowledge on ConceptNet to achieve an F1 score of 0.912 and 0.824, outperforming previous best results (Jastrzebski et al., 2018). Further analysis demonstrates that language models can discover unique features of Winograd Schema contexts that decide the correct answers without explicit supervision.", "publication_year": "2018", "authors": ["Trieu H. Trinh", "Quoc V. Le"], "related_topics": ["Computer Science"], "references": ["85b68477a6e031d88b963833e15a4b4fc6855264", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "3febb2bed8865945e7fddc99efd791887bb7e14f", "edfc63d7ae5c1552c1c57606eca990e247b2942f", "77fb0b7aef619dfac650423d4677170df2158e0d", "e86e81ad3fa4ab0b736f7fef721689e293ee788e", "85f94d8098322f8130512b4c6c4627548ce4a6cc", "c319af92127378e7ee64ec6a3e2e8752fe1421c7", "96743201dc771df1829f061c2648fd0ee1a70e59", "17c6d7e79b566279afb869fee262467e6370f43a"], "references_count": 31, "citations_count": 14}, {"id": "9405cc0d6169988371b2755e573cc28650d14dfe", "url": "https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners", "abstract": "It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations. Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.", "publication_year": "2019", "authors": ["Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever"], "related_topics": ["Computer Science"], "references": ["cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "3bbf2ee642ed311e500017def1f54df453a935c1", "93b8da28d006415866bf48f9a6e06b5242129195", "afc2850945a871e72c245818f9bc141bd659b453", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "19281b9ecdb5c07a93423a506627ab9d9b0cf039", "cea967b59209c6be22829699f05b8b1ac4dc092d", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "85f94d8098322f8130512b4c6c4627548ce4a6cc"], "references_count": 75, "citations_count": 10526}, {"id": "26e9eb44ed8065122d37b0c429a8d341bfeea9a5", "url": "https://www.semanticscholar.org/paper/Reference-Aware-Language-Models-Yang-Blunsom/26e9eb44ed8065122d37b0c429a8d341bfeea9a5", "title": "Reference-Aware Language Models", "abstract": "Experiments on three representative applications show the coreference model variants outperform models based on deterministic attention and standard language modeling baselines. We propose a general class of language models that treat reference as discrete stochastic latent variables. This decision allows for the creation of entity mentions by accessing external databases of referents (required by, e.g., dialogue generation) or past internal state (required to explicitly model coreferentiality). Beyond simple copying, our coreference model can additionally refer to a referent using varied mention forms (e.g., a reference to \u201cJane\u201d can be realized as \u201cshe\u201d), a characteristic feature of reference in natural languages. Experiments on three representative applications show our model variants outperform models based on deterministic attention and standard language modeling baselines.", "publication_year": "2016", "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "related_topics": ["Computer Science"], "references": ["7719679e6255c51d157116fcfbc858b7e7dfdb59", "77770099cd73e6da90f046ac92fa2f9d32e469f6", "0fa5142f908afc94c923ca2adbe14a5673bc76eb", "6c5325c2b67bf88f2b846cf5a6df6c2e6362d75b", "e8e76b1062918624e9904e0073e11794d7594593", "aa5b35dcf8b024f5352db73cc3944e8fad4f3793", "efbd381493bb9636f489b965a2034d529cd56bcd", "e957747f4f8600940be4c5bb001aa70c84e53a53", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "4d4b46e545e1a3f6871b49cc69640ef2eb1a4654"], "references_count": 32, "citations_count": 76}, {"id": "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "url": "https://www.semanticscholar.org/paper/A-Neural-Conversational-Model-Vinyals-Le/85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "title": "A Neural Conversational Model", "abstract": "A simple approach to conversational modeling which uses the recently proposed sequence to sequence framework, and is able to extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.", "publication_year": "2015", "authors": ["Oriol Vinyals", "Quoc V. Le"], "related_topics": ["Computer Science"], "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "1956c239b3552e030db1b78951f64781101125ed", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5247a6e3a60ff0381355e66bfc313bf27512ae0c"], "references_count": 20, "citations_count": 1628}, {"id": "ef9ddbc35676ce8ffc2a8067044473727839dbac", "url": "https://www.semanticscholar.org/paper/Breaking-the-Softmax-Bottleneck%3A-A-High-Rank-RNN-Yang-Dai/ef9ddbc35676ce8ffc2a8067044473727839dbac", "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model", "abstract": "It is shown that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck, and a simple and effective method is proposed to address this issue. We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.", "publication_year": "2017", "authors": ["Zhilin Yang", "Zihang Dai", "Ruslan Salakhutdinov", "William W. Cohen"], "related_topics": ["Computer Science"], "references": ["63e39cdf1ad884da6bc69096bb3413b5b1100559", "58c6f890a1ae372958b7decf56132fe258152722", "efbd381493bb9636f489b965a2034d529cd56bcd", "424aef7340ee618132cc3314669400e23ad910ba", "d1275b2a2ab53013310e759e5c6878b96df643d4", "f0b6c1ffed9984317050d0c1dfb005cb65582f13", "ccbd9da8f26337309c45b6ed97ab4715147dfba0", "fa9decd1395cc2f39e9921f870ebc8a8ec2bd08d", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "5d833331b0e22ff359db05c62a8bca18c4f04b68"], "references_count": 43, "citations_count": 311}, {"id": "efbd381493bb9636f489b965a2034d529cd56bcd", "url": "https://www.semanticscholar.org/paper/Pointer-Sentinel-Mixture-Models-Merity-Xiong/efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models", "abstract": "The pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank while using far fewer parameters than a standard softmax LSTM and the freely available WikiText corpus is introduced. Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.", "publication_year": "2016", "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "related_topics": ["Computer Science"], "references": ["d1275b2a2ab53013310e759e5c6878b96df643d4", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "aa5b35dcf8b024f5352db73cc3944e8fad4f3793", "e44da7d8c71edcc6e575fa7faadd5e75785a7901", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "e957747f4f8600940be4c5bb001aa70c84e53a53", "452059171226626718eb677358836328f884298e", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "2a76c2121eee30af82a24058b4e149f05bcda911"], "references_count": 29, "citations_count": 1397}, {"id": "58c6f890a1ae372958b7decf56132fe258152722", "url": "https://www.semanticscholar.org/paper/Regularizing-and-Optimizing-LSTM-Language-Models-Merity-Keskar/58c6f890a1ae372958b7decf56132fe258152722", "title": "Regularizing and Optimizing LSTM Language Models", "abstract": "This paper proposes the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization and introduces NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.", "publication_year": "2017", "authors": ["Stephen Merity", "Nitish Shirish Keskar", "Richard Socher"], "related_topics": ["Computer Science"], "references": ["d1275b2a2ab53013310e759e5c6878b96df643d4", "424aef7340ee618132cc3314669400e23ad910ba", "fc18e99f918d8906ec44be3f7d90d8f9ebabae96", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "f0b6c1ffed9984317050d0c1dfb005cb65582f13", "67d968c7450878190e45ac7886746de867bf673d", "efbd381493bb9636f489b965a2034d529cd56bcd", "2d876ed1dd2c58058d7197b734a8e4d349b8f231", "63e39cdf1ad884da6bc69096bb3413b5b1100559"], "references_count": 45, "citations_count": 990}, {"id": "2927dfc481446568fc9108795570eb4d416be021", "url": "https://www.semanticscholar.org/paper/Entity-Linking-via-Joint-Encoding-of-Types%2C-and-Gupta-Singh/2927dfc481446568fc9108795570eb4d416be021", "title": "Entity Linking via Joint Encoding of Types, Descriptions, and Context", "abstract": "This work presents a neural, modular entity linking system that learns a unified dense representation for each entity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types. For accurate entity linking, we need to capture various information aspects of an entity, such as its description in a KB, contexts in which it is mentioned, and structured knowledge. Additionally, a linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features. In this work we present a neural, modular entity linking system that learns a unified dense representation for each entity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types. We show that the resulting entity linking system is effective at combining these sources, and performs competitively, sometimes out-performing current state-of-the-art systems across datasets, without requiring any domain-specific training data or hand-engineered features. We also show that our model can effectively \u201cembed\u201d entities that are new to the KB, and is able to link its mentions accurately.", "publication_year": "2017", "authors": ["Nitish Gupta", "Sameer Singh", "Dan Roth"], "related_topics": ["Computer Science"], "references": ["43b239a996358af9463689c3dbb080104e28f337", "332b88bbf73a32bc238e09ee3d33b8c331317278", "3b294fb99aa967558befd9b0e2d6f925915080ae", "994afdf0db0cb0456f4f76468380822c2f532726", "c6b53dd64d79a59f49f261baac8d2581a29ca06a", "b4774c78477df60bc2391b2c36220e0970b2fded", "4988a269e9f61c6fd1da502e34648b93dfd1a54d", "eda32bba3e4e8a42d63e1d6a2648b8f831b782a4", "86412306b777ee35aba71d4795b02915cb8a04c3", "12f7b71324ee8e1796a9ef07af05b66674fe6af0"], "references_count": 37, "citations_count": 190}, {"id": "604764133befe7a0aaa692919545846197e6e065", "url": "https://www.semanticscholar.org/paper/Neural-Text-Generation-from-Structured-Data-with-to-Lebret-Grangier/604764133befe7a0aaa692919545846197e6e065", "title": "Neural Text Generation from Structured Data with Application to the Biography Domain", "abstract": "A neural model for concept-to-text generation that scales to large, rich domains and significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU is introduced. This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.", "publication_year": "2016", "authors": ["R{\\'e"], "related_topics": ["Computer Science"], "references": ["41de65f718207cc5f98b561a62b16d5e818cb98c", "f273adbfe0e6ba39a583b9669d94cc8d828d8c25", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "1956c239b3552e030db1b78951f64781101125ed", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d", "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "1a6b7cf5e1a3e069338498d1c17aa7d46c1ac7e9", "4f16c553b68a663585b0ee7d7a7b31d2da8e29a5", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "17708302b7e611608da42feda0f35fb8b92a9ee4"], "references_count": 44, "citations_count": 416}, {"id": "6fba4968f1b39d490bf95fe4030e3d385f167074", "url": "https://www.semanticscholar.org/paper/Machine-Learning-with-World-Knowledge%3A-The-Position-Song-Roth/6fba4968f1b39d490bf95fe4030e3d385f167074", "title": "Machine Learning with World Knowledge: The Position and Survey", "abstract": "This paper starts from the comparison of world knowledge with domain-specific knowledge, and introduces three key problems in using world knowledge in learning processes, i.e., explicit and implicit feature representation, inference for knowledge linking and disambiguation, and learning with direct or indirect supervision. Machine learning has become pervasive in multiple domains, impacting a wide variety of applications, such as knowledge discovery and data mining, natural language processing, information retrieval, computer vision, social and health informatics, ubiquitous computing, etc. Two essential problems of machine learning are how to generate features and how to acquire labels for machines to learn. Particularly, labeling large amount of data for each domain-specific problem can be very time consuming and costly. It has become a key obstacle in making learning protocols realistic in applications. In this paper, we will discuss how to use the existing general-purpose world knowledge to enhance machine learning processes, by enriching the features or reducing the labeling work. We start from the comparison of world knowledge with domain-specific knowledge, and then introduce three key problems in using world knowledge in learning processes, i.e., explicit and implicit feature representation, inference for knowledge linking and disambiguation, and learning with direct or indirect supervision. Finally we discuss the future directions of this research topic.", "publication_year": "2017", "authors": ["Yangqiu Song", "Dan Roth"], "related_topics": ["Computer Science"], "references": ["d1cbb4331d4983a23ad19943405fe087d488823c", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "2e396850373bb03e409b5686b956b2590bbc20fe", "cf5ea582bccc7cb21a2ebeb7a0987f79652bde8d", "c54f38857d25315ad1ca4024010cfd985d361e9b", "6df8cd4c69e75b286b1ba27417fd41a21d4982e1", "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "fb8f6c5670755a7d282fb9322bc8439492ea052a", "07abd02f02774d178f26ca99937e5f94001a9ec9"], "references_count": 250, "citations_count": 9}, {"id": "033f25ad905ef2ed32a8331cf38b83953ff15922", "url": "https://www.semanticscholar.org/paper/A-Review-of-Relational-Machine-Learning-for-Graphs-Nickel-Murphy/033f25ad905ef2ed32a8331cf38b83953ff15922", "title": "A Review of Relational Machine Learning for Knowledge Graphs", "abstract": "This paper provides a review of how statistical models can be \u201ctrained\u201d on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph) and how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be \u201ctrained\u201d on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's knowledge vault project as an example of such combination.", "publication_year": "2015", "authors": ["Maximilian Nickel", "Kevin P. Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "related_topics": ["Computer Science"], "references": ["5f8aaefa3c07563cb11884f3f227bd94431544ff", "6c71c34eb649c28288bf05d445d544aa15a5f82c", "81bbe42e3ec09c28b8864956148e58f4cb5aa860", "bafcfdb587decd9f1ccdc1ccc793a8035ef292d7", "c3cdd505ac569baf21e736aa4ca59b99174b15a2", "80416b542f4dab56e61999c62dacfc66e877706f", "473b3f2cc2c942c0116d980fe5b36a338f6017de", "46225772ac4f68a003a26f053bb248d77c7dbf87", "498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "32b12924aa35e0da4367d821f25e466b14f3189a"], "references_count": 156, "citations_count": 1329}, {"id": "50d53cc562225549457cbc782546bfbe1ac6f0cf", "url": "https://www.semanticscholar.org/paper/Reasoning-With-Neural-Tensor-Networks-for-Knowledge-Socher-Chen/50d53cc562225549457cbc782546bfbe1ac6f0cf", "title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "abstract": "An expressive neural tensor network suitable for reasoning over relationships between two entities given a subset of the knowledge base is introduced and performance can be improved when entities are represented as an average of their constituting word vectors. Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the \\\"Sumatran tiger\\\" and \\\"Bengal tiger.\\\" Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.", "publication_year": "2013", "authors": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "A. Ng"], "related_topics": ["Computer Science"], "references": ["1f4a4769e4d2fb846e59c2f185e0377190739f18", "687bac2d3320083eb4530bf18bb8f8f721477600", "27e38351e48fe4b7da2775bf94341738bc4da07e", "00a3f6924f90fcd77e6e7e6534b957a75d0ced07", "f2f72cfb48d15d4d2bd1e91a92e7f3ac8635d433", "473b3f2cc2c942c0116d980fe5b36a338f6017de", "e703e928bc07900527c368db2428d0d5c57148c2", "796918285116a29537489bb7dc1778f2b1f3e4e8", "57458bc1cffe5caa45a885af986d70f723f406b4", "81bbe42e3ec09c28b8864956148e58f4cb5aa860"], "references_count": 25, "citations_count": 1786}, {"id": "79baf8cf6be6510f69be8c515516136138678cf5", "url": "https://www.semanticscholar.org/paper/The-More-You-Know%3A-Using-Knowledge-Graphs-for-Image-Marino-Salakhutdinov/79baf8cf6be6510f69be8c515516136138678cf5", "title": "The More You Know: Using Knowledge Graphs for Image Classification", "abstract": "This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification, and introduces the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification.", "publication_year": "2016", "authors": ["Kenneth Marino", "Ruslan Salakhutdinov", "Abhinav Kumar Gupta"], "related_topics": ["Computer Science"], "references": ["b7d3fca013d5bb578c341383c219669fd2bf52a5", "9bc0295460089592d04e754a5fd427060b7bfa8c", "53e4ab9730e983242a3409c7bf1af945041a6563", "4deb435bd9ddd9db30909abe9a20e85c4eced5f1", "ea8fe33cc1596b2e493ddd87f22cd21f563664e8", "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "7c6de5a9e02a779e24504619050c6118f4eac181", "e49ff72d420c8d72e62a9353e3abc053445e59bd", "5e925a9f1e20df61d1e860a7aa71894b35a1c186", "eb42cf88027de515750f230b23b1a057dc782108"], "references_count": 46, "citations_count": 290}, {"id": "d77de3a4ddfa62f8105c0591fd41e549edcfd95f", "url": "https://www.semanticscholar.org/paper/TransG-%3A-A-Generative-Mixture-Model-for-Knowledge-Xiao-Huang/d77de3a4ddfa62f8105c0591fd41e549edcfd95f", "title": "TransG : A Generative Mixture Model for Knowledge Graph Embedding", "abstract": "This paper addresses a new issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, TransG, which can discover latent semantics for a relation and leverage a mixture of relation component vectors for embeddedding a fact triple. Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, TransG. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, which is able to deal with multiple relation semantics. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.", "publication_year": "2015", "authors": ["Han Xiao", "Minlie Huang", "Yu Hao", "Xiaoyan Zhu"], "related_topics": ["Computer Science"], "references": ["994afdf0db0cb0456f4f76468380822c2f532726", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "02e2059c328bd9fad4e676266435199663bed804", "0dddf37145689e5f2899f8081d9971882e6ff1e9", "aa1b05e8449eb5ee93b114453d9c946ae00459b1", "473b3f2cc2c942c0116d980fe5b36a338f6017de", "69418ff5d4eac106c72130e152b807004e2b979c", "2a3f862199883ceff5e3c74126f0c80770653e05", "9f7cbae1e5c23687b1197d273851048f4bd6fbfd", "f6764d853a14b0c34df1d2283e76277aead40fde"], "references_count": 32, "citations_count": 91}, {"id": "18bd7cd489874ed9976b4f87a6a558f9533316e0", "url": "https://www.semanticscholar.org/paper/Knowledge-Graph-Embedding-via-Dynamic-Mapping-Ji-He/18bd7cd489874ed9976b4f87a6a558f9533316e0", "title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "abstract": "A more fine-grained model named TransD, which is an improvement of TransR/CTransR, which not only considers the diversity of relations, but also entities, which makes it can be applied on large scale graphs. Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms state-of-the-art methods.", "publication_year": "2015", "authors": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao"], "related_topics": ["Computer Science"], "references": ["994afdf0db0cb0456f4f76468380822c2f532726", "2a3f862199883ceff5e3c74126f0c80770653e05", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "1f4a4769e4d2fb846e59c2f185e0377190739f18", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "473b3f2cc2c942c0116d980fe5b36a338f6017de", "033f25ad905ef2ed32a8331cf38b83953ff15922", "00a3f6924f90fcd77e6e7e6534b957a75d0ced07", "7e97c43a02fd7c8ce2c899ec477dcb7ac6e4c1f1"], "references_count": 21, "citations_count": 1158}, {"id": "06a73ad09664435f8b3cd90293f4e05a047cf375", "url": "https://www.semanticscholar.org/paper/K-BERT%3A-Enabling-Language-Representation-with-Graph-Liu-Zhou/06a73ad09664435f8b3cd90293f4e05a047cf375", "title": "K-BERT: Enabling Language Representation with Knowledge Graph", "abstract": "This work proposes a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge, which significantly outperforms BERT and reveals promising results in twelve NLP tasks. Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.", "publication_year": "2019", "authors": ["Weijie Liu", "Peng Zhou", "Zhe Zhao", "Zhiruo Wang", "Qi Ju", "Haotang Deng", "Ping Wang"], "related_topics": ["Computer Science"], "references": ["5f994dc8cae24ca9d1ed629e517fcc652660ddde", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "f48ae425e2567be2d993efcaaf74c2274fc9d7c5", "6dd3b79f34a8b40320d1d745b9abf2d70e1d4db8", "93b8da28d006415866bf48f9a6e06b5242129195", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "2c69bbb3b7ba3f324276924bab6f41de467c928a", "7e928ef936c2815d7522c5176163d6ab7309a8b7"], "references_count": 25, "citations_count": 446}, {"id": "bfeb827d06c1a3583b5cc6d25241203a81f6af09", "url": "https://www.semanticscholar.org/paper/Knowledge-Enhanced-Contextual-Word-Representations-Peters-Neumann/bfeb827d06c1a3583b5cc6d25241203a81f6af09", "title": "Knowledge Enhanced Contextual Word Representations", "abstract": "After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert\u2019s runtime is comparable to BERT\u2019s and it scales to large KBs.", "publication_year": "2019", "authors": ["Matthew E. Peters", "Mark Neumann", "IV RobertL.Logan", "Roy Schwartz", "Vidur Joshi", "Sameer Singh", "Noah A. Smith"], "related_topics": ["Computer Science"], "references": ["2927dfc481446568fc9108795570eb4d416be021", "f0462312d9e985f13fd20d65178f9565d967f07e", "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810", "59761abc736397539bdd01ad7f9d91c8607c0457", "28f3a20ebd5e2f3afa871b1784076cf7004415b8", "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "d6a13d8d168936a8947101d76fe060704d2f26ec", "d95738f38d97a030d98508357e4d5c78a4a208ba", "4af09143735210777281b66997ec12994dbb43d4"], "references_count": 71, "citations_count": 487}, {"id": "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "url": "https://www.semanticscholar.org/paper/Knowledge-Graph-and-Text-Jointly-Embedding-Wang-Zhang/f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "title": "Knowledge Graph and Text Jointly Embedding", "abstract": "Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. We examine the embedding approach to reason new relational facts from a largescale knowledge graph and a text corpus. We propose a novel method of jointly embedding entities and words into the same continuous vector space. The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus. Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space. Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be handled by previous embedding methods. At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).", "publication_year": "2014", "authors": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "related_topics": ["Computer Science"], "references": ["834cb8e1e738b8d2c6d24e652ac966d6e7089a46", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "2a3f862199883ceff5e3c74126f0c80770653e05", "1f4a4769e4d2fb846e59c2f185e0377190739f18", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "d84b57362e2010f6f65357267df7e0157af30684", "c2908deec5b09ea4bdfce7ad5e827606ef425ee4"], "references_count": 19, "citations_count": 361}, {"id": "70af3ee98c53441d9090119f7b76efb1b6d03edd", "url": "https://www.semanticscholar.org/paper/SimplE-Embedding-for-Link-Prediction-in-Knowledge-Kazemi-Poole/70af3ee98c53441d9090119f7b76efb1b6d03edd", "title": "SimplE Embedding for Link Prediction in Knowledge Graphs", "abstract": "It is proved SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity and shown empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE's code is available on GitHub at this https URL.", "publication_year": "2018", "authors": ["Seyed Mehran Kazemi", "David L. Poole"], "related_topics": ["Computer Science"], "references": ["ae64d2b8deec906a8ae4138a696388032efa9e3b", "322aa32b2a409d2e135dbb14736d9aeb497f1c52", "9697d32ed0a16da167f2bdba05ef96d0da066eb5", "a4dfb121275a6408d290b803baf8c9caeb23dc5b", "2cefe5adb11295b830ce27176c6d84b66fb20c2c", "994afdf0db0cb0456f4f76468380822c2f532726", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "cd8a9914d50b0ac63315872530274d158d6aff09", "2218e2e1df2c3adfb70e0def2e326a39928aacfc", "86412306b777ee35aba71d4795b02915cb8a04c3"], "references_count": 64, "citations_count": 490}, {"id": "6dd3b79f34a8b40320d1d745b9abf2d70e1d4db8", "url": "https://www.semanticscholar.org/paper/Joint-Representation-Learning-of-Text-and-Knowledge-Han-Liu/6dd3b79f34a8b40320d1d745b9abf2d70e1d4db8", "title": "Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion", "abstract": "This work proposes a novel framework to embed words, entities and relations into the same continuous vector space and shows that the model can significantly and consistently improve the performance on the three tasks as compared with other baselines. Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately. In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space. In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration. In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text. The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines.", "publication_year": "2016", "authors": ["Xu Han", "Zhiyuan Liu", "Maosong Sun"], "related_topics": ["Computer Science"], "references": ["7e928ef936c2815d7522c5176163d6ab7309a8b7", "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "834cb8e1e738b8d2c6d24e652ac966d6e7089a46", "994afdf0db0cb0456f4f76468380822c2f532726", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "054ba27fe5cc6085d20ea2707de886db6865dbed"], "references_count": 26, "citations_count": 40}, {"id": "994afdf0db0cb0456f4f76468380822c2f532726", "url": "https://www.semanticscholar.org/paper/Learning-Entity-and-Relation-Embeddings-for-Graph-Lin-Liu/994afdf0db0cb0456f4f76468380822c2f532726", "title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "abstract": "TransR is proposed to build entity and relation embeddings in separate entity space and relation spaces by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. \\n \\n Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH.\\n \\n", "publication_year": "2015", "authors": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "related_topics": ["Computer Science"], "references": ["e7e7b9a731678bf0494fe29cbebb42a822224cc6", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "2a3f862199883ceff5e3c74126f0c80770653e05", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "834cb8e1e738b8d2c6d24e652ac966d6e7089a46", "d84b57362e2010f6f65357267df7e0157af30684", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "473b3f2cc2c942c0116d980fe5b36a338f6017de"], "references_count": 23, "citations_count": 2519}, {"id": "f7b0d94fd4a32c4c9be472b4e8d6c5bc308f0dfa", "url": "https://www.semanticscholar.org/paper/Bridge-Text-and-Knowledge-by-Learning-Entity-Cao-Huang/f7b0d94fd4a32c4c9be472b4e8d6c5bc308f0dfa", "title": "Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding", "abstract": "A novel Multi-Prototype Mention Embedding model is proposed, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base, and an efficient language model based approach to disambiguate each mention to a specific sense. Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance.", "publication_year": "2017", "authors": ["Yixin Cao", "Lifu Huang", "Heng Ji", "Xu Chen", "Juan-Zi Li"], "related_topics": ["Computer Science"], "references": ["9a501e501a60b431b6031f81dc2c19b390b0aff3", "43b239a996358af9463689c3dbb080104e28f337", "d95738f38d97a030d98508357e4d5c78a4a208ba", "a9aa7531b35811348c1f8a174da9045b120a8af4", "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "ea5907c9b0742baa2593d3abf99b7d0084a902a9", "96c697be387566a0637941fc7492fcfc25ad56bb", "3b294fb99aa967558befd9b0e2d6f925915080ae", "12f7b71324ee8e1796a9ef07af05b66674fe6af0", "306fad753dc178a9d8168bec3500d89d80320317"], "references_count": 40, "citations_count": 88}, {"id": "9697d32ed0a16da167f2bdba05ef96d0da066eb5", "url": "https://www.semanticscholar.org/paper/Convolutional-2D-Knowledge-Graph-Embeddings-Dettmers-Minervini/9697d32ed0a16da167f2bdba05ef96d0da066eb5", "title": "Convolutional 2D Knowledge Graph Embeddings", "abstract": "ConvE, a multi-layer convolutional network model for link prediction, is introduced, and it is found that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets. \\n \\n Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models \u2014 which potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree \u2014 which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set \u2014 however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets \u2014 deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets.\\n \\n", "publication_year": "2017", "authors": ["Tim Dettmers", "Pasquale Minervini", "Pontus Stenetorp", "Sebastian Riedel"], "related_topics": ["Computer Science"], "references": ["cd8a9914d50b0ac63315872530274d158d6aff09", "f86e65797301b7e35aec66672a320a1697018924", "86412306b777ee35aba71d4795b02915cb8a04c3", "2218e2e1df2c3adfb70e0def2e326a39928aacfc", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "033f25ad905ef2ed32a8331cf38b83953ff15922", "955fe2ee26d888ae22749b0853981b8b581b133d", "50de83b8a00f448b3e344701a60dfdcfd84881f4", "e15d062ef07abab8fae65244f64ccd2aac8d2b94", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82"], "references_count": 43, "citations_count": 1568}, {"id": "822f1ed9a76a57cc19d8fda7745365b97130b97a", "url": "https://www.semanticscholar.org/paper/Injecting-Logical-Background-Knowledge-into-for-Rockt%C3%A4schel-Singh/822f1ed9a76a57cc19d8fda7745365b97130b97a", "title": "Injecting Logical Background Knowledge into Embeddings for Relation Extraction", "abstract": "This paper introduces a paradigm for learning low-dimensional embeddings of entity-pairs and relations that combine the advantages of matrix factorization with first-order logic domain knowledge, and shows that this method is able to learn accurate extractors with little or no distant supervision alignments, while at the same time generalizing to textual patterns that do not appear in the formulae. Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data. Unfortunately, these methods share a shortcoming with all other distantly supervised approaches: they cannot learn to extract target relations without existing data in the knowledge base, and likewise, these models are inaccurate for relations with sparse data. Rule-based extractors, on the other hand, can be easily extended to novel relations and improved for existing but inaccurate relations, through first-order formulae that capture auxiliary domain knowledge. However, usually a large set of such formulae is necessary to achieve generalization. In this paper, we introduce a paradigm for learning low-dimensional embeddings of entity-pairs and relations that combine the advantages of matrix factorization with first-order logic domain knowledge. We introduce simple approaches for estimating such embeddings, as well as a novel training algorithm to jointly optimize over factual and first-order logic information. Our results show that this method is able to learn accurate extractors with little or no distant supervision alignments, while at the same time generalizing to textual patterns that do not appear in the formulae.", "publication_year": "2015", "authors": ["Tim Rockt{\\\"a"], "related_topics": ["Computer Science"], "references": ["5fac0ca1b3ea3b6f234dd0821e1f3678f0b6096d", "311eb232e4bd3ed53b1ef3381d75b65615d4e29c", "3fbc710e6584187e143582c5be20ebcdb4ff363a", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "1f4a4769e4d2fb846e59c2f185e0377190739f18", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "d84b57362e2010f6f65357267df7e0157af30684", "4a6de89efc5da79c4aabccdb4737ebeedbea7ab2"], "references_count": 51, "citations_count": 242}, {"id": "af2e6165b68e75c911dfdb8f81f9ab6627722ab7", "url": "https://www.semanticscholar.org/paper/Compositional-Vector-Space-Models-for-Knowledge-Neelakantan-Roth/af2e6165b68e75c911dfdb8f81f9ab6627722ab7", "title": "Compositional Vector Space Models for Knowledge Base Completion", "abstract": "This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z)\u2192 containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%.", "publication_year": "2015", "authors": ["Arvind Neelakantan", "Benjamin Roth", "Andrew McCallum"], "related_topics": ["Computer Science"], "references": ["50d53cc562225549457cbc782546bfbe1ac6f0cf", "86412306b777ee35aba71d4795b02915cb8a04c3", "5346525f5022b3d60e7a954b50772ea75d967e7d", "2aea6cc6c42101b2615753c2933a33e57dd665f2", "974d1048fa45227a3ef9f71efe5501f79683dfdf", "5fac0ca1b3ea3b6f234dd0821e1f3678f0b6096d", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "687bac2d3320083eb4530bf18bb8f8f721477600", "4ea80c206b8ad73a6d320c9d8ed0321d84fe6d85", "af44f5db5b4396e1670cda07eff5ad84145ba843"], "references_count": 50, "citations_count": 253}, {"id": "86412306b777ee35aba71d4795b02915cb8a04c3", "url": "https://www.semanticscholar.org/paper/Embedding-Entities-and-Relations-for-Learning-and-Yang-Yih/86412306b777ee35aba71d4795b02915cb8a04c3", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "abstract": "It is found that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. Abstract: We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \\\"BornInCity(a,b) and CityInCountry(b,c) =&gt; Nationality(a,c)\\\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.", "publication_year": "2014", "authors": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "related_topics": ["Computer Science"], "references": ["2582ab7c70c9e7fcb84545944eba8f3a7f253248", "1f4a4769e4d2fb846e59c2f185e0377190739f18", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "473b3f2cc2c942c0116d980fe5b36a338f6017de", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "311eb232e4bd3ed53b1ef3381d75b65615d4e29c", "f6764d853a14b0c34df1d2283e76277aead40fde", "1e7cf9047604f39e517951d129b2b3eecf9e1cfb", "498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "fdb813d8b927bdd21ae1858cafa6c34b66a36268"], "references_count": 38, "citations_count": 2143}, {"id": "1ef01e7bfab2041bc0c0a56a57906964df9fc985", "url": "https://www.semanticscholar.org/paper/Question-Answering-over-Freebase-with-Multi-Column-Dong-Wei/1ef01e7bfab2041bc0c0a56a57906964df9fc985", "title": "Question Answering over Freebase with Multi-Column Convolutional Neural Networks", "abstract": "This paper introduces multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects and learn their distributed representations and develops a method to compute the salience scores of question words in different column networks. Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct extensive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.", "publication_year": "2015", "authors": ["Li Dong", "Furu Wei", "M. Zhou", "Ke Xu"], "related_topics": ["Computer Science"], "references": ["af44f5db5b4396e1670cda07eff5ad84145ba843", "4cfad7889dc12825309325cd4b4f3febed424e36", "a584211768d49f80192f13b8ed2fda9c058dec34", "33261d252218007147a71e40f8367ed152fa2fe0", "c0be2ac2f45681f1852fc1d298af5dceb85834f4", "bbc860159dea10df35c54d1271bcc0a6b9e2df22", "b29447ba499507a259ae9d8f685d60cc1597d7d3", "a129f612a9eff903d9133244a6f0914ef3cbda72", "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "3d1d42c9435b419ac928ebf7bcf4c86a460d6ef4"], "references_count": 30, "citations_count": 394}, {"id": "e745b0506f4133263633eb05e5006a8cff4129f0", "url": "https://www.semanticscholar.org/paper/Traversing-Knowledge-Graphs-in-Vector-Space-Guu-Miller/e745b0506f4133263633eb05e5006a8cff4129f0", "title": "Traversing Knowledge Graphs in Vector Space", "abstract": "It is demonstrated that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results. Path queries on a knowledge graph can be used to answer compositional questions such as \\\"What languages are spoken by people living in Lisbon?\\\". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new \\\"compositional\\\" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.", "publication_year": "2015", "authors": ["Kelvin Guu", "John Miller", "Percy Liang"], "related_topics": ["Computer Science"], "references": ["af2e6165b68e75c911dfdb8f81f9ab6627722ab7", "2aea6cc6c42101b2615753c2933a33e57dd665f2", "5346525f5022b3d60e7a954b50772ea75d967e7d", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "054ba27fe5cc6085d20ea2707de886db6865dbed", "cf5ea582bccc7cb21a2ebeb7a0987f79652bde8d", "86412306b777ee35aba71d4795b02915cb8a04c3", "7c05a4ffee7e159e34b2efea7e44d994333ec628", "33261d252218007147a71e40f8367ed152fa2fe0", "2582ab7c70c9e7fcb84545944eba8f3a7f253248"], "references_count": 28, "citations_count": 329}, {"id": "6b7d6e6416343b2a122f8416e69059ce919026ef", "url": "https://www.semanticscholar.org/paper/Inductive-Representation-Learning-on-Large-Graphs-Hamilton-Ying/6b7d6e6416343b2a122f8416e69059ce919026ef", "title": "Inductive Representation Learning on Large Graphs", "abstract": "GraphSAGE is presented, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data and outperforms strong baselines on three inductive node-classification benchmarks. Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.", "publication_year": "2017", "authors": ["William L. Hamilton", "Zhitao Ying", "Jure Leskovec"], "related_topics": ["Computer Science"], "references": ["36ee2c8bd605afd48035d15fdc6b8c8842363376", "3d846cb01f6a975554035d2210b578ca61344b22", "0834e74304b547c9354b6d7da6fa78ef47a48fa8", "36eff562f65125511b5dfab68ce7f7a943c27478", "322cf9bcde458a45eaeca989a1eec92f7c6db984", "492f57ee9ceb61fb5a47ad7aebfec1121887a175", "fff114cbba4f3ba900f33da574283e3de7f26c83", "9ca9f28676ad788d04ba24a51141a9a0a0df4d67", "54906484f42e871f7c47bbfe784a358b1448231f", "63b9f657a3133d62c97eadaf5ed7d6d0c2f5cf3d"], "references_count": 40, "citations_count": 8515}, {"id": "97f7ef7a5332218e0e9ce75ad5cf77048466ca83", "url": "https://www.semanticscholar.org/paper/Column-Networks-for-Collective-Classification-Pham-Tran/97f7ef7a5332218e0e9ce75ad5cf77048466ca83", "title": "Column Networks for Collective Classification", "abstract": "\\n \\n Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computationally challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient with linear complexity in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all of these applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.\\n \\n", "publication_year": "2016", "authors": ["Trang Pham", "T. Tran", "Dinh Q. Phung", "Svetha Venkatesh"], "related_topics": ["Computer Science"], "references": ["81734672f5fecbbf6027180b6927b836e0fef6d4", "9cc36397e1fef5c922d64e88211a7e08ecc64759", "d582539ba974385a1b08c000b63fe9869a63c1ed", "5f8aaefa3c07563cb11884f3f227bd94431544ff", "5e27712db641bc8f16c510292f7fd5440acd563d", "43d2ed5c3c55c1100450cd74dc1031afa24d37b2", "bc82b4f9f202062857958f0336fc28327a75563b", "c8af456dedb2dfe664641f801d5e3060fbd4e293", "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references_count": 33, "citations_count": 138}, {"id": "473b3f2cc2c942c0116d980fe5b36a338f6017de", "url": "https://www.semanticscholar.org/paper/A-latent-factor-model-for-highly-multi-relational-Jenatton-Roux/473b3f2cc2c942c0116d980fe5b36a338f6017de", "title": "A latent factor model for highly multi-relational data", "abstract": "This paper proposes a method for modeling large multi-relational datasets, with possibly thousands of relations, based on a bilinear structure, which captures various orders of interaction of the data and also shares sparse latent factors across different relations. Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relations between entities. While there is a large body of work focused on modeling these data, modeling these multiple types of relations jointly remains challenging. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures various orders of interaction of the data, and also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient and semantically meaningful verb representations.", "publication_year": "2012", "authors": ["Rodolphe Jenatton", "Nicolas Le Roux", "Antoine Bordes", "Guillaume Obozinski"], "related_topics": ["Computer Science"], "references": ["f6764d853a14b0c34df1d2283e76277aead40fde", "fec691d09b564986ad27162ce15344604c840ff9", "81bbe42e3ec09c28b8864956148e58f4cb5aa860", "4e07791ee0872401215f12aefde342bd843240cc", "498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "1a27b23a56b42cd52249ed3767f3b320acd07c91", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "8b27153da18537bd7ec7fd8205d24a34d1c64883", "5c12193fc84ed7973fe4515ae893625d8af4ce4f", "b6ce4ec0d28c050b99ec647a16e47116c939473c"], "references_count": 28, "citations_count": 395}, {"id": "f6764d853a14b0c34df1d2283e76277aead40fde", "url": "https://www.semanticscholar.org/paper/A-Three-Way-Model-for-Collective-Learning-on-Data-Nickel-Tresp/f6764d853a14b0c34df1d2283e76277aead40fde", "title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "abstract": "This work presents a novel approach to relational learning based on the factorization of a three-way tensor that is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorizations. Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.", "publication_year": "2011", "authors": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "related_topics": ["Computer Science"], "references": ["fec691d09b564986ad27162ce15344604c840ff9", "81bbe42e3ec09c28b8864956148e58f4cb5aa860", "19680a17240eb1eab6cfec04e3faa2bd5be61ab9", "c8967a8d3d132673d2e3ff5c785b83b5402bf440", "611dac316bf03112c778cf7365d08e4a9d171876", "a32aa4d00a4038e721f0736b9b01b58d09681c15", "43d2ed5c3c55c1100450cd74dc1031afa24d37b2", "16840a46b3980eb39382814adfe2270bd5bbdbc7", "39d61c55cdf80bcb473a320492ddb64a308e54c0", "b6ce4ec0d28c050b99ec647a16e47116c939473c"], "references_count": 21, "citations_count": 1792}, {"id": "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "url": "https://www.semanticscholar.org/paper/A-semantic-matching-energy-function-for-learning-Glorot-Bordes/eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "title": "A semantic matching energy function for learning with multi-relational data", "abstract": "A new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced, demonstrating that it can scale up to tens of thousands of nodes and thousands of types of relation. Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature as well as on data from a real-world knowledge base (WordNet). In addition, we present how our method can be applied to perform word-sense disambiguation in a context of open-text semantic parsing, where the goal is to learn to assign a structured meaning representation to almost any sentence of free text, demonstrating that it can scale up to tens of thousands of nodes and thousands of types of relation.", "publication_year": "2013", "authors": ["Xavier Glorot", "Antoine Bordes", "Jason Weston", "Yoshua Bengio"], "related_topics": ["Computer Science"], "references": ["1f4a4769e4d2fb846e59c2f185e0377190739f18", "f2f72cfb48d15d4d2bd1e91a92e7f3ac8635d433", "498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "27e38351e48fe4b7da2775bf94341738bc4da07e", "233d861338cfcd479b1d21897453fcc66418d5e1", "93bb6228776eafa606965e21f229d548de1998eb", "db328685d00ec35fe35f9350f884c7b4b8db3f4c", "12f17c64eb20f051552295d2c928f036f5b8163b", "aea0f946e8dcddb65cc2e907456c42453f246a50", "16840a46b3980eb39382814adfe2270bd5bbdbc7"], "references_count": 72, "citations_count": 593}, {"id": "834cb8e1e738b8d2c6d24e652ac966d6e7089a46", "url": "https://www.semanticscholar.org/paper/Connecting-Language-and-Knowledge-Bases-with-Models-Weston-Bordes/834cb8e1e738b8d2c6d24e652ac966d6e7089a46", "title": "Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction", "abstract": "This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge, based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.", "publication_year": "2013", "authors": ["Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier"], "related_topics": ["Computer Science"], "references": ["d48edf9e81653f4c3da716b037b0b50d54c5b034", "233d861338cfcd479b1d21897453fcc66418d5e1", "9c7f4412b8f0310a91334aed79b8553b2ad70908", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "2a2d03a1534b365c5b048c824c0886e16ccf7dfa", "d84b57362e2010f6f65357267df7e0157af30684", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "796918285116a29537489bb7dc1778f2b1f3e4e8", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "f2f72cfb48d15d4d2bd1e91a92e7f3ac8635d433"], "references_count": 23, "citations_count": 232}, {"id": "8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092", "url": "https://www.semanticscholar.org/paper/Learning-New-Facts-From-Knowledge-Bases-With-Neural-Chen-Socher/8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092", "title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors", "abstract": "A neural tensor network (NTN) model is introduced which predicts new relationship entries that can be added to the database and can classify unseen relationships in WordNet with an accuracy of 75.8%. Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 75.8%.", "publication_year": "2013", "authors": ["Danqi Chen", "Richard Socher", "Christopher D. Manning", "A. Ng"], "related_topics": ["Computer Science"], "references": ["1f4a4769e4d2fb846e59c2f185e0377190739f18", "e703e928bc07900527c368db2428d0d5c57148c2", "f2f72cfb48d15d4d2bd1e91a92e7f3ac8635d433", "00a3f6924f90fcd77e6e7e6534b957a75d0ced07", "27e38351e48fe4b7da2775bf94341738bc4da07e", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "dac72f2c509aee67524d3321f77e97e8eff51de6", "81bbe42e3ec09c28b8864956148e58f4cb5aa860", "57458bc1cffe5caa45a885af986d70f723f406b4"], "references_count": 15, "citations_count": 62}, {"id": "498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "url": "https://www.semanticscholar.org/paper/Factorizing-YAGO%3A-scalable-machine-learning-for-Nickel-Tresp/498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "title": "Factorizing YAGO: scalable machine learning for linked data", "abstract": "This work presents an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts, and shows how ontological knowledge can be incorporated in the factorizations to improve learning results and how computation can be distributed across multiple nodes. Vast amounts of structured information have been published in the Semantic Web's Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD's size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD's data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize non-deterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO~2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 \u22c5 1014 possible triples in the YAGO~2 core ontology.", "publication_year": "2012", "authors": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "related_topics": ["Computer Science"], "references": ["477244deb1116aa0681a404c77ca2f466876bf0b", "a32aa4d00a4038e721f0736b9b01b58d09681c15", "f6764d853a14b0c34df1d2283e76277aead40fde", "19680a17240eb1eab6cfec04e3faa2bd5be61ab9", "9d8825281ef9cb55e2b4d518da1a92691257f985", "16840a46b3980eb39382814adfe2270bd5bbdbc7", "81bbe42e3ec09c28b8864956148e58f4cb5aa860", "b7d2ff895947e8730d788b6240de2807c9c1f4be", "de7a42ed958cb152e50bbda0816184e45c6a7788", "cd366ddeb824f2b26213e9040ba3ff8a4497cec4"], "references_count": 38, "citations_count": 411}, {"id": "1f4a4769e4d2fb846e59c2f185e0377190739f18", "url": "https://www.semanticscholar.org/paper/Learning-Structured-Embeddings-of-Knowledge-Bases-Bordes-Weston/1f4a4769e4d2fb846e59c2f185e0377190739f18", "title": "Learning Structured Embeddings of Knowledge Bases", "abstract": "A learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced would allow data from any KB to be easily used in recent machine learning meth- ods for prediction and information retrieval. \\n \\n Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigorous symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like nat- ural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning meth- ods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.\\n \\n", "publication_year": "2011", "authors": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "related_topics": ["Computer Science"], "references": ["a88966cdaeddd15d0a3de365a8f0a5931aebd756", "2b776119a1347e1455dc498ff5078b3a94029ed9", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "57458bc1cffe5caa45a885af986d70f723f406b4", "364fc5142fcad8ed0f9aaee6044276eb269fb017", "aea0f946e8dcddb65cc2e907456c42453f246a50", "f7312b8568d63bbbb239583ed282f46cdc40978d", "8df1e4d7a7c2f288b7ca4645b444b128b076a572", "08c81389b3ac4b8253d718a7cebe04a5536efa78"], "references_count": 23, "citations_count": 840}, {"id": "fec691d09b564986ad27162ce15344604c840ff9", "url": "https://www.semanticscholar.org/paper/Relational-learning-via-collective-matrix-Singh-Gordon/fec691d09b564986ad27162ce15344604c840ff9", "title": "Relational learning via collective matrix factorization", "abstract": "This model generalizes several existing matrix factorization methods, and therefore yields new large-scale optimization algorithms for these problems, which can handle any pairwise relational schema and a wide variety of error models. Relational learning is concerned with predicting unknown values of a relation, given a database of entities and observed relations among entities. An example of relational learning is movie rating prediction, where entities could include users, movies, genres, and actors. Relations encode users' ratings of movies, movies' genres, and actors' roles in movies. A common prediction technique given one pairwise relation, for example a #users x #movies ratings matrix, is low-rank matrix factorization. In domains with multiple relations, represented as multiple matrices, we may improve predictive accuracy by exploiting information from one relation while predicting another. To this end, we propose a collective matrix factorization model: we simultaneously factor several matrices, sharing parameters among factors when an entity participates in multiple relations. Each relation can have a different value type and error distribution; so, we allow nonlinear relationships between the parameters and outputs, using Bregman divergences to measure error. We extend standard alternating projection algorithms to our model, and derive an efficient Newton update for the projection. Furthermore, we propose stochastic optimization methods to deal with large, sparse matrices. Our model generalizes several existing matrix factorization methods, and therefore yields new large-scale optimization algorithms for these problems. Our model can handle any pairwise relational schema and a wide variety of error models. We demonstrate its efficiency, as well as the benefit of sharing parameters among relations.", "publication_year": "2008", "authors": ["Ajit Paul Singh", "Geoffrey J. Gordon"], "related_topics": ["Computer Science"], "references": ["2549520c25c4f3fcc80cc9c99157e760a9548f14", "f273e19d9f5c8a01bb0626a923db2a381d527363", "5c58ad9a6c09782814a7d048bebd6ef1609c0fb4", "6ac17e450f35b5b4d8e8f1345a2912edfc4ca187", "6a221e1c54217a2f9e6a38c79e2f2d82d339797c", "6cd49dd5d26d1e8e33891f8e64ad3b5012e90ba6", "348b65c8dcabb6f1028f120e36eee7b4d8ae32a6", "1cb5c5cd4ccafe870e4ac8a0f040e88554ec6fc6", "6fb07b90b7fd2785ffec0da1069e75c53f7313c2", "5020ba7489d8453952c61f863af6dda91ad10bb9"], "references_count": 41, "citations_count": 1142}, {"id": "81bbe42e3ec09c28b8864956148e58f4cb5aa860", "url": "https://www.semanticscholar.org/paper/Modelling-Relational-Data-using-Bayesian-Clustered-Sutskever-Salakhutdinov/81bbe42e3ec09c28b8864956148e58f4cb5aa860", "title": "Modelling Relational Data using Bayesian Clustered Tensor Factorization", "abstract": "The Bayesian Clustered Tensor Factorization (BCTF) model is introduced, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework that is fully Bayesian but scales well to large data sets. We consider the problem of learning probabilistic models for complex relational structures between various types of objects. A model can help us \\\"understand\\\" a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true. Often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have given better predictive performance on large data sets. We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework. Inference is fully Bayesian but scales well to large data sets. The model simultaneously discovers interpretable clusters and yields predictive performance that matches or beats previous probabilistic models for relational data.", "publication_year": "2009", "authors": ["Ilya Sutskever", "Ruslan Salakhutdinov", "Joshua B. Tenenbaum"], "related_topics": ["Computer Science"], "references": ["ce97e12004784109965febf3abf06fabed91c6a1", "5262fe8369992259be27165ccd09d1d31c7a4def", "1a27b23a56b42cd52249ed3767f3b320acd07c91", "d9b9fb207013bf8afb064f23f3dffc7edd005f73", "e19971e7d100386b9b4cf4ea1a0782b62fe036e5", "b6ce4ec0d28c050b99ec647a16e47116c939473c", "87ca5a0f345533c30217f6359bc4325a2442a0b9", "be860525cb9c5d99722a5f3535bbbbbd605a7ea5", "91e62d27c08db29cf011a0326a61509e574cf772", "6305dcc03c8378e371e73b0a68ff29f1167a65f0"], "references_count": 36, "citations_count": 271}, {"id": "4e07791ee0872401215f12aefde342bd843240cc", "url": "https://www.semanticscholar.org/paper/Nonparametric-Latent-Feature-Models-for-Link-Miller-Griffiths/4e07791ee0872401215f12aefde342bd843240cc", "title": "Nonparametric Latent Feature Models for Link Prediction", "abstract": "This work pursues a similar approach with a richer kind of latent variable\u2014latent features\u2014using a Bayesian nonparametric approach to simultaneously infer the number of features at the same time the authors learn which entities have each feature, and combines these inferred features with known covariates in order to perform link prediction. As the availability and importance of relational data\u2014such as the friendships summarized on a social networking website\u2014increases, it becomes increasingly important to have good models for such data. The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting Bayesian nonparametric methods to jointly infer how many latent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable\u2014latent features\u2014using a Bayesian nonparametric approach to simultaneously infer the number of features at the same time we learn which entities have each feature. Our model combines these inferred features with known covariates in order to perform link prediction. We demonstrate that the greater expressiveness of this approach allows us to improve performance on three datasets.", "publication_year": "2009", "authors": ["Kurt T. Miller", "Thomas L. Griffiths", "Michael I. Jordan"], "related_topics": ["Computer Science"], "references": ["4d63618acc0bc6ecb1b3e88d5050b1cef06c3bed", "c8967a8d3d132673d2e3ff5c785b83b5402bf440", "82e4390c043754d5af22d48964a42a891f81e8b3", "dbe30a96b7db2df4e8f6c3492e2092c68feedcd6", "769139f6cc8e50b1ad426c1bf48a3332c86819fc", "bce1b1140472a45ca4864c1cade951ccc9893291", "ab30b9de25048c15df0ebc353c64f4f3cf6ed52b", "d415b4d43bfea32554964d511c8b34638b79a7fd", "630bab4b708bc6621f97789b14f790153f115d15", "7c7374c4ccd94f1900790d4cd3b90d5681c8901b"], "references_count": 26, "citations_count": 420}]