[{"ID": "12508951ba96b7d4c0906ed95542287d3ebdfd95", "Title": "The Eighth Visual Object Tracking VOT2020 Challenge Results", "Abstract": "The Visual Object Tracking challenge VOT2020 is the eighth annual tracker benchmarking activity organized by the VOT initiative. Results of 58 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The VOT2020 challenge was composed of five sub-challenges focusing on different tracking domains: (i) VOT-ST2020 challenge focused on short-term tracking in RGB, (ii) VOT-RT2020 challenge focused on \u201creal-time\u2026\u00a0", "Publication Year": "2020", "Authors": ["M. Kristan", "A. Leonardis", "Ziang Ma"], "Related Topics": "Computer Science", "Citation Count": "144", "Reference Count": "88", "References": ["786577081e00d69eeac8e9612eaf2dad59765e73", "219e9a4527110baf1feb3df20db12064eeafdfb7", "350d507f5d899e4d7293b1aa951aa0f81b9fd30a", "047ea298464b041a90c4ab4e716356c019d613ab", "966aad492f75b17f698e981e008b73b51816c6aa", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508", "dd45fe910a0200d43aaa77362f658542f6e175ff", "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7", "45512d44f1205bc92775f2e880858b3f23c9f5fd"]}, {"ID": "c6db34ade32b3681a92068b22a354903b2953d52", "Title": "Benign and malignant breast tumors classification based on region growing and CNN segmentation", "Abstract": "", "Publication Year": "2015", "Authors": ["R. Rouhi", "M. Jafari", "P. Keshavarzian"], "Related Topics": "Computer Science, Medicine", "Citation Count": "329", "Reference Count": "59", "References": ["3c948ca247c6f55ef994400e713412b5f845dd40", "9b5d0a48b0feb156a1270da54d90d0963a3f0404", "76389ebb7c1496239e66fd663b0e7e43d391bca9", "d0059280e3f69b8fd07ce036e7d2407e3ebcff9e", "2dfb1fd3adfa58a3448251e03b1a5a78239958b3", "46c409dd878e643271ef63f1817ded8b57abc01e", "342da5d8633aebf27d914a8618e523579a130289", "fe83150bc326fd62d352cb2993ac91344f195e10", "483f0f12feb8ac1c396349e5526a7552b6b067cd", "a9d0b3485f3091e832f87edb469c350c90cabae1"]}, {"ID": "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73", "Title": " Event Detection and Summarization in Soccer Videos Using Bayesian Network and Copula", "Abstract": "", "Publication Year": "2014", "Authors": ["M. Tavassolipour", "Mahmood Karimian", "S. Kasaei"], "Related Topics": "Computer Science", "Citation Count": "96", "Reference Count": "38", "References": ["d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f", "d71c84fbaff0d7f2cbcf2f03630e189c58939a4a", "43eb2d80721daa318cc19e4cce405122934e88f2", "0cca81f3b2928f6d4005e1a8b617960e2fb14d3e", "0365c1382924394e200cb627e59cb9c21f8e75bd", "c9edc10d5c22ae62a700c37a41bb0ea22961a0aa", "90fbd777cf57096e9292601dfe0dbab30198d40f", "f7b4fcc9dbb97997dc1793d7a366cba274f53134", "8e0f60b718fa19c2ed10bd93401796683af79512", "c2f8178ce89a6cc1ad0e1dd5db4bf155d5a80620"]}, {"ID": "53fc0415e0d00f9691994a49b8232a1cc2dfad5f", "Title": " An efficient PCA-based color transfer method", "Abstract": "", "Publication Year": "2007", "Authors": ["A. Abadpour", "S. Kasaei"], "Related Topics": "Computer Science", "Citation Count": "67", "Reference Count": "45", "References": ["e1dfff8cff33ede3564a35724632bddc5b8619b1", "d93e4e77e2baba98f1af7f23d47fbf9b46be4df5", "05f63bdf9e60d0a299cfe5e8d7ba043904f1fea1", "0c2ced886708cc3aea4705f8765d152cd3f69cd2", "a2882b8b0c9635d39d15a28138e3f47907f3177b", "61579369e7b97dec9c699c058edaafcde2817d21", "4a6c5c9b1fb106f7d82508ae593d30e207c8ea45", "ab67b9d0da50e251a4f7e42370540547b891ceb1", "d5c6edb53dc41f298f145041cd2c53e40e3acf2b", "ed7e166f65bcecc522c6c4bbb29fcf8048010873"]}, {"ID": "1fbb4201af091aef55360f113ba35814063923e4", "Title": " Deep Learning for Visual Tracking: A Comprehensive Survey", "Abstract": "Visual target tracking is one of the most sought-after yet challenging research topics in computer vision. Given the ill-posed nature of the problem and its popularity in a broad range of real-world scenarios, a number of large-scale benchmark datasets have been established, on which considerable methods have been developed and demonstrated with significant progress in recent years \u2013 predominantly by recent deep learning (DL)-based methods. This survey aims to systematically investigate the\u2026\u00a0", "Publication Year": "2019", "Authors": ["Seyed Mojtaba Marvasti-Zadeh", "Li Cheng", "S. Kasaei"], "Related Topics": "Computer Science", "Citation Count": "156", "Reference Count": "281", "References": ["26e2ca763087be09e3799ad294302aa91077942d", "021d0c7013da519b508610064f264c76d768fdf1", "0a400fd7f0ee28694889baaa4faef150b6912dfa", "311bc4e48838d8e5ef619df3ce0bc598aba788a1", "388d29f001411ff80650f80cf197afc440d98b51", "f24015a365ea2454391c285cd30b8ae723dbb05e", "7574b7e5a75fdd338c27af5aeb77ab79460c4437", "320d05db95ab42ade69294abe46cd1aca6aca602", "1855818c492d5f42dbe14814e4dd9b5733d54790", "e2e34b202363e4a46a14cd35fd4088d88b2e650e"]}, {"ID": "786577081e00d69eeac8e9612eaf2dad59765e73", "Title": " The Seventh Visual Object Tracking VOT2019 Challenge Results", "Abstract": "The Visual Object Tracking challenge VOT2019 is the seventh annual tracker benchmarking activity organized by the VOT initiative. Results of 81 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis as well as the standard VOT methodology for long-term tracking analysis. The VOT2019 challenge was composed\u2026\u00a0", "Publication Year": "2019", "Authors": ["M. Kristan", "Jiri Matas", "Zihan Ni"], "Related Topics": "Computer Science", "Citation Count": "323", "Reference Count": "111", "References": ["350d507f5d899e4d7293b1aa951aa0f81b9fd30a", "966aad492f75b17f698e981e008b73b51816c6aa", "6179ac06f1a8fd1ac6b693b02824948dff438d54", "047ea298464b041a90c4ab4e716356c019d613ab", "6767812e114c426d45ea83894b156f7906e525cd", "19d6b9725a59f4b624205829d5f03ac893ca1367", "23f8927f996d56f3b5076d8993a70bcfc70182a1", "dd45fe910a0200d43aaa77362f658542f6e175ff", "9926020dda21874dc7a5ef1511bae6c4cef5ecb9", "320d05db95ab42ade69294abe46cd1aca6aca602"]}, {"ID": "219e9a4527110baf1feb3df20db12064eeafdfb7", "Title": " The Sixth Visual Object Tracking VOT2018 Challenge Results", "Abstract": "The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a \u201creal-time\u201d experiment simulating a situation where a tracker processes images as if provided\u2026\u00a0", "Publication Year": "2018", "Authors": ["M. Kristan", "A. Leonardis", "Zhiqun He"], "Related Topics": "Computer Science", "Citation Count": "588", "Reference Count": "100", "References": ["350d507f5d899e4d7293b1aa951aa0f81b9fd30a", "966aad492f75b17f698e981e008b73b51816c6aa", "047ea298464b041a90c4ab4e716356c019d613ab", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "19d6b9725a59f4b624205829d5f03ac893ca1367", "6767812e114c426d45ea83894b156f7906e525cd", "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac", "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7", "3275944117b43cc44beebe7c82bffc13ec8cb0fa", "9926020dda21874dc7a5ef1511bae6c4cef5ecb9"]}, {"ID": "350d507f5d899e4d7293b1aa951aa0f81b9fd30a", "Title": " The Visual Object Tracking VOT2017 Challenge Results", "Abstract": "The Visual Object Tracking challenge VOT2017 is the fifth annual tracker benchmarking activity organized by the VOT initiative. Results of 51 trackers are presented; many are state-of-the-art published at major computer vision conferences or journals in recent years. The evaluation included the standard VOT and other popular methodologies and a new \"real-time\" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. Performance of the\u2026\u00a0", "Publication Year": "2017", "Authors": ["M. Kristan", "A. Leonardis", "Zhiqun He"], "Related Topics": "Computer Science", "Citation Count": "429", "Reference Count": "132", "References": ["966aad492f75b17f698e981e008b73b51816c6aa", "047ea298464b041a90c4ab4e716356c019d613ab", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "4dff84213493bb177dc6bff266a9893538a1f879", "6767812e114c426d45ea83894b156f7906e525cd", "dd45fe910a0200d43aaa77362f658542f6e175ff", "f3c842c88b14cfcc631e5c2cab5f376b9efa09e3", "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "eda3368a5198ca55768b07b6f5667aea28baf2cd"]}, {"ID": "047ea298464b041a90c4ab4e716356c019d613ab", "Title": " The Visual Object Tracking VOT2015 Challenge Results", "Abstract": "The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 predecessor are: (i) a new VOT2015 dataset twice\u2026\u00a0", "Publication Year": "2015", "Authors": ["M. Kristan", "Jiri Matas", "Zhe Chen"], "Related Topics": "Computer Science", "Citation Count": "398", "Reference Count": "84", "References": ["4b1a47709d0546e5bc614bf9a521c550e6881d04", "6767812e114c426d45ea83894b156f7906e525cd", "4dff84213493bb177dc6bff266a9893538a1f879", "17f16b89edaed5d16867287ed8a85e917304b4ba", "f3c842c88b14cfcc631e5c2cab5f376b9efa09e3", "91f2b2aeb7e65d0b673ed7e782488b3365027979", "9926020dda21874dc7a5ef1511bae6c4cef5ecb9", "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "7b75da6f5edac80575d9dcf63db164ce24933907"]}, {"ID": "966aad492f75b17f698e981e008b73b51816c6aa", "Title": " The Visual Object Tracking VOT2016 Challenge Results", "Abstract": "The Visual Object Tracking challenge VOT2016 aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 70 trackers are presented, with a large number of trackers being published at major computer vision conferences and journals in the recent years. The number of tested state-of-the-art trackers makes the VOT 2016 the largest and most challenging benchmark on short-term tracking to date. For each participating tracker, a\u2026\u00a0", "Publication Year": "2016", "Authors": ["M. Kristan", "A. Leonardis", "Zhizhen Chi"], "Related Topics": "Computer Science", "Citation Count": "705", "Reference Count": "112", "References": ["15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "6767812e114c426d45ea83894b156f7906e525cd", "91f2b2aeb7e65d0b673ed7e782488b3365027979", "9926020dda21874dc7a5ef1511bae6c4cef5ecb9", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "5bae9822d703c585a61575dced83fa2f4dea1c6d", "6b175816b1f81127f5e2a2fe998df99d62290a1c", "f15d5c0a9d2f3678b4c16330da29b3b4511fdef5", "16be98fa5924131816bc991a2c7ed91b8c69eaaa"]}, {"ID": "4b1a47709d0546e5bc614bf9a521c550e6881d04", "Title": " The Visual Object Tracking VOT2013 Challenge Results", "Abstract": "Visual tracking has attracted a significant attention in the last few decades. The recent surge in the number of publications on tracking-related problems have made it almost impossible to follow the developments in the field. One of the reasons is that there is a lack of commonly accepted annotated data-sets and standardized evaluation protocols that would allow objective comparison of different tracking methods. To address this issue, the Visual Object Tracking (VOT) workshop was organized in\u2026\u00a0", "Publication Year": "2013", "Authors": ["M. Kristan", "Juan E. Sala Matas", "Z. Niu"], "Related Topics": "Computer Science", "Citation Count": "333", "Reference Count": "137", "References": ["4dff84213493bb177dc6bff266a9893538a1f879", "047ea298464b041a90c4ab4e716356c019d613ab", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "6767812e114c426d45ea83894b156f7906e525cd", "2822a883d149956934a20614d6934c6ddaac6857", "f3c842c88b14cfcc631e5c2cab5f376b9efa09e3", "e3c433ab9608d7329f944552ba1721e277a42d74", "882c5e862f2256e10bb7dd74d5bbc984b01489fe", "9926020dda21874dc7a5ef1511bae6c4cef5ecb9"]}, {"ID": "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508", "Title": "Performance Evaluation Methodology for Long-Term Single-Object Tracking", "Abstract": "A long-term visual object tracking performance evaluation methodology and a benchmark are proposed. Performance measures are designed by following a long-term tracking definition to maximize the analysis probing strength. The new measures outperform existing ones in interpretation potential and in better distinguishing between different tracking behaviors. We show that these measures generalize the short-term performance measures, thus linking the two tracking problems. Furthermore, the new\u2026\u00a0", "Publication Year": "2020", "Authors": ["A. Luke\u017ei\u010d", "L. \u010c. Zajc", "M. Kristan"], "Related Topics": "Computer Science", "Citation Count": "16", "Reference Count": "0", "References": []}, {"ID": "dd45fe910a0200d43aaa77362f658542f6e175ff", "Title": " A thermal Object Tracking benchmark", "Abstract": "", "Publication Year": "2015", "Authors": ["A. Berg", "J. Ahlberg", "M. Felsberg"], "Related Topics": "Computer Science", "Citation Count": "94", "Reference Count": "23", "References": ["6a699d87bb55477cbe7bfb45b7991b889fd976b6", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "2c76de57b8b1c9e63b1883cbdea9ec8e68ddf493", "201d116761d9d300193df370107f26d7d475023b", "bfba194dfd9c7c27683082aa8331adc4c5963a0d", "c26eaf62a20ebac360148276a1f098d92c7b0738", "d867d7c8cfefe0f5a297a3c613ae6d79c851f4b9", "a8ccbb0981b104cc0f753514cc28c01e5309dc41", "fe2aaad872a2cf08c09dd52ca972f323666306db", "2873de80204743249012f52821419978f4d8b27e"]}, {"ID": "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7", "Title": " Long-term Tracking in the Wild: A Benchmark", "Abstract": "We introduce the OxUvA dataset and benchmark for evaluating single-object tracking algorithms. Benchmarks have enabled great strides in the field of object tracking by defining standardized evaluations on large sets of diverse videos. However, these works have focused exclusively on sequences that are just tens of seconds in length and in which the target is always visible. Consequently, most researchers have designed methods tailored to this \u201cshort-term\u201d scenario, which is poorly\u2026\u00a0", "Publication Year": "2018", "Authors": ["Jack Valmadre", "Luca Bertinetto", "E. Gavves"], "Related Topics": "Computer Science", "Citation Count": "144", "Reference Count": "35", "References": ["19d6b9725a59f4b624205829d5f03ac893ca1367", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "91f2b2aeb7e65d0b673ed7e782488b3365027979", "703505a00579c0aa67712836acc41d94fa6d6edc", "754504cf01ef3846259783e748b1d3ea52fa2c81", "d3d36c3caa255053877a7e3250d47d906eec81d2", "3275944117b43cc44beebe7c82bffc13ec8cb0fa", "29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "350d507f5d899e4d7293b1aa951aa0f81b9fd30a"]}, {"ID": "45512d44f1205bc92775f2e880858b3f23c9f5fd", "Title": " D3S \u2013 A Discriminative Single Shot Segmentation Tracker", "Abstract": "Template-based discriminative trackers are currently the dominant tracking paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot segmentation tracker - D3S, which narrows the gap between visual object tracking and video object segmentation. A single-shot network applies two target models with complementary geometric properties, one invariant to\u2026\u00a0", "Publication Year": "2019", "Authors": ["A. Luke\u017ei\u010d", "Jiri Matas", "M. Kristan"], "Related Topics": "Computer Science", "Citation Count": "138", "Reference Count": "53", "References": ["12fae9a2c1ed867997e1ca70eba271b3c741c42f", "d58e13f7e5e06440c9470a9101ccbb1bfd91b5a1", "320d05db95ab42ade69294abe46cd1aca6aca602", "c316d5ec14e5768d7eda3d8916bddc1de142a1c2", "f5c5c5a2ae127e3e21c1ea94ccad4c17fd02b914", "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b", "b3249763ac9ecc4df6ef96721c8c7410e0f0468a", "8b74008565b575f9ab7a0962ca5f6955d64db045", "966aad492f75b17f698e981e008b73b51816c6aa", "4a70c20ad66e5f3bb12fccd84c63ba619053c811"]}, {"ID": "3c948ca247c6f55ef994400e713412b5f845dd40", "Title": "An adaptive region growing algorithm for breast masses in mammograms", "Abstract": "This study attempted to accurately segment the mammographic masses and distinguish malignant from benign tumors. An adaptive region growing algorithm with hybrid assessment function combined with maximum likelihood analysis and maximum gradient analysis was developed in this paper. In order to accommodate different situations of masses, the likelihood and the edge gradients of segmented masses were weighted adaptively by the use of information entropy. 106 benign and 110 malignant tumors were\u2026\u00a0", "Publication Year": "2010", "Authors": ["Ying Cao", "Xin Hao", "Shun-ren Xia"], "Related Topics": "Computer Science", "Citation Count": "25", "Reference Count": "31", "References": ["3f116f1763a3604275b06c6ccf0dcd65910d13b5", "7642ea8a5bd70c3eede1c1c9bf8b666bf4c5c0a3", "5e99438461ae96f4a5816e37a9e01f9d9ae6ab4e", "c909583f2560864a2650dc25dbcd09da8eb96fbd", "e3f3ff00759dca433cf5740b38dd8dce25ee45a0", "a23937d6345c6e707cf17509ec27c76fef4ce7cc", "34c44883a6152c5298f2c452670c1127072400e6", "a277fe4df26e09a6c375e32d77472ae2c9a7632d", "931d0f894fd46ada2051e2b451c1b3ee675a397c", "0ffefa7d286b24f7c20bedf779cd7bc7d1f64ceb"]}, {"ID": "9b5d0a48b0feb156a1270da54d90d0963a3f0404", "Title": "Directional features for automatic tumor classification of mammogram images", "Abstract": "", "Publication Year": "2011", "Authors": ["I. Buciu", "A. Gacs\u00e1di"], "Related Topics": "Medicine, Computer Science", "Citation Count": "116", "Reference Count": "32", "References": ["16b9ef18b616b495e32bb9aa0c562c6a39168c65", "34c44883a6152c5298f2c452670c1127072400e6", "0837f79a0efa2180c7bd35a6792c0e3a3e8cd258", "09ca54c7866c907641e48b59bc194105f9aacba3", "6de99853193ed9ad9ae36e0994f7a552b546b86a", "2dfb1fd3adfa58a3448251e03b1a5a78239958b3", "e4488ebd82674a981852c66c3138cb0bb9fe6b26", "ef0fe1d3d1b1ac5092374b6bc7533c51f761161d", "ce4e601a216a1d7872c611065d4f865d0b19eddf", "543f1f8bd4ae4d8de56828e4afdeaba14ac849c5"]}, {"ID": "76389ebb7c1496239e66fd663b0e7e43d391bca9", "Title": "Performance evaluation of a region growing procedure for mammographic breast lesion identification", "Abstract": "", "Publication Year": "2011", "Authors": ["G. Rabottino", "A. Mencattini", "R. Lojacono"], "Related Topics": "Medicine", "Citation Count": "23", "Reference Count": "25", "References": ["5a568905f56d5c424c671757552afc03e70f73fd", "e652b81c4aba8e104c8aee064cc90923293cdb82", "a496b4de4b92d905ec2daaecd9af154ea47481f3", "baac72cbca8efd95e76ea17577e8e90e49fc7275", "37ed2eba228d850b3b34472c216d4ed20e33e151", "e83fdb20beb9908fa6a8b52df03197ba84ef2188", "8d46682acbe8ac27ab8e29163e64f1b3cbb70bb0", "e4488ebd82674a981852c66c3138cb0bb9fe6b26", "04fc0c398508822f3b22a8065a48f17ed69baeb0", "295f223238012947e5ffc1e5a0e7c9bd52807977"]}, {"ID": "d0059280e3f69b8fd07ce036e7d2407e3ebcff9e", "Title": " Building an ensemble system for diagnosing masses in mammograms", "Abstract": "PurposeClassification of a suspicious mass (region of interest, ROI) in a mammogram as malignant or benign may be achieved using mass shape features. An ensemble system was built for this purpose and tested.MethodsMultiple contours were generated from a single ROI using various parameter settings of the image enhancement functions for the segmentation. For each segmented contour, the mass shape features were computed. For classification, the dataset was partitioned into four subsets based on\u2026\u00a0", "Publication Year": "2012", "Authors": ["Yu Zhang", "Noriko Tomuro", "D. Raicu"], "Related Topics": "Computer Science, Medicine", "Citation Count": "48", "Reference Count": "33", "References": ["995a9f5653e95ff874e39da5d2d0beeb36aaa950", "a9761b957e9e00dfe99b41c36afbf68e4dd6e5c8", "71f98dc5cc9409ceb35f057eb5cbe6ede187a1ba", "7577f9d9beecb8b5f97822cbcb9742f97afa9cd4", "9c4424874d34e511373e6daefd3630faa3921a80", "34c44883a6152c5298f2c452670c1127072400e6", "0ff3cefb443e5e0f168d8b3be38435848d8a93a1", "0db9038edba073a75014870b486981b8b8ed9050", "b5ec7cb440fc82239b940fbe02d21d9e94483d4f", "cc3a4e8bcb49ae31977cc3d99549529596e36fe7"]}, {"ID": "2dfb1fd3adfa58a3448251e03b1a5a78239958b3", "Title": " Classification of benign and malignant patterns in digital mammograms for the diagnosis of breast cancer", "Abstract": "", "Publication Year": "2010", "Authors": ["B. Verma", "P. McLeod", "A. Klevansky"], "Related Topics": "Computer Science, Medicine", "Citation Count": "99", "Reference Count": "26", "References": ["34c44883a6152c5298f2c452670c1127072400e6", "5fec51cf2d044c764a049d8455f96869d31f1e99", "cf7396f247441e51071d0a774a3ef7a83da6231a", "9bf32a68edfea8b8c072bcd3ee0d696687bab403", "88d38b88e4890b80373eed7070a2096648c5cf30", "483f0f12feb8ac1c396349e5526a7552b6b067cd", "995a9f5653e95ff874e39da5d2d0beeb36aaa950", "227b786b828240506830fea3660b5dc1de6e2a8e", "fb5989e1fff6024adcc6c3d4d85d43e728b9646d", "3f64347fcf97e81a22f421fc8facb8e229221fea"]}, {"ID": "46c409dd878e643271ef63f1817ded8b57abc01e", "Title": " Classification of benign and malignant masses based on Zernike moments", "Abstract": "", "Publication Year": "2011", "Authors": ["Amir Tahmasbi", "Fatemeh Saki", "S. B. Shokouhi"], "Related Topics": "Computer Science", "Citation Count": "246", "Reference Count": "54", "References": ["63a83600a73ffbfd6a58e315a247aa4f3da90a9b", "474ae46626676d01c7b38328c107b1531b181b46", "3e63f9f1323de17e96c4eaffda77b701d138c1f8", "ce1e33c037689acbfd19d4dcfc53021bc59dee2b", "40bbb7667b260bad22b0e2cb34562f57735d67f8", "d98b3a493bdf327f3447e75b4814339e1b436d05", "ea7685f5ee65c4c152478111b5bfcb23a446d358", "802c5bf4c93b795f9f509a3b90efabc501e21c01", "9bf32a68edfea8b8c072bcd3ee0d696687bab403", "603b2f00c9948e63514bdefa4ac2d3357cc2a1dc"]}, {"ID": "342da5d8633aebf27d914a8618e523579a130289", "Title": "Breast Cancer Detection Using Neural Network Models", "Abstract": "Breast cancer is the leading cause of death in women. If breast cancer is detected in early stage, then chances of survival are very high. In body new cells take place of old cells by orderly growth as old cells die out. The process of mutation controls the activation of genes in cells. Due to this cells get ability to go on dividing without control and producing cells like it, forming a tumor. This tumor can be of benign or malignant. The benign tumors are not dangerous while malignant tumors\u2026\u00a0", "Publication Year": "2013", "Authors": ["P. Pawar", "D. Patil"], "Related Topics": "Medicine", "Citation Count": "27", "Reference Count": "11", "References": ["1945ecc90dc463d65ac113fa0b89679bd8f31718", "cf6fe2d57289af4c401a8f6f006fe243f66d369b", "a8bd26fae7a98d45a8028371b1a3e96fca0ff64d", "c120a2f53a87602a97ba1a594a07305dd5a882eb", "d37bd9a7bc19ce640e33449831181fa3e43f90d2", "1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76", "045310b06e8a3983a363a118cc9dcc3f292970b4", "089a76dbc62a06ad30ae1925530e8733e850268e"]}, {"ID": "fe83150bc326fd62d352cb2993ac91344f195e10", "Title": "Support vector machines combined with feature selection for breast cancer diagnosis", "Abstract": "", "Publication Year": "2009", "Authors": ["M. Akay"], "Related Topics": "Computer Science", "Citation Count": "752", "Reference Count": "31", "References": ["82d5b4635858db58fd38dd829ae5e2c4d13e3e66", "19b751327a27a76c170d6c64f92623a2eee8e2f5", "e663fa59a758b945ed0e8e620a52022d617a9b03", "10ea1a358fecfd1623e230df98f11f967490c47a", "456a261579904536364a67207a44660304da5592", "c4a422669ec9b6a60b05d2d2595314008a5fb419", "9008cdacbdcff8a218a6928e94fe7c6dfc237b24", "1fda96d554f4e5a21e35bf33b9720141da47664b", "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8", "0d28c0390b21244cc52e9af856249cb601f6b22d"]}, {"ID": "483f0f12feb8ac1c396349e5526a7552b6b067cd", "Title": " A ranklet-based image representation for mass classification in digital mammograms.", "Abstract": "Regions of interest (ROIs) found on breast radiographic images are classified as either tumoral mass or normal tissue by means of a support vector machine classifier. Classification features are the coefficients resulting from the specific image representation used to encode each ROI. Pixel and wavelet image representations have already been discussed in one of our previous works. To investigate the possibility of improving classification performances, a novel nonparametric, orientation\u2026\u00a0", "Publication Year": "2006", "Authors": ["M. Masotti"], "Related Topics": "Computer Science", "Citation Count": "46", "Reference Count": "33", "References": ["ca75aca02d71ae1c2242024daf664d5753e32efa", "3a54399b141999271e49c652979f65fcd968daa9", "f876d2b733ed1e307c81df3008d10ffa60675f0b", "34856bb7b455a317126787076be40f4f57154273", "e764d70b416fa5fee1fd76ed21d64e26ef187bb9", "29dab22d41369fe0af3dd6bca85e0c5aa6859ded", "931d0f894fd46ada2051e2b451c1b3ee675a397c", "38f7c8cfb6b52df515c1cb29a2e42f6673e7a377", "413bb48a77c0730693ea8217d89cab28a686e7a3", "eddad94fd10693fb0cf3340792f11af15232b49c"]}, {"ID": "a9d0b3485f3091e832f87edb469c350c90cabae1", "Title": " Breast mass contour segmentation algorithm in digital mammograms", "Abstract": "", "Publication Year": "2013", "Authors": ["Tolga Berber", "A. Alpkocak", "O. Dicle"], "Related Topics": "Computer Science, Physics", "Citation Count": "67", "Reference Count": "31", "References": ["6c4aac0941b002d74264b7f702881e85f2eb47a2", "1c26e62c7d9719dcdd652932345090534d661629", "81b568a5899f22f6d8ea7dd2fc31f31f6d423e1a", "728d5be150b5ef7e2b22586f61b7a905b33ee7e6", "103a9fc043d9fcccec9d545a53a76e22dcfe9c3f", "c83e00120d7adcda72ac01b95234134c47cba76a", "8847e8f317f0c24da2e07addd5916030d21e95d7", "dda0dc40bc5cb79d5638d8cac955337362e6473e", "2d78c76f2696918bb9457d30e2b9b62c10ccae45", "f23ff10c8f24cb48f1b90f41d247e722b40a1453"]}, {"ID": "d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f", "Title": " Automatic Soccer Video Analysis and Summarization", "Abstract": "We propose a fully automatic and computationally efficient framework for analysis and summarization of soccer videos using cinematic and object-based features. The proposed framework includes some novel low-level soccer video processing algorithms, such as dominant color region detection, robust shot boundary detection, and shot classification, as well as some higher-level algorithms for goal detection, referee detection, and penalty-box detection. The system can output three types of summaries\u2026\u00a0", "Publication Year": "2003", "Authors": ["A. Ekin", "A. Tekalp"], "Related Topics": "Computer Science", "Citation Count": "904", "Reference Count": "40", "References": ["d6151de801659937574c3efe13c2d207e9e2f2cd", "339acc20ebbf3c08c797373f873e4e6b9f2f9c9a", "7b068d9ff88588a15d6d5fa8e7932d4e635cd252", "564649846003db680733697947f974a1ef03c4ea", "5a7571db7df03cca52c48f89595c4abefeb51e5c", "a3a7a3fe0fc54665d5e22461a27e2aa2816666a8", "832f48f5c28956e892e0ece93e2802f4501036ab", "f0bafbf9cf2dfb1fd7e439e2336f1dd3af19478c", "5055097cd7279bc3cd16474e1629cf0c24ed7249", "c8de9f939f497d430c15d5bfa54174e38d4ac681"]}, {"ID": "d71c84fbaff0d7f2cbcf2f03630e189c58939a4a", "Title": " Semantic analysis of soccer video using dynamic Bayesian network", "Abstract": "Video semantic analysis is formulated based on the low-level image features and the high-level knowledge which is encoded in abstract, nongeometric representations. This paper introduces a semantic analysis system based on Bayesian network (BN) and dynamic Bayesian network (DBN). It is validated in the particular domain of soccer game videos. Based on BN/DBN, it can identify the special events in soccer games such as goal event, corner kick event, penalty kick event, and card event. The video\u2026\u00a0", "Publication Year": "2006", "Authors": ["Chung-Lin Huang", "H. Shih", "Chung-Yuan Chao"], "Related Topics": "Computer Science", "Citation Count": "162", "Reference Count": "26", "References": ["d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f", "b5a23310cdc5b492175325ba90af69ecda3dc377", "5055097cd7279bc3cd16474e1629cf0c24ed7249", "c51d7cbfb95ee370d1eddb4e0ff03290b8bb479a", "cc85119fdac7f6e9b0afa5e5a87983f6bca2f1c9", "3dcd7ed1905e94b06b0c8f087007f00951b030ab", "d6151de801659937574c3efe13c2d207e9e2f2cd", "54e04284d0d33fabcd7be961ff29af2223638dee", "5a7571db7df03cca52c48f89595c4abefeb51e5c", "c7967ff0c51732110e0e1470975fe0a974fa8a2e"]}, {"ID": "43eb2d80721daa318cc19e4cce405122934e88f2", "Title": " Event Detection of Broadcast Baseball Videos", "Abstract": "This paper presents an effective and efficient event detection system for broadcast baseball videos. It integrates midlevel cues including scoreboard information and shot transition patterns into event classification rules. First, a simple scoreboard detection and recognition scheme is developed to extract the game status from videos. Then, a shot transition classifier is designed to obtain the shot transition patterns, which contains several novel schemes including adaptive playfield\u2026\u00a0", "Publication Year": "2008", "Authors": ["Mao-Hsiung Hung", "C. Hsieh"], "Related Topics": "Computer Science", "Citation Count": "38", "Reference Count": "16", "References": ["d0ec6dee3f7cd0c1571d179d756afc823cda636b", "8268944e509094df7eda14e3963d1add1db87528", "ed9456c605c18aeb9b0aff5eaae4038ef4adb564", "a0b6c6f8a8b12b843ca1b7c3430b2f0494ec84c2", "3dcd7ed1905e94b06b0c8f087007f00951b030ab", "e89332600d8abd393bcd8020d69c6a7a3298a966", "d71c84fbaff0d7f2cbcf2f03630e189c58939a4a", "5727fdb06696f92d406adb81459dd2e56da1dfeb", "263eb915898a9e79ff1cb4d234dacd6b2ea72be3", "ce8883dbe8c695d8ef770eefe34e48efe7c0cf64"]}, {"ID": "0cca81f3b2928f6d4005e1a8b617960e2fb14d3e", "Title": " Knowledge-Discounted Event Detection in Sports Video", "Abstract": "Automatic events annotation is an essential requirement for constructing an effective sports video summary. Researchers worldwide have actively been seeking the most robust and powerful solutions to detect and classify key events (or highlights) in different sports. Most of the current and widely used approaches have employed rules that model the typical pattern of audiovisual features within particular sport events. These rules are mainly based on manual observation and heuristic knowledge\u2026\u00a0", "Publication Year": "2010", "Authors": ["D. Tjondronegoro", "Yi-Ping Phoebe Chen"], "Related Topics": "Education", "Citation Count": "89", "Reference Count": "37", "References": ["c7bc06203ee09cf2066b47c3ac1fa444dfa8d878", "217478d6a95a5bceef11d7846895b57718d63e73", "ad31e976cfa5cba56b84a2f7e05adceee99366df", "8e0f60b718fa19c2ed10bd93401796683af79512", "cb6742b271ae81baa217d2d6391ead067f6e7018", "cc8756e654c8c3016c1e86189b76fe6b8a08773a", "ba73512ff40f576a4ab6a8f1be6de08256fdf038", "77eba439796f8482a1f36be31dfddbae7536d792", "5a7571db7df03cca52c48f89595c4abefeb51e5c", "5727fdb06696f92d406adb81459dd2e56da1dfeb"]}, {"ID": "0365c1382924394e200cb627e59cb9c21f8e75bd", "Title": " A semantic event-detection approach and its application to detecting hunts in wildlife vide", "Abstract": "We propose a three-level video-event detection methodology and apply it to animal-hunt detection in wildlife documentaries. The first level extracts color, texture, and motion features, and detects shot boundaries and moving object blobs. The mid-level employs a neural network to determine the object class of the moving object blobs. This level also generates shot descriptors that combine features from the first level and inferences from the mid-level. The shot descriptors are then used by the\u2026\u00a0", "Publication Year": "2000", "Authors": ["N. Haering", "R. J. Qian", "M. Sezan"], "Related Topics": "Computer Science", "Citation Count": "130", "Reference Count": "44", "References": ["e7bd444bbe813273dee084e9efc67d95f411cc19", "93954c1e3cee3039d2b6b53c61ff6137e9e335bd", "3eeebdce6255b997c4ced11d9d45d5bef421b2c6", "54ecdf01c1bbbe8106cd27a35aed672c3564ef34", "3a8ce1bcf4a092761246d99f0cde788f1804577f", "dc185ecd84439165d6cfe90001997cab9b202736", "eea56eadd6be66cff71748ef7d9ab54b61033bd8", "9895be389b31013b477e3bb48a006ad5c73f3a14", "19c8dc7b4acdeecf092526d767156ac8950c02d8", "edea2f25d705d43ce90f725eed62f7dba6fbd50f"]}, {"ID": "c9edc10d5c22ae62a700c37a41bb0ea22961a0aa", "Title": "Integrated Mining of Visual Features, Speech Features, and Frequent Patterns for Semantic Video Annotation", "Abstract": "To support effective multimedia information retrieval, video annotation has become an important topic in video content analysis. Existing video annotation methods put the focus on either the analysis of low-level features or simple semantic concepts, and they cannot reduce the gap between low-level features and high-level concepts. In this paper, we propose an innovative method for semantic video annotation through integrated mining of visual features, speech features, and frequent semantic\u2026\u00a0", "Publication Year": "2008", "Authors": ["V. Tseng", "Ja-Hwung Su", "Chih-Jen Chen"], "Related Topics": "Computer Science", "Citation Count": "63", "Reference Count": "26", "References": ["a544b12d39c059a1a9ba7da3d5fe78747c9cabaa", "0d10531bea859670320ff1fbfd882af8dcf9abf3", "e7afd8e942e370e7fdaf3f395492c4aba0b8080b", "30ee51ff3120051bc30d64b2a80cc7edcba7d511", "3a8ce1bcf4a092761246d99f0cde788f1804577f", "fad43474e4fa71eb2527ef30adfa7bb7870baa83", "933faea49491113927c18d6739ed24fc5d8624eb", "ba4e1089e2c5a1c12e9f6c2686e9c8d1870c718e", "09460c5170f5b65e0772cd8d18491accfd9d78d9", "ed7368b4c65d872f6886260271b9b94c2fa2b89b"]}, {"ID": "90fbd777cf57096e9292601dfe0dbab30198d40f", "Title": " Using Webcast Text for Semantic Event Detection in Broadcast Sports Video", "Abstract": "Sports video semantic event detection is essential for sports video summarization and retrieval. Extensive research efforts have been devoted to this area in recent years. However, the existing sports video event detection approaches heavily rely on either video content itself, which face the difficulty of high-level semantic information extraction from video content using computer vision and image processing techniques, or manually generated video ontology, which is domain specific and\u2026\u00a0", "Publication Year": "2008", "Authors": ["Changsheng Xu", "Yifan Zhang", "Qingming Huang"], "Related Topics": "Computer Science", "Citation Count": "162", "Reference Count": "44", "References": ["7eb16b3e160622fbf58836b2fb884af30a2b19b5", "cc8756e654c8c3016c1e86189b76fe6b8a08773a", "e891530c62ab431de4330693ed5dbda8509802ce", "d993c5d2a62242c9a20550587379c15f6d4ce860", "d0ec6dee3f7cd0c1571d179d756afc823cda636b", "833deab2d9f7bde03848c58b5d8066d153ee60af", "20b253b8846814b5e06007cb337785b963633308", "217478d6a95a5bceef11d7846895b57718d63e73", "99009072d31cf16cd817dffd0aac6b134d71ddb5", "7b29d588cf0d910f867f8eeee3d0a2b0e183e0c8"]}, {"ID": "f7b4fcc9dbb97997dc1793d7a366cba274f53134", "Title": " Automatic player detection, labeling and tracking in broadcast soccer video", "Abstract": "", "Publication Year": "2009", "Authors": ["Jia Liu", "Xiaofeng Tong", "Hongqi Wang"], "Related Topics": "Computer Science", "Citation Count": "171", "Reference Count": "25", "References": ["cc4d03faf65c7fad0eeb345ae0db102a7be3770f", "f12307a3b6d6e12174b31ffb2644108089855f04", "fd294153765faf0851e08a751d3849b4a71e1eb1", "38dd730a7d2466b3bde4674daddecb6532fd3ae3", "dc5181c650b3a7b86989c2d3179fa0f9bcdeb3c3", "197c7b40c4f5ceb6b1d862de0bfc27b57e61d19d", "b46da16dca784e66f600cfa05aa3d9d8bc1dee6d", "f101e675936ea326354ca3c9d4d70699f1a71843", "fb444dc25bab36a8e273ed654d49e3841905e5af", "2c9326ed69e586b08dff3c0da0a2bcab86bbc15e"]}, {"ID": "8e0f60b718fa19c2ed10bd93401796683af79512", "Title": " Event detection in field sports video using audio-visual features and a support vector Machine", "Abstract": "In this paper, we propose a novel audio-visual feature-based framework for event detection in broadcast video of multiple different field sports. Features indicating significant events are selected and robust detectors built. These features are rooted in characteristics common to all genres of field sports. The evidence gathered by the feature detectors is combined by means of a support vector machine, which infers the occurrence of an event based on a model generated during a training phase\u2026\u00a0", "Publication Year": "2005", "Authors": ["D. A. Sadlier", "N. O'Connor"], "Related Topics": "Computer Science", "Citation Count": "258", "Reference Count": "45", "References": ["6a9a5667b7595bf06290665f7b32f540e60dbbc3", "5a7571db7df03cca52c48f89595c4abefeb51e5c", "1248d51ccfbcbc89f5682774f5f9f88da4f68611", "e2beed07f3841bc6f97efa2fa65b232d15f6e9d2", "217478d6a95a5bceef11d7846895b57718d63e73", "d203dbaf6047ac6e1cdfecb8b753bd2593f1023b", "c51d7cbfb95ee370d1eddb4e0ff03290b8bb479a", "0ba6ab976e3ec7650df31642ea58759d8bae46f5", "20b253b8846814b5e06007cb337785b963633308", "278dc8bce9b16b135a26a1651db7e3f43eb289ac"]}, {"ID": "c2f8178ce89a6cc1ad0e1dd5db4bf155d5a80620", "Title": "A Video Event Detection and Mining Framework", "Abstract": "We present a video event mining framework that consists of comprehensive set of tools for event detection, annotation, content browsing and a video analysis database. Central to our framework is the video analysis database and the VideoViews database browser that supports both top-down and bottom-up analysis of the video data. to support event mining. We present two methods for video event detection, namely an expert system (CLIPS) rules based approach and a 2-level Hidden Markov Model built\u2026\u00a0", "Publication Year": "2003", "Authors": ["S. Guler", "Winnie H. Liang", "Ian A. Pushee"], "Related Topics": "Computer Science", "Citation Count": "18", "Reference Count": "15", "References": ["ecab8d10ca24b53eca2bf1580e8cd03fe7984676", "1bfe26fac93ad96c81cf1a580b9e7744477f56aa", "518597d91ed49c28f5cf3f0a0b05609568b7e084", "bc4b948b1a0f91525bc3d47e9e192b392bf790ed", "3ff52ba9498b2ac084c9d8bbf637c343679df402", "87cc226aa060db976fbf6ac3a07969b33b544b96", "824aac4970a4d149b35c19a9d2d2dec4c994688e", "18b02beb27288f6bd9d4376ca41e70655a698084", "1c99600451dedd42dcdc02ed6cfcaa81e70e9899", "9fd7f3022db657ef5e9619209962e5525ffdce4e"]}, {"ID": "e1dfff8cff33ede3564a35724632bddc5b8619b1", "Title": " A fast and efficient fuzzy color transfer method", "Abstract": "Each image has its own color content that greatly influences the perception of human observer. Being able to transfer the color content of an image into another image, while preserving other features, (like texture), opens a new horizon in human-perception-based image processing. In this paper, after a brief review on the few efficient works performed in the field, a novel fuzzy principle component analysis (PCA) based color transfer method is proposed. The proposed method accomplishes the\u2026\u00a0", "Publication Year": "2004", "Authors": ["A. Abadpour", "S. Kasaei"], "Related Topics": "Computer Science", "Citation Count": "45", "Reference Count": "13", "References": ["05f63bdf9e60d0a299cfe5e8d7ba043904f1fea1", "d5c6edb53dc41f298f145041cd2c53e40e3acf2b", "3fed78dcdcf2588f2f1b34ad6885a60789574203", "4e4504e867e867f8b2bc366b75e036686582e5bf", "b5bd72d8bc9f51dae65c15842f0ab443c3b437e3", "577d19a115f9ef6f002483fcf88adbb3b5479556", "f3a11158e9d8bdfdf07dca756335c084fce0123e", "78b4f65c167185f18b573433e5f3e8814acf656f", "a9407584b7641f70bf0882e495ddef561d0ee62b"]}, {"ID": "d93e4e77e2baba98f1af7f23d47fbf9b46be4df5", "Title": " New Principle Component Analysis Based Colorizing Method", "Abstract": "Although many modern imaging systems are still producing grayscale images, colored-images are more preferred for the larger amount of information they are carrying. Computing the grayscale representation of a color image is a straightforward task, while the inverse problem has no objective solution. The search through out literature has not revealed much history of the past works. In this paper, after a brief review of related research, a new dimensionreduction method is proposed for natural\u2026\u00a0", "Publication Year": "2004", "Authors": ["A. Abadpour", "S. Kasaei"], "Related Topics": "Computer Science", "Citation Count": "6", "Reference Count": "18", "References": ["d5c6edb53dc41f298f145041cd2c53e40e3acf2b", "ab67b9d0da50e251a4f7e42370540547b891ceb1", "b5bd72d8bc9f51dae65c15842f0ab443c3b437e3", "cc470d34b6d76518ef4435b627ba1ec01ac55c03", "95a057bf3b2b7af6778e30847ad8177191ec43c9", "3cf04e19e55cf6d2d18157c136885a042ab578d1", "577d19a115f9ef6f002483fcf88adbb3b5479556", "8d946c3eb1d1db376a89ad9342282163b5ae0930", "1061dea79f8c5e55bf11f7873b9de109c51cbc67", "f3a11158e9d8bdfdf07dca756335c084fce0123e"]}, {"ID": "05f63bdf9e60d0a299cfe5e8d7ba043904f1fea1", "Title": "Fast algorithms for color image processing by principal component analysis", "Abstract": "", "Publication Year": "2003", "Authors": ["Shyi-Chyi Cheng", "Shih-Chang Hsia"], "Related Topics": "Computer Science", "Citation Count": "35", "Reference Count": "14", "References": ["03cbb2aff1cb933886edca9ea98161a8896552c5", "94ffa7842b83a4326b7d23a1face2c296edef92c", "fdc9ae0ec9b249b008a9592ddae98881bd46b606", "1e4039217f8f594d8f1be9ca1dd452589dafdcaa", "616388801bd609f9a905f64bbaa08534a4cd3dac", "d367a7305b26d0f6fbe45e85a8ffe8ab4afec718", "91df6a976c1cbed21b67a66d5de81db61ae63f3a", "8a54d38754c216ad26d2c3e61fee980a57a9ad56", "d520e606c8819b465e6eaaad2ee891ac23c42155", "0e90e6853d04c5dc03893c1269320f3828550651"]}, {"ID": "0c2ced886708cc3aea4705f8765d152cd3f69cd2", "Title": " A new FPCA-based fast segmentation method for color images", "Abstract": "Fuzzy objective function-based clustering methods are proved to be fast tools for classification and segmentation purposes. Unfortunately, most of the available fuzzy clustering methods are using the spherical or ellipsoidal distances, which are proved to result in spurious clusters, when working on color data. In this paper, a general case of clustering is discussed and a general method is proposed and its convergence is proved. Also, it is proved that the FCM and the FCV methods are special\u2026\u00a0", "Publication Year": "2004", "Authors": ["A. Abodpour", "S. Kasaei"], "Related Topics": "Computer Science", "Citation Count": "12", "Reference Count": "41", "References": ["5ac1bbc582e591e2560d1a3167f30fd5a9073b25", "2377b94096298613d0f06f7c8110a5303bc09f53", "4debea4d29ca203433c830a1dd1c0c1bb4828b6c", "253b74d147ba829b9b1926c478815f1d904f9e36", "1186c8e998b2a1c3dd87e55400929d753877bd19", "1380bcf86538fef43dd2356d71b64523867b58c0", "a34c9af1897c779be9aee293ac43e1dca097a33c", "d738b2654fcdc4569d036fbd958b8151eab4ba19", "fd3828d1465baf3719195ad98971fad66162ce67", "989af4dd904c1958e5f9a6f08f70572259303425"]}, {"ID": "a2882b8b0c9635d39d15a28138e3f47907f3177b", "Title": "Linear color segmentation and its implementation", "Abstract": "", "Publication Year": "2004", "Authors": ["D. Nikolaev", "P. Nikolayev"], "Related Topics": "Computer Science", "Citation Count": "50", "Reference Count": "32", "References": ["015e56c1042e0be60154fac7095bcc681a0c2960", "1e48105dd2b6d4a21be627040fa6e2074a576bef", "84818dc24b6efe355ee5bcd7ac1c28473d0e8e3d", "5e3ab08e93e5eb529692825cfedf6d3b6763bd76", "157282748f001bc9876f9eaf1d53b98a6e579f19", "f4145c995bd7fc3415d8a366a2bf25d100d7b9a9", "5924e7dc6c65efc2a7482dadc8f1d3585e6a420d", "794537f1939fad46800d6cf678ea07dc383c4a30", "281b195c5b155c461d679444c3e2aeb06aa7e351", "3a1c431cb819dc5ced519a6be65d6b4a0658aeea"]}, {"ID": "61579369e7b97dec9c699c058edaafcde2817d21", "Title": " Multithresholding of color and gray-level images through a neural network technique", "Abstract": "", "Publication Year": "2000", "Authors": ["N. Papamarkos", "C. Strouthopoulos", "I. Andreadis"], "Related Topics": "Computer Science", "Citation Count": "95", "Reference Count": "20", "References": ["a050eb794e7cb65207d998371f8f0287e7ed53ab", "42c40ae5648dabf463fb43f24441cf3253782a12", "1d4816c612e38dac86f2149af667a5581686cdef", "53e4349b6e585426389059c5107a366b8873eb67", "ce1e7aa9fd5ba2f54b342b7cac2625835771daf2", "2c419cfb621051a3a007a4e937a3da22f18653e1", "af7dcf5e823f6e1eaa4aff2afa2912585ea32147", "1d8237039244543cf8bca7cc022cf6c8e39aa260", "ab67b9d0da50e251a4f7e42370540547b891ceb1", "69d3e3535263f1e240920f47ebb462658b3d3761"]}, {"ID": "4a6c5c9b1fb106f7d82508ae593d30e207c8ea45", "Title": " Color Segmentation Based on Separate Anisotropic Diffusion of Chromatic and Achromatic Channels", "Abstract": "The paper presents a new technique for segmenting images only on the basis of colour information. It is shown how segmentation can benefit from splitting colour signals into chromatic and achromatic channels and separately smoothing them through anisotropic diffusion. Operatively, this is accomplished through two independent diffusion processes: one involves only the chromatic information, conveniently embedded in a complex function, while the other affects the lightness information. The\u2026\u00a0", "Publication Year": "2001", "Authors": ["L. Lucchese", "S. Mitra"], "Related Topics": "Computer Science", "Citation Count": "34", "Reference Count": "31", "References": ["251a7ee2ecd78aff6e83318c92b01a55bf6a762f", "f566985a7df1dcd6af66b019a50338aeea5f1cf2", "496c3d75b81b336411e53da1ac632a8139655604", "3a15aa74d7db2004b8895ee822170231f95a0b6c", "674285f115841d8a237a68e55b4e651cc558bf9d", "fa4dce7d484da0d91d872261e0c41006521e732f", "8854a69749b2a02afbf880a413f77988eaacbfde", "e9e42f0079b6d9ab86857e418a7d5157e381928a", "d077f1a275c922e288cda1e3fa949154316503cf", "a9b9866314054a7c8386bd44362e9034a0690007"]}, {"ID": "ab67b9d0da50e251a4f7e42370540547b891ceb1", "Title": "Color information for region segmentation", "Abstract": "", "Publication Year": "1980", "Authors": ["Y. Ohta", "T. Kanade", "T. Sakai"], "Related Topics": "Physics", "Citation Count": "997", "Reference Count": "2", "References": ["8421556ed9e1b67be2d48de9f47629b4f9a7013b", "b30de454c99b8006db60cd53bea5f9f2d6fc8cd4"]}, {"ID": "d5c6edb53dc41f298f145041cd2c53e40e3acf2b", "Title": " Transferring color to greyscale images", "Abstract": "We introduce a general technique for \"colorizing\" greyscale images by transferring color between a source, color image and a destination, greyscale image. Although the general problem of adding chromatic values to a greyscale image has no exact, objective solution, the current approach attempts to provide a method to help minimize the amount of human labor required for this task. Rather than choosing RGB colors from a palette to color individual components, we transfer the entire color \"mood\u2026\u00a0", "Publication Year": "2002", "Authors": ["T. Welsh", "M. Ashikhmin", "K. Mueller"], "Related Topics": "Computer Science", "Citation Count": "872", "Reference Count": "11", "References": ["8d8d0a2d534c3cee59e7e325c07c94d5869a67ca", "923562d216386a88947d40da310d94bbb1376a41", "dd7bf950093fc65f3ae6ad79666ce1077f9dfb2e", "bba3264d6794538381687ad6e151a7f42f3872a9", "f3a11158e9d8bdfdf07dca756335c084fce0123e", "78b4f65c167185f18b573433e5f3e8814acf656f", "a5bca61f72c7eb77bef61ef7e152113ec6cfdff1", "c636437d53514d8f59ed9e7cab165d33b2b86aa2", "b85f5a1497e471587653e2337e8eb87b914795fe"]}, {"ID": "ed7e166f65bcecc522c6c4bbb29fcf8048010873", "Title": "GRAYSCALE IMAGE MATTING AND COLORIZATION", "Abstract": "This paper presents a novel approach to grayscale image matting and colorization. The first part of this approach is an efficient grayscale image matting algorithm in Bayesian framework. The foreground and background color distributions, and the alpha\u2019s distribution are modelled with spatially varying sets of Gaussians. The major novelties of this matting algorithm are the introduction of alpha\u2019s distribution and gradient into the Bayesian framework and an efficient optimization scheme. This\u2026\u00a0", "Publication Year": "2004", "Authors": ["Tongbo Chen", "Yan Wang", "C. Meinel"], "Related Topics": "Computer Science", "Citation Count": "63", "Reference Count": "18", "References": ["d5c6edb53dc41f298f145041cd2c53e40e3acf2b", "03b2a8c90b9e5068bb05bfc885588e647f97356d", "61a1c4ae4dbc182e2923c495339466bb0812f53d", "8fd18a0f65134b1abfbb1adf8653ebba63bb2c0e", "923562d216386a88947d40da310d94bbb1376a41", "46c20018841c0ae8226e7cb5d7107ff30742f8f5", "41093fe0f19cb37b239a62adb5f2c0cd058fec83", "64ce3c02fde4157458d84b977dc74a3bd7eda50d", "ec5ece85d618d71bffa9e6d655fe2f38416a4e9d", "69a28cf71d454b06eed2aed3c7a48114ea969455"]}, {"ID": "26e2ca763087be09e3799ad294302aa91077942d", "Title": "Deep visual tracking: Review and experimental comparison", "Abstract": "", "Publication Year": "2018", "Authors": ["Peixia Li", "D. Wang", "Huchuan Lu"], "Related Topics": "Computer Science", "Citation Count": "415", "Reference Count": "105", "References": ["bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0", "bc4cfc075e406f9f5c621fe27a3e0002eec4a8b3", "f546534e4608ca4519be0d27ca6b0e8e76fa11f0", "b2180fc4f5cb46b5b5394487842399c501381d67", "1b3a107739e7f7e05c50999a3d79b8225746f662", "e3c433ab9608d7329f944552ba1721e277a42d74", "c46b08850b9c458704a3ca69172e6a0d40a6cb7f", "f48d00ff375327c8743ade0fccf60db845f4a826", "6ac3acd8c0cd3d6b234cb2c3f9ca747056c794e0", "28c0b726a0673ccd55ffb7ea002d82d7bee83dcd"]}, {"ID": "021d0c7013da519b508610064f264c76d768fdf1", "Title": " Real-Time Deep Tracking via Corrective Domain Adaptation", "Abstract": "Visual tracking is one of the fundamental problems in computer vision. Recently, some deep-learning-based tracking algorithms have been illustrating record-breaking performances. However, due to the high complexity of neural networks, most deep trackers suffer from low tracking speed and are, thus, impractical in many real-world applications. Some recently proposed deep trackers with smaller network structure achieve high efficiency while at the cost of significant decrease in precision. In\u2026\u00a0", "Publication Year": "2019", "Authors": ["Hanxi Li", "Xinyu Wang", "Mingwen Wang"], "Related Topics": "Computer Science", "Citation Count": "12", "Reference Count": "44", "References": ["2ac7ab669a56af6ada5cc1459f2c7e93dcdb025a", "b2180fc4f5cb46b5b5394487842399c501381d67", "f24015a365ea2454391c285cd30b8ae723dbb05e", "5f0850ec47a17f22ba2611a5cb67a30cb02cf306", "29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "2e195a4edeae8d6be0885d7fd9cb7c70f365a326", "1b3a107739e7f7e05c50999a3d79b8225746f662", "9cf3c67529085d31c646091b97be1a1e3dc191f2", "e3c433ab9608d7329f944552ba1721e277a42d74", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9"]}, {"ID": "0a400fd7f0ee28694889baaa4faef150b6912dfa", "Title": "An In-Depth Analysis of Visual Tracking with Siamese Neural Networks", "Abstract": "This survey presents a deep analysis of the learning and inference capabilities in nine popular trackers. It is neither intended to study the whole literature nor is it an attempt to review all kinds of neural networks proposed for visual tracking. We focus instead on Siamese neural networks which are a promising starting point for studying the challenging problem of tracking. These networks integrate efficiently feature learning and the temporal matching and have so far shown state-of-the-art\u2026\u00a0", "Publication Year": "2017", "Authors": ["R. Pflugfelder"], "Related Topics": "Computer Science", "Citation Count": "17", "Reference Count": "129", "References": ["388d29f001411ff80650f80cf197afc440d98b51", "7574b7e5a75fdd338c27af5aeb77ab79460c4437", "26e2ca763087be09e3799ad294302aa91077942d", "311bc4e48838d8e5ef619df3ce0bc598aba788a1", "b2180fc4f5cb46b5b5394487842399c501381d67", "ca302b4d7e2d50cb4a4970b78fc237a1294ade40", "29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "5b4b43f10c5779d67ccee15d8d0be10ed036971b", "a3a4471e82260f573d240cc34aeff431cf236571", "c316d5ec14e5768d7eda3d8916bddc1de142a1c2"]}, {"ID": "311bc4e48838d8e5ef619df3ce0bc598aba788a1", "Title": " Convolutional Features for Correlation Filter Based Visual Tracking", "Abstract": "", "Publication Year": "2015", "Authors": ["Martin Danelljan", "Gustav H\u00e4ger", "M. Felsberg"], "Related Topics": "Computer Science", "Citation Count": "858", "Reference Count": "42", "References": ["1b3a107739e7f7e05c50999a3d79b8225746f662", "b2180fc4f5cb46b5b5394487842399c501381d67", "09769e80cdf027db32a1fcb695a1aa0937214763", "f5dbe4550d24d5374d9e10fce44a35b105c7ee07", "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "e6824bc799efcf5cb854591474c92cdbb5716e32", "0cae491292feccbc9ad1d864cf8b7144923ce6de", "5b4b43f10c5779d67ccee15d8d0be10ed036971b", "6270baedeba28001cd1b563a199335720d6e0fe0", "9ea03a9cb11bdc68ac2f56e290c8486868511476"]}, {"ID": "388d29f001411ff80650f80cf197afc440d98b51", "Title": " Good Features to Correlate for Visual Tracking", "Abstract": "During the recent years, correlation filters have shown dominant and spectacular results for visual object tracking. The types of the features that are employed in this family of trackers significantly affect the performance of visual tracking. The ultimate goal is to utilize the robust features invariant to any kind of appearance change of the object, while predicting the object location as properly as in the case of no appearance change. As the deep learning based methods have emerged, the\u2026\u00a0", "Publication Year": "2017", "Authors": ["Erhan Gundogdu", "A. Alatan"], "Related Topics": "Computer Science", "Citation Count": "150", "Reference Count": "73", "References": ["311bc4e48838d8e5ef619df3ce0bc598aba788a1", "5404718135548b01516a668e0c022c5cb22b422e", "09769e80cdf027db32a1fcb695a1aa0937214763", "1b3a107739e7f7e05c50999a3d79b8225746f662", "5c8a6874011640981e4103d120957802fa28f004", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "f5dbe4550d24d5374d9e10fce44a35b105c7ee07", "084bd219dd239dc4c9a02621a5333d3bc1446566", "000178cd12c8a6e5da8215b6365fae03c20fd18d", "b4035bb1dc4514a72f069d911011ab5845ca1591"]}, {"ID": "f24015a365ea2454391c285cd30b8ae723dbb05e", "Title": " Deep tracking with objectness", "Abstract": "Visual tracking is a fundamental problem in computer vision. However, due to the (sometimes) ambiguous target information given at the first frame, it has also been criticized as less well-posed compared with other tasks with clearly-defined targets, such as object detection and semantic segmentation. In this paper, we try to evaluate the importance of object category in visual tracking by tracking objects with known object types. The proposed algorithm, termed Deep-Track with Objectness (DTO\u2026\u00a0", "Publication Year": "2017", "Authors": ["Xinyu Wang", "Hanxi Li", "Mingwen Wang"], "Related Topics": "Computer Science", "Citation Count": "6", "Reference Count": "24", "References": ["b2180fc4f5cb46b5b5394487842399c501381d67", "5b9ace65f7368f6dc6907c8f6f7c3b0c248d9bc4", "29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "5c8a6874011640981e4103d120957802fa28f004", "421bf4eeba623f722bf98340d71e3d229881e92d", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "1b3a107739e7f7e05c50999a3d79b8225746f662", "9d57723b4908397654fb1846d37db403d8b2b56a", "d20d7d3490fd970992b3631048c75a8c5fe2e4e3", "5f0850ec47a17f22ba2611a5cb67a30cb02cf306"]}, {"ID": "7574b7e5a75fdd338c27af5aeb77ab79460c4437", "Title": " Learning Dynamic Siamese Network for Visual Object Tracking", "Abstract": "How to effectively learn temporal variation of target appearance, to exclude the interference of cluttered background, while maintaining real-time response, is an essential problem of visual object tracking. Recently, Siamese networks have shown great potentials of matching based trackers in achieving balanced accuracy and beyond realtime speed. However, they still have a big gap to classification & updating based trackers in tolerating the temporal changes of objects and imaging conditions. In\u2026\u00a0", "Publication Year": "2017", "Authors": ["Qing Guo", "Wei Feng", "Song Wang"], "Related Topics": "Computer Science", "Citation Count": "610", "Reference Count": "36", "References": ["29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "5f0850ec47a17f22ba2611a5cb67a30cb02cf306", "09769e80cdf027db32a1fcb695a1aa0937214763", "c316d5ec14e5768d7eda3d8916bddc1de142a1c2", "5c8a6874011640981e4103d120957802fa28f004", "311bc4e48838d8e5ef619df3ce0bc598aba788a1", "0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5", "3dc60732c1c08165c9d4e7b334ce66e511474bb2", "3d5fe9ef560c08f0c56249360247c7d4b40ce023", "c46b08850b9c458704a3ca69172e6a0d40a6cb7f"]}, {"ID": "320d05db95ab42ade69294abe46cd1aca6aca602", "Title": " High Performance Visual Tracking with Siamese Region Proposal Network", "Abstract": "Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal\u2026\u00a0", "Publication Year": "2018", "Authors": ["Bo Li", "Junjie Yan", "Xiaolin Hu"], "Related Topics": "Computer Science", "Citation Count": "1", "Reference Count": "41", "References": ["29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "1131c53b9baaa740a4deef4c1282821b23d18687", "7ccbb845829234548bfa9b24c61297b4f0cd678e", "5404718135548b01516a668e0c022c5cb22b422e", "c2046fc4744a9d358ea7a8e9c21c92fd58df7a64", "966aad492f75b17f698e981e008b73b51816c6aa", "c316d5ec14e5768d7eda3d8916bddc1de142a1c2", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "dda27eb7ddc4510f94cac0e5134b5d56aa77b075", "5f0850ec47a17f22ba2611a5cb67a30cb02cf306"]}, {"ID": "1855818c492d5f42dbe14814e4dd9b5733d54790", "Title": "Densely Connected Discriminative Correlation Filters for Visual Tracking", "Abstract": "Discriminative Correlation Filters (DCFs)-based approaches have recently achieved competitive performance in visual tracking. However, such conventional DCF-based trackers often lack the discriminative ability due to the shallow architecture. As a result, they can hardly tackle drastic appearance variations and easily drift when the target suffers heavy occlusions. To address this issue, a novel densely connected DCFs framework is proposed for visual tracking. We incorporate multiple nested\u2026\u00a0", "Publication Year": "2018", "Authors": ["Cheng Peng", "Fanghui Liu", "N. Kasabov"], "Related Topics": "Computer Science", "Citation Count": "3", "Reference Count": "41", "References": ["49a5aeefcb257ca92652acf4f875efbad5a2b00d", "09769e80cdf027db32a1fcb695a1aa0937214763", "5c8a6874011640981e4103d120957802fa28f004", "ece7625a346edbc5f6fab541c0c246ec06939121", "b16a583ee173f222c690242aaff7925838893fe8", "0cae491292feccbc9ad1d864cf8b7144923ce6de", "0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5", "096710211d9e4eb77dc2d0f11a7ff818c8acc5ff", "29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "000178cd12c8a6e5da8215b6365fae03c20fd18d"]}, {"ID": "e2e34b202363e4a46a14cd35fd4088d88b2e650e", "Title": "Visual Tracking via Auto-Encoder Pair Correlation Filter", "Abstract": "Robust visual tracking is one of the most challenging problems in computer vision applications. However, the limited training data and the computational complexity have severely affected tracking performance. In this paper, we propose an auto-encoder pair model for visual tracking which is composed of source domain network and target domain network to help a more accurate localization. We adopt the dense circular samples of the object state to increase the number of training samples and prevent\u2026\u00a0", "Publication Year": "2020", "Authors": ["Xu Cheng", "Yifeng Zhang", "Yuhui Zheng"], "Related Topics": "Computer Science", "Citation Count": "14", "Reference Count": "54", "References": ["09769e80cdf027db32a1fcb695a1aa0937214763", "b4035bb1dc4514a72f069d911011ab5845ca1591", "7574b7e5a75fdd338c27af5aeb77ab79460c4437", "311bc4e48838d8e5ef619df3ce0bc598aba788a1", "6410c97ae03d356e14544c8e95f5367fb7ebb6e6", "b2180fc4f5cb46b5b5394487842399c501381d67", "6683442ae358ae4261fdcde0164f83dd1ccd621b", "7069a994c150b0228c4e471ca48ed55d7646bc62", "c2046fc4744a9d358ea7a8e9c21c92fd58df7a64", "776bc8955e801f6965e85b35d8e2dd6f2f1498ad"]}, {"ID": "6179ac06f1a8fd1ac6b693b02824948dff438d54", "Title": " The Visual Object Tracking VOT 2016 Challenge Results", "Abstract": "The Visual Object Tracking challenge VOT2016 aims at comparing short-term single-object visual trackers that do not apply prelearned models of object appearance. Results of 70 trackers are presented, with a large number of trackers being published at major computer vision conferences and journals in the recent years. The number of tested state-of-the-art trackers makes the VOT 2016 the largest and most challenging benchmark on short-term tracking to date. For each participating tracker, a short\u2026\u00a0", "Publication Year": "2018", "Authors": ["M. Kristan", "A. Leonardis", "Zhizhen Chi"], "Related Topics": "Computer Science", "Citation Count": "74", "Reference Count": "109", "References": ["047ea298464b041a90c4ab4e716356c019d613ab", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "6767812e114c426d45ea83894b156f7906e525cd", "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac", "9926020dda21874dc7a5ef1511bae6c4cef5ecb9", "91f2b2aeb7e65d0b673ed7e782488b3365027979", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "2bcf2bd59219d89f335cbc8d1dd4f431076b4c4c", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "5bae9822d703c585a61575dced83fa2f4dea1c6d"]}, {"ID": "6767812e114c426d45ea83894b156f7906e525cd", "Title": " The Thermal Infrared Visual Object Tracking VOT-TIR2015 Challenge Results", "Abstract": "", "Publication Year": "2015", "Authors": ["M. Felsberg", "M. Kristan", "Zhenyu He"], "Related Topics": "Computer Science", "Citation Count": "133", "Reference Count": "72", "References": ["966aad492f75b17f698e981e008b73b51816c6aa", "dd45fe910a0200d43aaa77362f658542f6e175ff", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "4dff84213493bb177dc6bff266a9893538a1f879", "7c78f89fb80449c862ed28d6253d791675319f9b", "f15d5c0a9d2f3678b4c16330da29b3b4511fdef5", "f3c842c88b14cfcc631e5c2cab5f376b9efa09e3", "16be98fa5924131816bc991a2c7ed91b8c69eaaa", "0cae491292feccbc9ad1d864cf8b7144923ce6de", "f5dbe4550d24d5374d9e10fce44a35b105c7ee07"]}, {"ID": "19d6b9725a59f4b624205829d5f03ac893ca1367", "Title": " Long-Term Visual Object Tracking Benchmark", "Abstract": "We propose a new long video dataset (called Track Long and Prosper - TLP) and benchmark for single object tracking. The dataset consists of 50 HD videos from real world scenarios, encompassing a duration of over 400 minutes (676K frames), making it more than 20 folds larger in average duration per sequence and more than 8 folds larger in terms of total covered duration, as compared to existing generic datasets for visual tracking. The proposed dataset paves a way to suitably assess long term\u2026\u00a0", "Publication Year": "2017", "Authors": ["A. Moudgil", "Vineet Gandhi"], "Related Topics": "Computer Science", "Citation Count": "74", "Reference Count": "47", "References": ["ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7", "703505a00579c0aa67712836acc41d94fa6d6edc", "047ea298464b041a90c4ab4e716356c019d613ab", "3275944117b43cc44beebe7c82bffc13ec8cb0fa", "1c721511e4c0e21bd264ca71c0d909528511b7ad", "754504cf01ef3846259783e748b1d3ea52fa2c81", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "1009859c2c69d6b55e03952f863ac81a4dd85d32", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "5f0850ec47a17f22ba2611a5cb67a30cb02cf306"]}, {"ID": "23f8927f996d56f3b5076d8993a70bcfc70182a1", "Title": " Performance Evaluation Methodology for Long-Term Visual Object Tracking", "Abstract": "A long-term visual object tracking performance evaluation methodology and a benchmark are proposed. Performance measures are designed by following a long-term tracking definition to maximize the analysis probing strength. The new measures outperform existing ones in interpretation potential and in better distinguishing between different tracking behaviors. We show that these measures generalize the short-term performance measures, thus linking the two tracking problems. Furthermore, the new\u2026\u00a0", "Publication Year": "2019", "Authors": ["A. Luke\u017ei\u010d", "L. \u010c. Zajc", "M. Kristan"], "Related Topics": "Computer Science", "Citation Count": "6", "Reference Count": "45", "References": ["3275944117b43cc44beebe7c82bffc13ec8cb0fa", "19d6b9725a59f4b624205829d5f03ac893ca1367", "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7", "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b", "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac", "966aad492f75b17f698e981e008b73b51816c6aa", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "754504cf01ef3846259783e748b1d3ea52fa2c81", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "1009859c2c69d6b55e03952f863ac81a4dd85d32"]}, {"ID": "9926020dda21874dc7a5ef1511bae6c4cef5ecb9", "Title": " Long-Term Tracking through Failure Cases", "Abstract": "Long term tracking of an object, given only a single instance in an initial frame, remains an open problem. We propose a visual tracking algorithm, robust to many of the difficulties which often occur in real-world scenes. Correspondences of edge-based features are used, to overcome the reliance on the texture of the tracked object and improve invariance to lighting. Furthermore we address long-term stability, enabling the tracker to recover from drift and to provide redetection following\u2026\u00a0", "Publication Year": "2013", "Authors": ["K. Lebeda", "Simon Hadfield", "R. Bowden"], "Related Topics": "Computer Science", "Citation Count": "54", "Reference Count": "17", "References": ["c63a34ac6a4e049118070e707ca7679fbb132d33", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "00058304b7c51b9dcf837d4125cab0a4b0588aef", "5b9ace65f7368f6dc6907c8f6f7c3b0c248d9bc4", "e684b61e3bc1a9b34dc52a3c42aaca19e48bcbca", "4411f262853bf7f1eb8e2efe03eb0402f5e9ad2c", "fcba52c59e8537e48e747207837cefd04786bd3d", "948a74b25508ab674be06f6a94ca8bc07a082361", "dfa5a0cce66f9e840dd98ac8094434efcc4a9de5", "779220bb5190b976e025dc649b2e9a0e4b3597b9"]}, {"ID": "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac", "Title": " A Novel Performance Evaluation Methodology for Single-Target Trackers", "Abstract": "This paper addresses the problem of single-target tracker performance evaluation. We consider the performance measures, the dataset and the evaluation system to be the most important components of tracker evaluation and propose requirements for each of them. The requirements are the basis of a new evaluation methodology that aims at a simple and easily interpretable tracker comparison. The ranking-based methodology addresses tracker equivalence in terms of statistical significance and practical\u2026\u00a0", "Publication Year": "2015", "Authors": ["M. Kristan", "Jiri Matas", "Luka Cehovin"], "Related Topics": "Computer Science", "Citation Count": "548", "Reference Count": "88", "References": ["edf6607f0c819390a13e60b722cc40f97359c9c4", "2258e01865367018ed6f4262c880df85b94959f8", "a52a6cf39054e6f406f67b57cc895e9df1163fc8", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "8f1a4c9be59b43175c86954829690084ac1e8a1a", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "047ea298464b041a90c4ab4e716356c019d613ab", "681ee0059ed573265847785d110237861458304e", "0cae491292feccbc9ad1d864cf8b7144923ce6de", "5bae9822d703c585a61575dced83fa2f4dea1c6d"]}, {"ID": "3275944117b43cc44beebe7c82bffc13ec8cb0fa", "Title": " Now you see me: evaluating performance in long-term visual tracking", "Abstract": "We propose a new long-term tracking performance evaluation methodology and present a new challenging dataset of carefully selected sequences with many target disappearances. We perform an extensive evaluation of six long-term and nine short-term state-of-the-art trackers, using new performance measures, suitable for evaluating long-term tracking - tracking precision, recall and F-score. The evaluation shows that a good model update strategy and the capability of image-wide re-detection are\u2026\u00a0", "Publication Year": "2018", "Authors": ["A. Luke\u017ei\u010d", "L. \u010c. Zajc", "M. Kristan"], "Related Topics": "Computer Science", "Citation Count": "54", "Reference Count": "34", "References": ["19d6b9725a59f4b624205829d5f03ac893ca1367", "754504cf01ef3846259783e748b1d3ea52fa2c81", "966aad492f75b17f698e981e008b73b51816c6aa", "bfba194dfd9c7c27683082aa8331adc4c5963a0d", "1009859c2c69d6b55e03952f863ac81a4dd85d32", "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "d3d36c3caa255053877a7e3250d47d906eec81d2", "c63a34ac6a4e049118070e707ca7679fbb132d33", "681ee0059ed573265847785d110237861458304e"]}, {"ID": "4dff84213493bb177dc6bff266a9893538a1f879", "Title": " The VOT2013 challenge: overview and additional results", "Abstract": "Visual tracking has attracted a significant attention in the last few decades. The recent surge in the number of publications on tracking-related problems have made it almost impossible to follow the developments in the field. One of the reasons is that there is a lack of commonly accepted annotated data-sets and standardized evaluation protocols that would allow objective comparison of different tracking methods. To address this issue, the Visual Object Tracking (VOT) challenge and workshop\u2026\u00a0", "Publication Year": "2014", "Authors": ["M. Kristan", "R. Pflugfelder", "Tom\u00e1s Voj\u00edr"], "Related Topics": "Computer Science", "Citation Count": "22", "Reference Count": "47", "References": ["4b1a47709d0546e5bc614bf9a521c550e6881d04", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "882c5e862f2256e10bb7dd74d5bbc984b01489fe", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "6169efdeca33714833b120152ae591b8a5f159fa", "2822a883d149956934a20614d6934c6ddaac6857", "bfba194dfd9c7c27683082aa8331adc4c5963a0d", "82635fb63640ae95f90ee9bdc07832eb461ca881", "edf6607f0c819390a13e60b722cc40f97359c9c4"]}, {"ID": "f3c842c88b14cfcc631e5c2cab5f376b9efa09e3", "Title": " Online Object Tracking with Proposal Selection", "Abstract": "Tracking-by-detection approaches are some of the most successful object trackers in recent years. Their success is largely determined by the detector model they learn initially and then update over time. However, under challenging conditions where an object can undergo transformations, e.g., severe rotation, these methods are found to be lacking. In this paper, we address this problem by formulating it as a proposal selection task and making two contributions. The first one is introducing novel\u2026\u00a0", "Publication Year": "2015", "Authors": ["Yang Hua", "Alahari Karteek", "C. Schmid"], "Related Topics": "Computer Science", "Citation Count": "104", "Reference Count": "52", "References": ["201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "d806cde30daef5ca1255c6a36c34c2931fd63604", "c63a34ac6a4e049118070e707ca7679fbb132d33", "61394599ed0aabe04b724c7ca3a778825c7e776f", "b6d31905b671e6d442311c0e275772652df3abb6", "fe2aaad872a2cf08c09dd52ca972f323666306db", "bfba194dfd9c7c27683082aa8331adc4c5963a0d", "1183db5f409e8498d1a0f542703f908275a6dc34", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "894767a3911ce9295844579380b4a727f7a2a0bf"]}, {"ID": "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "Title": " Object Tracking Benchmark", "Abstract": "", "Publication Year": "2015", "Authors": ["Yi Wu", "Jongwoo Lim", "Ming-Hsuan Yang"], "Related Topics": "Computer Science", "Citation Count": "2", "Reference Count": "106", "References": ["bfba194dfd9c7c27683082aa8331adc4c5963a0d", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "16e58cff042f6d556779431c5dc9dafcf092cbf9", "bf5e48bcaddc8d8bfb2c5b138efdb90e94f8258f", "739d084e486702dbdad01d668f77b431228bae9d", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "e13fc55a4dfbf933665e4555dafba558a17f9fa7", "eaf10795a2a34ba6638fd79815d4b81e20eb5955", "20402c2004cfb2e9caa433d9ca2fb61ff30b302e"]}, {"ID": "eda3368a5198ca55768b07b6f5667aea28baf2cd", "Title": " Visual Tracking: An Experimental Survey", "Abstract": "There is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. Object tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. A good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. However, the performance of proposed\u2026\u00a0", "Publication Year": "2014", "Authors": ["A. Smeulders", "D. M. Chu", "M. Shah"], "Related Topics": "Computer Science", "Citation Count": "1", "Reference Count": "120", "References": ["2258e01865367018ed6f4262c880df85b94959f8", "b762ecb0624005831f2f3d8eb626d53e8eca4b6c", "a52a6cf39054e6f406f67b57cc895e9df1163fc8", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "45e098084c676eee87a71806b7eb7ec03f0410c9", "68cc57640bfd04f697048534f82d16bf10a002ec", "8f1a4c9be59b43175c86954829690084ac1e8a1a", "da199480427da6b4c3800b11a91ef7f9bbbc90ee", "21d467174bdcf882eefcae2f10c23a1af5b3e73d", "070375a20acf9252f903164586c75110472cd84f"]}, {"ID": "17f16b89edaed5d16867287ed8a85e917304b4ba", "Title": " The Visual Object Tracking VOT2014 challenge results", "Abstract": ". The Visual Object Tracking challenge 2014, VOT2014, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 38 trackers are\u00a0", "Publication Year": "2014", "Authors": ["M. Kristan", "R. Pflugfelder", "Zhiheng Niu"], "Related Topics": "Computer Science", "Citation Count": "267", "Reference Count": "70", "References": ["4b1a47709d0546e5bc614bf9a521c550e6881d04", "1c42b5543c315556c8a961b1a4ee8bc027f70b22", "4dff84213493bb177dc6bff266a9893538a1f879", "9926020dda21874dc7a5ef1511bae6c4cef5ecb9", "2822a883d149956934a20614d6934c6ddaac6857", "bfba194dfd9c7c27683082aa8331adc4c5963a0d", "e684b61e3bc1a9b34dc52a3c42aaca19e48bcbca", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "f5dbe4550d24d5374d9e10fce44a35b105c7ee07"]}, {"ID": "91f2b2aeb7e65d0b673ed7e782488b3365027979", "Title": " NUS-PRO: A New Visual Tracking Challenge", "Abstract": "", "Publication Year": "2016", "Authors": ["Annan Li", "Min Lin", "Shuicheng Yan"], "Related Topics": "Computer Science", "Citation Count": "176", "Reference Count": "43", "References": ["bfba194dfd9c7c27683082aa8331adc4c5963a0d", "b762ecb0624005831f2f3d8eb626d53e8eca4b6c", "16e58cff042f6d556779431c5dc9dafcf092cbf9", "d908f10ca52c19cd98edeef4323fb5619cfcdf9a", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "9d57723b4908397654fb1846d37db403d8b2b56a", "b817d78457a2a28e62d343f534fca10b0d020ed8", "dd4bdfc5d3944e3e9573b887635487d4c5f5330f"]}, {"ID": "7b75da6f5edac80575d9dcf63db164ce24933907", "Title": " Using Discriminative Motion Context for Online Visual Object Tracking", "Abstract": "In this paper, we propose an algorithm for online, real-time tracking of arbitrary objects in videos from unconstrained environments. The method is based on a particle filter framework using different visual features and motion prediction models. We effectively integrate a discriminative online learning classifier into the model and propose a new method to collect negative training examples for updating the classifier at each video frame. Instead of taking negative examples only from the\u2026\u00a0", "Publication Year": "2016", "Authors": ["S. Duffner", "Christophe Garcia"], "Related Topics": "Computer Science", "Citation Count": "19", "Reference Count": "60", "References": ["f9dbc7f2ef8ed42ff3fdc08d12116761fbbb4865", "421bf4eeba623f722bf98340d71e3d229881e92d", "27b6ed73dce051539494100d2dfdaca27d671556", "c559e4099a6351837753b0a413f9bafed90f5dcd", "d11e63d81ca01288ff55a8ae2a9eddba7c9c1f6c", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "3d98b9822e6efc2eec421e0bfd7546fd4ba30407", "1183db5f409e8498d1a0f542703f908275a6dc34", "452e7a99b67efbdca55008d40e859c8156bfab9f", "d806cde30daef5ca1255c6a36c34c2931fd63604"]}, {"ID": "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d", "Title": " The Visual Object Tracking VOT2015 Challenge Results", "Abstract": "The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 pre-decessor are: (i) a new VOT2015 dataset twice\u2026\u00a0", "Publication Year": "2018", "Authors": ["M. Kristan", "Jiri Matas", "Zhibin Hong"], "Related Topics": "Computer Science", "Citation Count": "357", "Reference Count": "84", "References": ["4b1a47709d0546e5bc614bf9a521c550e6881d04", "6767812e114c426d45ea83894b156f7906e525cd", "4dff84213493bb177dc6bff266a9893538a1f879", "f3c842c88b14cfcc631e5c2cab5f376b9efa09e3", "91f2b2aeb7e65d0b673ed7e782488b3365027979", "9926020dda21874dc7a5ef1511bae6c4cef5ecb9", "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "7b75da6f5edac80575d9dcf63db164ce24933907"]}, {"ID": "5bae9822d703c585a61575dced83fa2f4dea1c6d", "Title": " MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking", "Abstract": "In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization\u2026\u00a0", "Publication Year": "2015", "Authors": ["L. Leal-Taix\u00e9", "Anton Milan", "K. Schindler"], "Related Topics": "Computer Science", "Citation Count": "614", "Reference Count": "57", "References": ["3e083dc8aeb7983a5cdff146985363d38caf0886", "2258e01865367018ed6f4262c880df85b94959f8", "de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42", "aa574e55ea3401ec9bc500eed990e4f402730d26", "616fda61990097f0401b33dbf01541bd83a939a0", "0302bb2d5476540cfb21467473f5eca843caf90b", "5b1e33f60514a307054de5642a13051c1e1438b6", "9b49f70bcaf6e473930681b9a0562f130ae01533", "b238f3f2a487271973c573634611229c432cf467", "f042e85c26cd3638fcdc6599aa546d85045a7c5d"]}, {"ID": "6b175816b1f81127f5e2a2fe998df99d62290a1c", "Title": " Robust visual tracking using template anchors", "Abstract": "Deformable part models exhibit excellent performance in tracking non-rigidly deforming targets, but are usually outperformed by holistic models when the target does not deform or in the presence of uncertain visual data. The reason is that part-based models require estimation of a larger number of parameters compared to holistic models and since the updating process is self-supervised, the errors in parameter estimation are amplified with time, leading to a faster accuracy reduction than in\u2026\u00a0", "Publication Year": "2016", "Authors": ["Luka Cehovin", "A. Leonardis", "M. Kristan"], "Related Topics": "Computer Science", "Citation Count": "35", "Reference Count": "36", "References": ["505f48d8236eb25f871da272c2ac2fe4b41ea289", "6c139dc7f1048cdd9240219541ef9d40df75ad31", "c559e4099a6351837753b0a413f9bafed90f5dcd", "8b421c7c42ae7eaa2c5bbf029354192c96b3e23d", "54142ebf7c0c1ae3983feb10cfe135517f720c59", "63b293b346b5f1c04b6716dfb00b8b343e2a1972", "e684b61e3bc1a9b34dc52a3c42aaca19e48bcbca", "2cf2cf481ed799ee3cf4851593234c9f00100f3e", "9d57723b4908397654fb1846d37db403d8b2b56a", "9926020dda21874dc7a5ef1511bae6c4cef5ecb9"]}, {"ID": "f15d5c0a9d2f3678b4c16330da29b3b4511fdef5", "Title": " Single Object Long-term Tracker for Smart Control of a PTZ camera", "Abstract": "In this paper, we present a single-object long-term tracker that supports high appearance changes in the tracked target, occlusions, and is also capable of recovering a target lost during the tracking process. The initial motivation was real time automatic speaker tracking by a static camera in order to control a PTZ camera capturing a lecture. The algorithm consists of a novel combination of state-of-the-art techniques. Subjective evaluation, over existing and newly recorded sequences, shows\u2026\u00a0", "Publication Year": "2014", "Authors": ["Antonio Gonz\u00e1lez", "R. Nieto", "J. Sanchez"], "Related Topics": "Computer Science", "Citation Count": "20", "Reference Count": "35", "References": ["aefb61185c181edf073dd54045030c406d9205b0", "aff9ed00a2196b4aa7a7968ced25207eb055695c", "2cfa006b33084abe8160b001f9a24944cda25d05", "c63a34ac6a4e049118070e707ca7679fbb132d33", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "e11dab9ddf9ec9b2a0fa35e4b91656ee2ad63aa0", "5f3bda82a3f43cae7deae1dabc7d10ff3dc7a1ae", "bb9535eb9e64cf2c0ec59f550dbfdbde02da76b9", "5cd62c4ace35e93dadb4d69fe76914edd1d331bb", "5254cfffad6e2b4f26dd8d2f32fbd62a6d4354ee"]}, {"ID": "16be98fa5924131816bc991a2c7ed91b8c69eaaa", "Title": " Texture-Independent Long-Term Tracking Using Virtual Corners", "Abstract": "Long-term tracking of an object, given only a single instance in an initial frame, remains an open problem. We propose a visual tracking algorithm, robust to many of the difficulties that often occur in real-world scenes. Correspondences of edge-based features are used, to overcome the reliance on the texture of the tracked object and improve invariance to lighting. Furthermore, we address long-term stability, enabling the tracker to recover from drift and to provide redetection following\u2026\u00a0", "Publication Year": "2016", "Authors": ["K. Lebeda", "Simon Hadfield", "R. Bowden"], "Related Topics": "Computer Science", "Citation Count": "26", "Reference Count": "34", "References": ["9926020dda21874dc7a5ef1511bae6c4cef5ecb9", "38c32f42794bc1162084c4d16e808114f21b796b", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "36b4ed1c2cdcd5acff711eb7d1fe712d4a7d2854", "00058304b7c51b9dcf837d4125cab0a4b0588aef", "e684b61e3bc1a9b34dc52a3c42aaca19e48bcbca", "4411f262853bf7f1eb8e2efe03eb0402f5e9ad2c", "948a74b25508ab674be06f6a94ca8bc07a082361", "c63a34ac6a4e049118070e707ca7679fbb132d33"]}, {"ID": "2822a883d149956934a20614d6934c6ddaac6857", "Title": " A survey of appearance models in visual object tracking", "Abstract": "", "Publication Year": "2013", "Authors": ["Xi Li", "Weiming Hu", "A. Hengel"], "Related Topics": "Computer Science", "Citation Count": "742", "Reference Count": "226", "References": ["505f48d8236eb25f871da272c2ac2fe4b41ea289", "88e91ef20f508b07862ac01253c2514c48ab9315", "29e1e20323f7cb6c15c6acf5cc6573a2f84e6478", "b762ecb0624005831f2f3d8eb626d53e8eca4b6c", "37e8bdbbc38f44a2d8ea72318d6cc72549c0e0eb", "4411f262853bf7f1eb8e2efe03eb0402f5e9ad2c", "da199480427da6b4c3800b11a91ef7f9bbbc90ee", "257cfe2995243b2a5f91a7a423bf2853e1c05420", "0930f78bfa3a2f8e9cc1d9aaf2c65c80a8fbe661", "e13fc55a4dfbf933665e4555dafba558a17f9fa7"]}, {"ID": "e3c433ab9608d7329f944552ba1721e277a42d74", "Title": " Transferring Rich Feature Hierarchies for Robust Visual Tracking", "Abstract": "", "Publication Year": "2015", "Authors": ["Naiyan Wang", "Siyi Li", "D. Yeung"], "Related Topics": "Computer Science", "Citation Count": "318", "Reference Count": "41", "References": ["1b3a107739e7f7e05c50999a3d79b8225746f662", "b2180fc4f5cb46b5b5394487842399c501381d67", "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "e589e0333900cefc5be5a5b9a7daff1a9be341a9", "e6824bc799efcf5cb854591474c92cdbb5716e32", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "739d084e486702dbdad01d668f77b431228bae9d", "d8398d4fa0c4722bd6f0eb0d374178c5a35cd3f3", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "9ea03a9cb11bdc68ac2f56e290c8486868511476"]}, {"ID": "882c5e862f2256e10bb7dd74d5bbc984b01489fe", "Title": " Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms", "Abstract": "Evaluating visual tracking algorithms, or trackers for short, is of great importance in computer vision. However, it is hard to fairly compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty\u2026\u00a0", "Publication Year": "2013", "Authors": ["Yu Pang", "Haibin Ling"], "Related Topics": "Computer Science", "Citation Count": "84", "Reference Count": "81", "References": ["bf5e48bcaddc8d8bfb2c5b138efdb90e94f8258f", "70c3c9b9a40ca55264e454586dca2a6cf416f6e0", "eaf10795a2a34ba6638fd79815d4b81e20eb5955", "e13fc55a4dfbf933665e4555dafba558a17f9fa7", "a1edc13e5c2517b57d432f4a6c6b693152e8c4bb", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "f342285b29a207f6918f49b12fd49aa7d9eb0d38", "7023060f0672f4686074f8739a73db683d05f676", "20402c2004cfb2e9caa433d9ca2fb61ff30b302e", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9"]}, {"ID": "6a699d87bb55477cbe7bfb45b7991b889fd976b6", "Title": " Comparison of infrared and visible imagery for object tracking: Toward trackers with superior IR performance", "Abstract": "The subject of this paper is the visual object tracking in infrared (IR) videos. Our contribution is twofold. First, the performance behaviour of the state-of-the-art trackers is investigated via a comparative study using IR-visible band video conjugates, i.e., video pairs captured observing the same scene simultaneously, to identify the IR specific challenges. Second, we propose a novel ensemble based tracking method that is tuned to IR data. The proposed algorithm sequentially constructs and\u2026\u00a0", "Publication Year": "2015", "Authors": ["Erhan Gundogdu", "Huseyin Ozkan", "S. K. Pakin"], "Related Topics": "Computer Science", "Citation Count": "37", "Reference Count": "25", "References": ["70c3c9b9a40ca55264e454586dca2a6cf416f6e0", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "bfba194dfd9c7c27683082aa8331adc4c5963a0d", "c63a34ac6a4e049118070e707ca7679fbb132d33", "421bf4eeba623f722bf98340d71e3d229881e92d", "460a435a492107e9c8ec23ae8e79b0420de619e9", "5616a93e3b8204d340a4d61269d4c4bd8425f0c3", "b7cd6c2234ea8f5b7d75acc454e35158ffbd42d3", "61394599ed0aabe04b724c7ca3a778825c7e776f", "29e1e20323f7cb6c15c6acf5cc6573a2f84e6478"]}, {"ID": "2c76de57b8b1c9e63b1883cbdea9ec8e68ddf493", "Title": " A Thermal Infrared Video Benchmark for Visual Analysis", "Abstract": "", "Publication Year": "2014", "Authors": ["Zheng Wu", "Nathan W. Fuller", "Margrit Betke"], "Related Topics": "Computer Science", "Citation Count": "128", "Reference Count": "21", "References": ["201d116761d9d300193df370107f26d7d475023b", "dee74a5b8e671909242e05671067eb7add599940", "8c79b44d99b370ac538723d3d22d1f8fa8b396ce", "28d47de5bcfff9ba609337f56a287a4310c2dd44", "2258e01865367018ed6f4262c880df85b94959f8", "f4c7ff4b8613f700aa9f89a2c0653b6ffcf658be", "a34d25e4c53c8261a474af722a631f619cf5a7c3", "d867d7c8cfefe0f5a297a3c613ae6d79c851f4b9", "bfba194dfd9c7c27683082aa8331adc4c5963a0d", "d057b2ef4962e48ed0b8b09381370173b2cf7f0e"]}, {"ID": "201d116761d9d300193df370107f26d7d475023b", "Title": " People detection and tracking from aerial thermal views", "Abstract": "Detection and tracking of people in visible-light images has been subject to extensive research in the past decades with applications ranging from surveillance to search-and-rescue. Following the growing availability of thermal cameras and the distinctive thermal signature of humans, research effort has been focusing on developing people detection and tracking methodologies applicable to this sensing modality. However, a plethora of challenges arise on the transition from visible-light to\u2026\u00a0", "Publication Year": "2014", "Authors": ["J\u00c3\u00bcrg Portmann", "Simon Lynen", "R. Siegwart"], "Related Topics": "Computer Science", "Citation Count": "159", "Reference Count": "17", "References": ["c26eaf62a20ebac360148276a1f098d92c7b0738", "1b7a5f9b9b18ad42608261f7fc7c4879087e7b8a", "ccbc65d05e753b097a6c6b1ece25624e2ee39d5d", "394b9fed48a08ca7f740b5bc26e9386a2f7a73c0", "5224b79368dba945a9e90506f23a1cfa91f6f404", "34e0ba2daabfa4d3d22913ade8265aff50b5f917", "20a056df333294e2fc34bb170d1a20c64202024c", "7f9844f16b6c011e561ee2afef9157405c1d7e0d", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "cec734d7097ab6b1e60d95228ffd64248eb89d66"]}, {"ID": "bfba194dfd9c7c27683082aa8331adc4c5963a0d", "Title": " Online Object Tracking: A Benchmark", "Abstract": "Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are\u2026\u00a0", "Publication Year": "2013", "Authors": ["Yi Wu", "Jongwoo Lim", "Ming-Hsuan Yang"], "Related Topics": "Computer Science", "Citation Count": "3", "Reference Count": "66", "References": ["2822a883d149956934a20614d6934c6ddaac6857", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "237c54150e151e2c9cbfe427219a2ab5864505c6", "2d705458ebae2cb368a6417fe879b2400bf457c9", "a65c76169bdb8479353806556f61bf94fdec7e10", "d908f10ca52c19cd98edeef4323fb5619cfcdf9a", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "e13fc55a4dfbf933665e4555dafba558a17f9fa7", "257cfe2995243b2a5f91a7a423bf2853e1c05420", "421bf4eeba623f722bf98340d71e3d229881e92d"]}, {"ID": "c26eaf62a20ebac360148276a1f098d92c7b0738", "Title": "Local Feature Based Person Detection and Tracking Beyond the Visible Spectrum", "Abstract": "One challenging field in computer vision is the automatic detection and tracking of objects in image sequences. Promising performance of local features and local feature based object detection approaches in the visible spectrum encourage the application of the same principles to data beyond the visible spectrum. Since these dedicated object detectors neither make assumptions on a static background nor a stationary camera, it is reasonable to use these object detectors as a basis for tracking\u2026\u00a0", "Publication Year": "2011", "Authors": ["Kai Jungling", "Michael Arens"], "Related Topics": "Computer Science", "Citation Count": "13", "Reference Count": "39", "References": ["275260b6118b56ee2a3d6bbdf250d0e424b4223c", "e87b897964abb2a093d503ad1ffc956b8129d420", "ccbc65d05e753b097a6c6b1ece25624e2ee39d5d", "49bdd3fb166e0faf7ad1c917aee32c22ebc0f9db", "461efc87636ff4e48323ffcb9f8fdf79cf736fb0", "b6d31905b671e6d442311c0e275772652df3abb6", "c3833f53c947bf89e2c06fd152ca4c7e5a651d6e", "d38345663f133ea519213362ac684e83e24d3464", "91335d9335e4fd06de48d769d1b79eaded4e431b", "192a2a35dae3f63b0cd1ddcb7d21da23c50e7bc4"]}, {"ID": "d867d7c8cfefe0f5a297a3c613ae6d79c851f4b9", "Title": " An iterative integrated framework for thermal-visible image registration, sensor fusion, and people tracking for video surveillance applications", "Abstract": "", "Publication Year": "2012", "Authors": ["Atousa Torabi", "Guillaume Mass\u00e9", "Guillaume-Alexandre Bilodeau"], "Related Topics": "Computer Science", "Citation Count": "144", "Reference Count": "26", "References": ["3ef7cb18b99007ab8edf92ce13b132e2d033d80c", "78e15870d66399ddd69c0ed453a6ae883843f180", "21616f2e0d4adf6c23cdcd8f8b635fa4f6c1508a", "ae5ce57a03558b4042c4924d2ec5784306a6de56", "70bdddb2e4365e2259cfe7133079a38f2d6afbdf", "cbae29796569bbdfe233212a78c3751947cbf0d2", "59daa7193eaf8d2163885f5c4c9dfc536c03e69d", "af65dc520f7a55349982082ecda4a1b28d4c7513", "d38345663f133ea519213362ac684e83e24d3464", "0cff6baa9493a295c1472b63027ddbae3d86a4da"]}, {"ID": "a8ccbb0981b104cc0f753514cc28c01e5309dc41", "Title": " Enhanced Distribution Field Tracking Using Channel Representations", "Abstract": "Visual tracking of objects under varying lighting conditions and changes of the object appearance, such as articulation and change of aspect, is a challenging problem. Due to its robustness and speed, distribution field tracking is among the state-of-the-art approaches for tracking objects with constant size in grayscale sequences. According to the theory of averaged shifted histograms, distribution fields are an approximation of kernel density estimates. Another, more efficient approximation\u2026\u00a0", "Publication Year": "2013", "Authors": ["M. Felsberg"], "Related Topics": "Computer Science", "Citation Count": "96", "Reference Count": "25", "References": ["bfba194dfd9c7c27683082aa8331adc4c5963a0d", "20402c2004cfb2e9caa433d9ca2fb61ff30b302e", "2cfa006b33084abe8160b001f9a24944cda25d05", "270d3d614f519f4bcc1b4a28f0d3c5bead86a46b", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "9b2b3e7460f86d252fa5e0bff6b83fc235fbd73e", "a9ea6b5ea0335aa92c8cab62f5802b5b3606ea90", "eec029c1b57a5b4fc4dab97a84d7e747d1273426", "e07bb324fe689cd8e6f61f045c895c9c1e992f6d", "389110a28961ebe80d8856cd204f8d8305a260ef"]}, {"ID": "fe2aaad872a2cf08c09dd52ca972f323666306db", "Title": " Robust object tracking via sparsity-based collaborative model", "Abstract": "", "Publication Year": "2012", "Authors": ["Wei Zhong", "Huchuan Lu", "Ming-Hsuan Yang"], "Related Topics": "Computer Science", "Citation Count": "1", "Reference Count": "35", "References": ["b762ecb0624005831f2f3d8eb626d53e8eca4b6c", "625e4ee7dce9e6e4183bfebfd8be532172697142", "c4548079b87946c63e9f9a70d6fcc688fb1bdb32", "f317983dfb75ff280d314f9538bb185dc51c6147", "8a2b6f9f4c68262f2b89ef24da48391f7d57b216", "421bf4eeba623f722bf98340d71e3d229881e92d", "9aa63eba69644e45782c9f0bbbaa1cf14d8ace20", "b5e17a0ed14349d6c4066d2408409751f9595e04", "16e36a4b59e214786737aa4ebc3ba86075b61e49", "ebc2edd037e2b3aedc72e22cda3c6d382d650058"]}, {"ID": "2873de80204743249012f52821419978f4d8b27e", "Title": "Particle Filter Tracking of Camouflaged Targets by Adaptive Fusion of Thermal and Visible Spectra Camera Data", "Abstract": "This paper presents a method for tracking a moving target by fusing bi-modal visual information from a deep infra-red thermal imaging camera and a conventional visible spectrum color camera. The tracking method builds on well-known methods for color-based tracking using particle filtering, but it extends these to handle fusion of color and thermal information when evaluating each particle. The key innovation is a method for continuously relearning local background models for each particle in\u2026\u00a0", "Publication Year": "2014", "Authors": ["M. Talha", "R. Stolkin"], "Related Topics": "Computer Science", "Citation Count": "52", "Reference Count": "20", "References": ["954e7e1d0fb187201276a89fb70f18558f32c91d", "4cdc7848bb23f2d43b50950251938bad51090da8", "e11dab9ddf9ec9b2a0fa35e4b91656ee2ad63aa0", "61ac4095650f35458e627015e42f24555441cc4e", "dd4bdfc5d3944e3e9573b887635487d4c5f5330f", "af65dc520f7a55349982082ecda4a1b28d4c7513", "b990e381353a8a12ca3fc57898506a1ae0af913e", "163dcde9f6fe1b7bfbf58a8bd7c06e63ab650cf9", "570502599a26bab7281ce7cbc07eb36bf7b12a51", "0cff6baa9493a295c1472b63027ddbae3d86a4da"]}, {"ID": "703505a00579c0aa67712836acc41d94fa6d6edc", "Title": " Need for Speed: A Benchmark for Higher Frame Rate Object Tracking", "Abstract": "", "Publication Year": "2017", "Authors": ["Hamed Kiani Galoogahi", "Ashton Fagg", "S. Lucey"], "Related Topics": "Computer Science", "Citation Count": "273", "Reference Count": "41", "References": ["204c65df01e958e0331bfdf658a21c55316e2085", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "311bc4e48838d8e5ef619df3ce0bc598aba788a1", "5f0850ec47a17f22ba2611a5cb67a30cb02cf306", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c", "29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "f5dbe4550d24d5374d9e10fce44a35b105c7ee07", "f46318bf67ab6b30284f125ac8bb6f9a7503595e", "c316d5ec14e5768d7eda3d8916bddc1de142a1c2"]}, {"ID": "754504cf01ef3846259783e748b1d3ea52fa2c81", "Title": " Long-term correlation tracking", "Abstract": "", "Publication Year": "2015", "Authors": ["Chao Ma", "Xiaokang Yang", "Ming-Hsuan Yang"], "Related Topics": "Computer Science", "Citation Count": "868", "Reference Count": "30", "References": ["1c721511e4c0e21bd264ca71c0d909528511b7ad", "9d57723b4908397654fb1846d37db403d8b2b56a", "d806cde30daef5ca1255c6a36c34c2931fd63604", "c63a34ac6a4e049118070e707ca7679fbb132d33", "201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9", "23d17cf651ced54ae538ce994ccb83b7ea2a94d3", "16e36a4b59e214786737aa4ebc3ba86075b61e49", "7069a994c150b0228c4e471ca48ed55d7646bc62", "9ea03a9cb11bdc68ac2f56e290c8486868511476", "bd2b8db9f397260aa4a153bb2caf63097be7c04a"]}, {"ID": "d3d36c3caa255053877a7e3250d47d906eec81d2", "Title": " Tracking for Half an Hour", "Abstract": "Long-term tracking requires extreme stability to the multitude of model updates and robustness to the disappearance and loss of the target as such will inevitably happen. For motivation, we have taken 10 randomly selected OTB-sequences, doubled each by attaching a reversed version and repeated each double sequence 20 times. On most of these repetitive videos, the best current tracker performs worse on each loop. This illustrates the difference between optimization for short-term versus long\u2026\u00a0", "Publication Year": "2017", "Authors": ["R. Tao", "E. Gavves", "A. Smeulders"], "Related Topics": "Computer Science", "Citation Count": "17", "Reference Count": "31", "References": ["1c721511e4c0e21bd264ca71c0d909528511b7ad", "c316d5ec14e5768d7eda3d8916bddc1de142a1c2", "4cbe61862bb95fc99293c24d6e02afcb50a05461", "001d36f857ae634b98e8c629853df324c21f323f", "d20d7d3490fd970992b3631048c75a8c5fe2e4e3", "c559e4099a6351837753b0a413f9bafed90f5dcd", "754504cf01ef3846259783e748b1d3ea52fa2c81", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "70c3c9b9a40ca55264e454586dca2a6cf416f6e0", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea"]}, {"ID": "29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "Title": " Fully-Convolutional Siamese Networks for Object Tracking", "Abstract": "", "Publication Year": "2016", "Authors": ["Luca Bertinetto", "Jack Valmadre", "Philip H. S. Torr"], "Related Topics": "Computer Science", "Citation Count": "3", "Reference Count": "52", "References": ["5f0850ec47a17f22ba2611a5cb67a30cb02cf306", "3dc60732c1c08165c9d4e7b334ce66e511474bb2", "bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0", "c316d5ec14e5768d7eda3d8916bddc1de142a1c2", "5c8a6874011640981e4103d120957802fa28f004", "e3c433ab9608d7329f944552ba1721e277a42d74", "9ad8c207d66553d0fa7a7cb57c5e1be12896d1d9", "311bc4e48838d8e5ef619df3ce0bc598aba788a1", "421bf4eeba623f722bf98340d71e3d229881e92d", "09769e80cdf027db32a1fcb695a1aa0937214763"]}, {"ID": "12fae9a2c1ed867997e1ca70eba271b3c741c42f", "Title": " Fast and Accurate Online Video Object Segmentation via Tracking Parts", "Abstract": "Online video object segmentation is a challenging task as it entails to process the image sequence timely and accurately. To segment a target object through the video, numerous CNN-based methods have been developed by heavily finetuning on the object mask in the first frame, which is time-consuming for online applications. In this paper, we propose a fast and accurate video object segmentation algorithm that can immediately start the segmentation process once receiving the images. We first\u2026\u00a0", "Publication Year": "2018", "Authors": ["Jingchun Cheng", "Yi-Hsuan Tsai", "Ming-Hsuan Yang"], "Related Topics": "Computer Science", "Citation Count": "207", "Reference Count": "47", "References": ["0d4b8f60be18585a1d199c63199f99c43d10b7de", "1190e0210430e8b743af24cdc43efdeef407b669", "19351b059b2fabafd885322d26a39ed469265654", "ccb9ffa26b28dffc4f7d613821d1a9f0d60ea3f4", "cb1c0d6be4c22c1f18b0ba20dddd93890f17add6", "b2bf41bf5e5e44d746c1cac3edd058d7d346980e", "c38dbf0bb5a1615b95dc6d3dbc0733dbcd8cf92e", "a213ec900a4e245f31413dc35c2c2e9ae2f09c88", "203ea8ab1d9c48977be97e6caf3fdbcc84101354", "7a59518a2bf7c24f775e3ce7bf98fceefada3e8f"]}, {"ID": "d58e13f7e5e06440c9470a9101ccbb1bfd91b5a1", "Title": " Fast Online Object Tracking and Segmentation: A Unifying Approach", "Abstract": "", "Publication Year": "2018", "Authors": ["Qiang Wang", "Li Zhang", "Philip H. S. Torr"], "Related Topics": "Computer Science", "Citation Count": "912", "Reference Count": "75", "References": ["e8e7eb0ef502d5a456b2d573eb290791e7657b76", "12fae9a2c1ed867997e1ca70eba271b3c741c42f", "29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "1190e0210430e8b743af24cdc43efdeef407b669", "cb1c0d6be4c22c1f18b0ba20dddd93890f17add6", "3d98b9822e6efc2eec421e0bfd7546fd4ba30407", "4960ab1cef23e5ccd60173725ea280f462164a0e", "320d05db95ab42ade69294abe46cd1aca6aca602", "ccb9ffa26b28dffc4f7d613821d1a9f0d60ea3f4", "48fe90ffcfa90170de68c5ef504a5bc2b70ccfad"]}, {"ID": "c316d5ec14e5768d7eda3d8916bddc1de142a1c2", "Title": " Siamese Instance Search for Tracking", "Abstract": "In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the\u2026\u00a0", "Publication Year": "2016", "Authors": ["R. Tao", "E. Gavves", "A. Smeulders"], "Related Topics": "Computer Science", "Citation Count": "933", "Reference Count": "58", "References": ["b2180fc4f5cb46b5b5394487842399c501381d67", "b762ecb0624005831f2f3d8eb626d53e8eca4b6c", "c559e4099a6351837753b0a413f9bafed90f5dcd", "eda3368a5198ca55768b07b6f5667aea28baf2cd", "1b3a107739e7f7e05c50999a3d79b8225746f662", "fe2aaad872a2cf08c09dd52ca972f323666306db", "61394599ed0aabe04b724c7ca3a778825c7e776f", "505f48d8236eb25f871da272c2ac2fe4b41ea289", "b5e17a0ed14349d6c4066d2408409751f9595e04", "c63a34ac6a4e049118070e707ca7679fbb132d33"]}, {"ID": "f5c5c5a2ae127e3e21c1ea94ccad4c17fd02b914", "Title": " Object Tracking by Reconstruction With View-Specific Discriminative Correlation Filters", "Abstract": "", "Publication Year": "2018", "Authors": ["Ugur Kart", "A. Luke\u017ei\u010d", "Jiri Matas"], "Related Topics": "Computer Science", "Citation Count": "57", "Reference Count": "42", "References": ["d10861d377be150b1e03cb942deb8763095de88f", "3f02406b9b59d6f966c735953930fede1d751d0d", "c06ecdf5b149c322db0381adb6b3fd5ccb31a720", "3b75cf84255fce6bdc1fe998761a115437c84c77", "17f2d1221e7a700e26bf8d5c17ca6fec9275439e", "965b01ffc25e643acd16e91dd74ed0d1879f99ec", "ce8c76bfedc5d86faabf0d49dc42a4924f75876d", "70c3c9b9a40ca55264e454586dca2a6cf416f6e0", "8cc3651488e02d51fa5ae8d3563b346e9e370f5a", "487eb86379e979a72ebfef67db6eb8f048d1d258"]}, {"ID": "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b", "Title": " TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild", "Abstract": "Despite the numerous developments in object tracking, further improvement of current tracking algorithms is limited by small and mostly saturated datasets. As a matter of fact, data-hungry trackers based on deep-learning currently rely on object detection datasets due to the scarcity of dedicated large-scale tracking datasets. In this work, we present TrackingNet, the first large-scale dataset and benchmark for object tracking in the wild. We provide more than 30K videos with more than 14\u2026\u00a0", "Publication Year": "2018", "Authors": ["Matthias M\u00fcller", "Adel Bibi", "Bernard Ghanem"], "Related Topics": "Computer Science", "Citation Count": "440", "Reference Count": "47", "References": ["5bae9822d703c585a61575dced83fa2f4dea1c6d", "703505a00579c0aa67712836acc41d94fa6d6edc", "4b1a47709d0546e5bc614bf9a521c550e6881d04", "29d1b9a6e6ff0a4216d10dd31376467d55e788a3", "966aad492f75b17f698e981e008b73b51816c6aa", "ac0d88ca5f75a4a80da90365c28fa26f1a26d4c4", "b7d540cd0de72e984cdec44afa4a4d039cfd5eea", "5f0850ec47a17f22ba2611a5cb67a30cb02cf306", "7574b7e5a75fdd338c27af5aeb77ab79460c4437", "eda3368a5198ca55768b07b6f5667aea28baf2cd"]}, {"ID": "b3249763ac9ecc4df6ef96721c8c7410e0f0468a", "Title": " Deformable Parts Correlation Filters for Robust Visual Tracking", "Abstract": "Deformable parts models show a great potential in tracking by principally addressing nonrigid object deformations and self occlusions, but according to recent benchmarks, they often lag behind the holistic approaches. The reason is that potentially large number of degrees of freedom have to be estimated for object localization and simplifications of the constellation topology are often assumed to make the inference tractable. We present a new formulation of the constellation model with\u2026\u00a0", "Publication Year": "2016", "Authors": ["A. Luke\u017ei\u010d", "Luka \u010cehovin Zajc", "M. Kristan"], "Related Topics": "Computer Science", "Citation Count": "109", "Reference Count": "57", "References": ["63b293b346b5f1c04b6716dfb00b8b343e2a1972", "c559e4099a6351837753b0a413f9bafed90f5dcd", "236d4de0b1c73217238f370e7d30243c7ee9707a", "a9448de3265ed9ecdc11c79273aee92daa31f79a", "93a163e4527741fc2027baeaa586d09af12f3a6c", "70c3c9b9a40ca55264e454586dca2a6cf416f6e0", "9d57723b4908397654fb1846d37db403d8b2b56a", "f6f6617ce6270df934403c18517ecfb10d51f438", "e684b61e3bc1a9b34dc52a3c42aaca19e48bcbca", "1183db5f409e8498d1a0f542703f908275a6dc34"]}, {"ID": "8b74008565b575f9ab7a0962ca5f6955d64db045", "Title": " VideoMatch: Matching based Video Object Segmentation", "Abstract": "Video object segmentation is challenging yet important in a wide variety of applications for video analysis. Recent works formulate video object segmentation as a prediction task using deep nets to achieve appealing state-of-the-art performance. Due to the formulation as a prediction task, most of these methods require fine-tuning during test time, such that the deep nets memorize the appearance of the objects of interest in the given video. However, fine-tuning is time-consuming and\u2026\u00a0", "Publication Year": "2018", "Authors": ["Yuan-Ting Hu", "Jia-Bin Huang", "A. Schwing"], "Related Topics": "Computer Science", "Citation Count": "201", "Reference Count": "61", "References": ["12fae9a2c1ed867997e1ca70eba271b3c741c42f", "4a70c20ad66e5f3bb12fccd84c63ba619053c811", "1190e0210430e8b743af24cdc43efdeef407b669", "ccb9ffa26b28dffc4f7d613821d1a9f0d60ea3f4", "da0d85c2431a60d3ef2aa017e565e9a7b6b1bb1a", "cf56a043577e41e5e33a4722352108660f80258f", "cb1c0d6be4c22c1f18b0ba20dddd93890f17add6", "d710777495f51144c5b9f0a7372d16e3843e1b25", "493670df447aab3b305411c6c352c91eb7021ecb", "e8e7eb0ef502d5a456b2d573eb290791e7657b76"]}, {"ID": "4a70c20ad66e5f3bb12fccd84c63ba619053c811", "Title": " Efficient Video Object Segmentation via Network Modulation", "Abstract": "", "Publication Year": "2018", "Authors": ["L. Yang", "Yanran Wang", "A. Katsaggelos"], "Related Topics": "Computer Science", "Citation Count": "277", "Reference Count": "39", "References": ["1190e0210430e8b743af24cdc43efdeef407b669", "da0d85c2431a60d3ef2aa017e565e9a7b6b1bb1a", "cb1c0d6be4c22c1f18b0ba20dddd93890f17add6", "ab494d4f2e8f4efb830f15d78cd68ef7f355c76b", "19351b059b2fabafd885322d26a39ed469265654", "9c373b1a7d3a987ceca9d33919725ec4bb290683", "c38dbf0bb5a1615b95dc6d3dbc0733dbcd8cf92e", "72c189a6cfa94e90e4c9275bd50f21df10732a04", "203ea8ab1d9c48977be97e6caf3fdbcc84101354", "05e9e85b5137016c93d042170e82f77bb551a108"]}, {"ID": "3f116f1763a3604275b06c6ccf0dcd65910d13b5", "Title": "Automatic segmentation of mammographic masses using fuzzy shadow and maximum-likelihood analysis", "Abstract": "This study attempted to accurately segment tumors in mammograms. Although this task is considered to be a preprocessing step in a computer analysis program, it plays an important role for further analysis of breast lesions. The region of interest (ROI) was segmented using the pixel aggregation and region growing techniques combined with maximum likelihood analysis. A fast segmentation algorithm has been developed to facilitate the segmentation process. The algorithm repetitively sweeps the ROI\u2026\u00a0", "Publication Year": "2002", "Authors": ["Lisa M. Kinnard", "S. Lo", "M. Chouikha"], "Related Topics": "Computer Science, Physics", "Citation Count": "13", "Reference Count": "13", "References": ["71f98dc5cc9409ceb35f057eb5cbe6ede187a1ba", "816deb10419a85444f125056207700c4f1311851", "e7aa02db15ac1c46249be1231b3fa693792844a6", "0c5abfc65c6d8d74ffd2c6490724197e4322a2b9", "7bda4a289f585f8a31d819b963006f6c6918478c", "c3d44f3cbb23519941b9290c93ef442db9129a10", "95962cddd05c9cff112a32f068fc49f7f79ec4e8", "0c408ad1562a3d253caa8b96cb43382e1992f41a", "51f8d113ad4a3aefadcdd061e901262eedac8151", "4aa6aaeb14e5f881100c97cd5d06306f16ab80d0"]}, {"ID": "7642ea8a5bd70c3eede1c1c9bf8b666bf4c5c0a3", "Title": "Segmentation of mass in mammograms using a novel intelligent algorithm", "Abstract": "In order to improve the performance of mass segmentation on mammograms, an intelligent algorithm is proposed in this paper. It establishes two mass models to characterize the various masses, and the ones in the denser tissue are represented with Model I, while the ones in the fatty tissue are represented with Model II. Then, it uses iterative thresholding to extract the suspicious area, as well as the rough regions of those masses matching Model II, and applies a DWT-based technique to locate\u2026\u00a0", "Publication Year": "2006", "Authors": ["Weidong Xu", "Shun-ren Xia", "Min Xiao"], "Related Topics": "Computer Science, Physics", "Citation Count": "18", "Reference Count": "20", "References": ["ac0ca4295b0622e370c19716071614b6d2d723f9", "ab74c37fd6a7081ce65189b5348222b30bea83a5", "0c5abfc65c6d8d74ffd2c6490724197e4322a2b9", "e01f9f37c0b096990025b9b2861cd05d84ffa665", "6aae39114e7d88ff82f331b6379d5b68e2513bc4", "e2ad29cca43ef504569365ab6484fe26488983ee", "634d09f2716fbd68f9d4298eb5a4a1bceeb07cec", "6d93319d3795a6fac8cc9b4cdaffe7392d5e8f9b", "0c1d091efb7f2f9f33272d94131280220c3e5f72", "9b652efaf193e5a5aae0fbb392f254ab66e0acb5"]}]