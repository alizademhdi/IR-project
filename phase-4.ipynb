{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر بیگی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>تمرین چهارم</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۶ بهمن <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیاده‌سازی Crawler (20 نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "   در این بخش باید یک Crawler برای استخراج اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
    "   اطلاعات مورد نظر برای استخراج، باید حاوی موارد زیر باشند:\n",
    "</font>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of all authors separated with \",\"</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of all topics separated with \",\"</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Number of citations of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Number of references of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first 10 references separated with \",\"</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  فرایند استخراج را از ۵ مقاله‌ی اولیه‌ی هر استاد شروع کنید و ۱۰ مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
    "  فرایند استخراج را تا زمانی که اطلاعات ۱۰۰۰ مقاله را داشته باشید ادامه دهید.\n",
    "  اطلاعات مقالات را در فایل <code dir=\"ltr\">${Professor Name}.json</code> ذخیره کنید.\n",
    "</font>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  در پیاده سازی Crawler به موارد زیر دقت کنید:\n",
    "<ul>\n",
    "<li>در زمان تحویل، کد Crawler شما اجرا خواهد شد و صحت عملکرد آن مورد بررسی قرار می‌گیرد.</li>\n",
    "<li>حق استفاده از سرویس API سایت Semantic Scholar را ندارید.</li>\n",
    "<li>بین هر بار درخواست از سایت، یک فاصله‌ی چند ثانیه‌ای بدهید کم‌تر دچار مشکل شوید (انجام این عمل ضروری است).</li>\n",
    "<li>در صورتی که ‌Crawler شما دچار مشکلی مثل Request Timeout شد، نباید کار خود را متوقف کند و باید روند استخراج را ادامه دهد.</li>\n",
    "<li>برای استخراج می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. همچنین برای تجزیه‌ی اطلاعات استخراج شده، می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.</li>\n",
    "</ul>\n",
    "</font>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  در صورتی که می‌خواهید از Scrapy برای پیاده‌سازی Crawler خود استفاده کنید، می‌توانید کد زیر را کامل کنید:\n",
    "</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from time import sleep\n",
    "\n",
    "class HumanVerificationMiddleware:\n",
    "    def process_request(self, request, response, spider):\n",
    "        if response.xpath('//head/title/text()') == 'Human Verification':\n",
    "            # TODO: solve captcha here and return retry request\n",
    "            pass\n",
    "        return response\n",
    "    \n",
    "class ScholarSpider(scrapy.Spider):\n",
    "    name = ...\n",
    "    start_urls = ...\n",
    "\n",
    "    # TODO: Define item count limit, middlewares, feed exports, etc.\n",
    "    custom_settings = ...\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield {\n",
    "            # TODO: Define item fields\n",
    "        }\n",
    "\n",
    "        # TODO: Parse next pages and yield requests\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  حال برای اجرای این Crawler، کافی است آن را درون فایل <code>scholar.py</code> قرار دهیم و سپس دستور زیر را اجرا کنیم:\n",
    "</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-07 00:17:55 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: scrapybot)\n",
      "2024-01-07 00:17:55 [scrapy.utils.log] INFO: Versions: lxml 5.0.1.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 23.3.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.7, Platform Linux-5.15.0-91-generic-x86_64-with-glibc2.35\n",
      "Usage\n",
      "=====\n",
      "  scrapy runspider [options] <spider_file>\n",
      "runspider: error: File not found: scholar.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! scrapy runspider scholar.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>LSI (20 نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl\">\n",
    "در گام نخست داده‌های استخراج شده را به شکل ماتریس term-document تشکیل دهید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    Term-document matrix\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_matrix = load_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl\">\n",
    "در گام بعد، روش LSI را روی این ماتریس ساخته شده اجرا کنید تا علاوه بر به دست آوردن component های U، S و Vt که از SVD به دست می‌آیند، خود ماتریس LSI هم به دست آید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lsi(term_document_matrix, n_components=2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    term_document_matrix: Term-document matrix\n",
    "    n_components: Number of latent semantic dimensions\n",
    "    Returns:\n",
    "    Reduced-dimensional representation\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_matrix = perform_lsi(term_document_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl\">\n",
    "حال از روی ماتریس LSI به دست آمده، میزان شباهت document های مختلف را به دست آورید در فضای latent space.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_document_similarity(lsi_matrix):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    lsi_matrix: Reduced-dimensional representation\n",
    "    Returns:\n",
    "    Document similarity matrix\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_similarity = calculate_document_similarity(lsi_matrix)\n",
    "print(document_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl\">\n",
    "حال document های خود را در فضای Latent Space ساخته شده، نمایش دهید و document های از یک فیلد را با رنگ یکسان نمایش دهید و بررسی کنید آیا آنها نزدیک به یکدیگر قرار میگیرند؟  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl\">\n",
    "برای هر مقاله، اولین فیلد related topics آن را به عنوان فیلد آن در نظر بگیرید\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_documents(lsi_matrix, labels):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    lsi_matrix: Reduced-dimensional representation\n",
    "    labels: List of document labels\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = None\n",
    "visualize_documents(lsi_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl\">\n",
    "در گام نهایی این بخش، روی داده‌های کاهش ابعاد شده که رسم هم کردید، الگوریتم K-Means Clustering را اجرا کنید و metric های ARI، AMI و Silhouette را برای خروجی این خوشه‌بندی به دست آورید. تعداد خوشه‌ها را به تعداد دسته‌بندی document ها در نظر بگیرید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(lsi_matrix, num_clusters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    lsi_matrix: Reduced-dimensional representation\n",
    "    num_clusters: Number of clusters\n",
    "    Returns:\n",
    "    Cluster labels\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def calculate_clustering_metrics(labels_true, labels_pred):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    labels_true: True cluster labels\n",
    "    labels_pred: Predicted cluster labels\n",
    "    Returns:\n",
    "    Dictionary containing clustering metrics\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = len(set(labels))\n",
    "cluster_labels = perform_kmeans_clustering(lsi_matrix, num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_metrics = calculate_clustering_metrics(labels, cluster_labels)\n",
    "print(clustering_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>شبکه‌های عصبی (45 نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>Advanced Disaster Event Predictor Through The Use of Tweets</h3>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "Twitter has become an important source of real-time information during disasters. However, it can be difficult to distinguish between tweets that are reporting real disasters and those that are using disaster-related language metaphorically. This project aims to build a machine learning model that can accurately predict which tweets are about real disasters.\n",
    "\n",
    "The project will use a dataset of 10,000 hand-classified tweets to train and evaluate the model. The model will be trained to extract features from the tweets that are relevant to disaster prediction, such as the presence of disaster-related keywords and phrases, the sentiment of the tweet, and the user's location.\n",
    "\n",
    "## Goal\n",
    "\n",
    "- Develop an accurate classifier.\n",
    "\n",
    "\n",
    "## Steps\n",
    "\n",
    "- **Data Preparation:** Assess the quality of the data, handle missing values and outliers, and clean the data.\n",
    "- **Exploratory Data Analysis:** Inspect the data to gain insights, explore the features, and extract the most useful and representative features.\n",
    "\n",
    "- **Problem Definition and Evaluation:** Formally define the type of task, metrics, and evaluation methods.\n",
    "- **Model Selection and Training:** Explore both traditional machine learning and deep learning algorithms to perform the task, and train the models.\n",
    "- **Model Evaluation and Selection:** Evaluate the performance of the trained models on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "url = 'https://drive.google.com/uc?id=15R-o6L6gi3OZs8b97k_FTxliHCF5X1k5'\n",
    "\n",
    "output = 'dataset.zip'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -q dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aliz/Desktop/personal/university/MIR/IR-project/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn\n",
    "import transformers\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AdamW, BertModel, BertTokenizer\n",
    "\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data\n",
    "\n",
    "## 1.1. Features\n",
    "\n",
    "The dataset includes the following features for each tweet:\n",
    "\n",
    "- id: A unique identifier for the tweet.\n",
    "- text: The text of the tweet.\n",
    "- location: The location where the tweet was sent from.\n",
    "- keyword: A particular keyword from the tweet (may be blank).\n",
    "- target: Whether the tweet is about a real disaster (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./train.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "df_test = pd.read_csv('./test.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "\n",
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data Quality Assessment\n",
    "\n",
    "### 1.2.1. Missing Values:\n",
    "\n",
    "The training and test sets have identical missing value ratios for the `keyword` and `location` features, with 0.8% and 33% of values missing, respectively. This suggests that the missing values are likely due to random sampling, rather than any systematic bias. Therefore, the missing values in these features are **imputed** with the values `no_keyword` and `no_location`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define missing columns\n",
    "missing_cols = ['keyword', 'location']\n",
    "\n",
    "# Calculate the percentage of null values for test and train sets\n",
    "null_percentages = {}\n",
    "for df, label in [(df_train, 'Training Set'), (df_test, 'Test Set')]:\n",
    "    null_percentages[label] = df[missing_cols].isnull().mean() * 100\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(25, 9))\n",
    "\n",
    "for i, (label, percentages) in enumerate(null_percentages.items()):\n",
    "    sns.barplot(x=percentages.index, y=percentages.values, ax=axes[i])\n",
    "    axes[i].set_ylabel('Percentage of Missing Values', size=25, labelpad=20)\n",
    "    axes[i].tick_params(axis='x', labelsize=25, rotation=45)  # Rotate x-axis labels\n",
    "    axes[i].tick_params(axis='y', labelsize=25)\n",
    "    axes[i].set_title(label, fontsize=23)\n",
    "    axes[i].set_yticks([0, 10, 20, 30, 40, 50])  # Set the same yticks for both plots\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and test set have same ratio of missing values in keyword and location.\n",
    "\n",
    "0.8% of keyword is missing in both training and test set\n",
    "33% of location is missing in both training and test set\n",
    "Since missing value ratios between training and test set are too close, they are most probably taken from the same sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Impute Missing Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - impute nullable cols\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Missing values in those features are filled with no_keyword and no_location respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Exploration\n",
    "\n",
    "To select or design an appropriate algorithm for achieving our goals, given the data we have, we should first understand the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Target Distribution\n",
    "\n",
    "The class distribution for the target variable is 57% for 0 (Not Disaster) and 43% for 1 (Disaster). The classes are almost equally balanced, so stratification by target is not necessary during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(df):\n",
    "    fig, axes = plt.subplots(ncols=1, figsize=(7, 5), dpi=100)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    sns.countplot(x=df['target'], hue=df['target'], ax=axes)\n",
    "\n",
    "    axes.set_ylabel('')\n",
    "    axes.set_xticklabels(['Not Disaster (4342)', 'Disaster (3271)'])\n",
    "    ''\n",
    "    axes.tick_params(axis='x')\n",
    "    axes.tick_params(axis='y')\n",
    "\n",
    "    axes.set_title('Target Count in Training Set', fontsize=13)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_target_distribution(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Feature Reduction: `location`\n",
    "\n",
    "The `location` feature is not automatically generated, but is instead a user input. This makes it a very noisy feature, with too many unique values to be useful as a feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Feature Visualization `keyword`\n",
    "\n",
    "The `keyword` feature, on the other hand, has more signal, as some keywords are only used in specific contexts. Keywords also have very different tweet counts and target means, making them a useful feature by itself or as a word added to the text. Additionally, every single keyword in the training set also exists in the test set, which suggests that the two sets were drawn from the same sample. If this is the case, then it is also possible to use target encoding on the `keyword` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_values(df, column_name, data_label):\n",
    "    unique_values = df[column_name].nunique()\n",
    "    print(f'Number of unique values in {column_name} = {unique_values} ({data_label})')\n",
    "\n",
    "# Define the columns and labels\n",
    "data_labels = ['Training', 'Test']\n",
    "\n",
    "# Loop through columns and data labels to print unique values\n",
    "for df, label in zip([df_train, df_test], data_labels):\n",
    "    print_unique_values(df, 'keyword', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate target mean by keyword\n",
    "df_train['target_mean'] = df_train.groupby('keyword')['target'].transform('mean')\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots(figsize=(8, 72), dpi=100)\n",
    "\n",
    "# Sort values by target mean and plot\n",
    "sns.countplot(y=df_train.sort_values(by='target_mean', ascending=False)['keyword'],\n",
    "              hue=df_train.sort_values(by='target_mean', ascending=False)['target'], ax=ax)\n",
    "\n",
    "# Set labels and legend\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.legend(loc=1)\n",
    "\n",
    "# Set title\n",
    "plt.title('Target Distribution in Keywords')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Drop the temporary 'target_mean' column\n",
    "df_train.drop(columns=['target_mean'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. Linguistic Features of Disaster Tweets\n",
    "\n",
    "Distributions of linguistic features in classes and datasets can be helpful to identify disaster tweets. For example, disaster tweets are often written in a more formal way with longer words compared to non-disaster tweets, because many disaster tweets come from news agencies. Non-disaster tweets, on the other hand, are often more informal and may contain more typos, because they are often written by individual users.\n",
    "\n",
    "- `word_count`: number of words in text.\n",
    "- `unique_word_count`: number of unique words in text.\n",
    "- `stop_word_count`: number of stop words in text.\n",
    "- `url_count`: number of urls in text.\n",
    "- `mean_word_length`: average character count in words.\n",
    "- `char_count`: number of characters in text.\n",
    "- `punctuation_count`: number of punctuations in text.\n",
    "- `hashtag_count`: number of hashtags (#) in text.\n",
    "- `mention_count`: number of mentions (@) in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Implement the following data cleaning functions\n",
    "\n",
    "def count_words(text):\n",
    "    pass\n",
    "\n",
    "def count_unique_words(text):\n",
    "    pass\n",
    "\n",
    "def count_stop_words(text):\n",
    "    pass\n",
    "\n",
    "def count_urls(text):\n",
    "    pass\n",
    "\n",
    "def mean_word_length(text):\n",
    "    pass\n",
    "\n",
    "def count_characters(text):\n",
    "    pass\n",
    "\n",
    "def count_punctuation(text):\n",
    "    pass\n",
    "\n",
    "def count_hashtags(text):\n",
    "    pass\n",
    "\n",
    "def count_mentions(text):\n",
    "    pass\n",
    "\n",
    "# Apply the functions to the dataframes\n",
    "def apply_text_features(df):\n",
    "    df['word_count'] = df['text'].apply(count_words)\n",
    "    df['unique_word_count'] = df['text'].apply(count_unique_words)\n",
    "    df['stop_word_count'] = df['text'].apply(count_stop_words)\n",
    "    df['url_count'] = df['text'].apply(count_urls)\n",
    "    df['mean_word_length'] = df['text'].apply(mean_word_length)\n",
    "    df['char_count'] = df['text'].apply(count_characters)\n",
    "    df['punctuation_count'] = df['text'].apply(count_punctuation)\n",
    "    df['hashtag_count'] = df['text'].apply(count_hashtags)\n",
    "    df['mention_count'] = df['text'].apply(count_mentions)\n",
    "\n",
    "# Apply the functions to the dataframes\n",
    "apply_text_features(df_train)\n",
    "apply_text_features(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of all meta features in the training and test sets are very similar, which confirms that they were drawn from the same sample.\n",
    "\n",
    "While all meta features contain information about the target variable, some, such as `url_count`, `hashtag_count`, and `mention_count`, are not as informative as others. In contrast, `word_count`, `unique_word_count`, `stop_word_count`, `mean_word_length`, `char_count`, and `punctuation_count` have very different distributions for disaster and non-disaster tweets, suggesting that they may be useful features for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metafeature_distribution(df, feature, disaster_tweets):\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(20, 5), dpi=100)\n",
    "\n",
    "    sns.distplot(df.loc[~disaster_tweets][feature], label='Not Disaster', ax=axes[0], color='green')\n",
    "    sns.distplot(df.loc[disaster_tweets][feature], label='Disaster', ax=axes[0], color='red')\n",
    "\n",
    "    sns.distplot(df[feature], label='Training', ax=axes[1])\n",
    "    sns.distplot(df_test[feature], label='Test', ax=axes[1])\n",
    "\n",
    "    for j in range(2):\n",
    "        axes[j].set_xlabel('')\n",
    "        axes[j].tick_params(axis='x', labelsize=12)\n",
    "        axes[j].tick_params(axis='y', labelsize=12)\n",
    "        axes[j].legend()\n",
    "\n",
    "    axes[0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n",
    "    axes[1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n",
    "\n",
    "METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n",
    "                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\n",
    "\n",
    "DISASTER_TWEETS = df_train['target'] == 1\n",
    "\n",
    "for feature in METAFEATURES:\n",
    "    plot_metafeature_distribution(df_train, feature, DISASTER_TWEETS)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5. Content Bigrams & Trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n_gram=1):\n",
    "    tokens = [token for token in text.lower().split(' ') if token and token not in STOPWORDS]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "def count_ngrams(df, n_gram=1):\n",
    "    disaster_ngrams = defaultdict(int)\n",
    "    nondisaster_ngrams = defaultdict(int)\n",
    "\n",
    "    for tweet in df[DISASTER_TWEETS]['text']:\n",
    "        for word in generate_ngrams(tweet, n_gram=n_gram):\n",
    "            disaster_ngrams[word] += 1\n",
    "\n",
    "    for tweet in df[~DISASTER_TWEETS]['text']:\n",
    "        for word in generate_ngrams(tweet, n_gram=n_gram):\n",
    "            nondisaster_ngrams[word] += 1\n",
    "\n",
    "    df_disaster_ngrams = pd.DataFrame(sorted(disaster_ngrams.items(), key=lambda x: x[1], reverse=True))\n",
    "    df_nondisaster_ngrams = pd.DataFrame(sorted(nondisaster_ngrams.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    return df_disaster_ngrams, df_nondisaster_ngrams\n",
    "\n",
    "def plot_top_ngrams(df_disaster, df_nondisaster, n, title, color):\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(18, 45), dpi=100)\n",
    "\n",
    "    for i, df in enumerate([df_disaster, df_nondisaster]):\n",
    "        sns.barplot(y=df[0].values[:n], x=df[1].values[:n], ax=axes[i], color=color)\n",
    "\n",
    "        axes[i].spines['right'].set_visible(False)\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylabel('')\n",
    "        axes[i].tick_params(axis='x', labelsize=13)\n",
    "        axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "        axes[i].set_title(f'Top {n} most common {title}', fontsize=15)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disaster_bigrams, df_nondisaster_bigrams = count_ngrams(df_train, n_gram=2)\n",
    "plot_top_ngrams(df_disaster_bigrams, df_nondisaster_bigrams, N, 'bigrams', 'orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disaster_trigrams, df_nondisaster_trigrams = count_ngrams(df_train, n_gram=3)\n",
    "plot_top_ngrams(df_disaster_trigrams, df_nondisaster_trigrams, N, 'trigrams', 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The absence of bigrams common to both disaster and non-disaster tweets suggests that the context is clearer in bigrams. The most common bigrams in disaster tweets provide more information about the disasters than unigrams, but punctuation must be removed from the words before analysis. The most common bigrams in non-disaster tweets are mostly about Reddit or YouTube and contain a lot of punctuation, which must also be removed from the words before analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Data Cleaning\n",
    "\n",
    "Tweets require significant cleaning, but it is inefficient to clean every tweet individually, as this would be too time-consuming. Therefore, a general cleaning approach must be implemented.\n",
    "\n",
    "The most common type of word that requires cleaning in OOV (out-of-vocabulary) words is words with punctuation at the beginning or end. These words do not have embeddings because of the trailing punctuation. The following punctuation marks are separated from words:\n",
    "```\n",
    "#, @, !, ?, +, &, -, $, =, <, >, |, {, }, ^, ', (, ), [, ], *, %, ..., ', ., :, ;\n",
    "```\n",
    "- Special characters attached to words are removed completely.\n",
    "\n",
    "- Punctuations are removed.\n",
    "\n",
    "- Contractions are expanded.\n",
    "\n",
    "- URLs are removed.\n",
    "\n",
    "Finally, hashtags and usernames contain a lot of information about the context, but they are written without spaces between words, so they do not have embeddings. Informative usernames and hashtags should be expanded, but there are too many of them. I expanded as many as I could, but it takes too much time to run the `clean()` function after adding those replace calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Implement the clean function\n",
    "\n",
    "def clean(tweet):\n",
    "    # TODO\n",
    "    cleaned_tweet = None\n",
    "    return cleaned_tweet\n",
    "\n",
    "\n",
    "df_train['text_cleaned'] = df_train['text'].apply(lambda s : clean(s))\n",
    "df_test['text_cleaned'] = df_test['text'].apply(lambda s : clean(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Construct Model's Input\n",
    "\n",
    "#### Regarding the features we analyzed above, prepare the model's input text data for each row of the dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4\n",
    "\n",
    "df_train['model_input'] = None\n",
    "df_test['model_input'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Selection & Configuration\n",
    "\n",
    "This task is a classification task because we have a discrete target that must be predicted. Since there is a slight class imbalance, <u>accuracy would be biased</u>. Therefore, we choose **Mean harmonic F1-score** as our main metric for evaluating our model's performance.\n",
    "\n",
    "For the <u>loss function</u>, we must use **Cross Entropy Loss** for this problem. With the loss function and metric chosen, the remaining settings are narrowed down to choosing the model's structure and some hyperparameters.\n",
    "\n",
    "Mean F-Score, which can be implemented with Macro Average F1-Score, is a useful metric for imbalanced classification tasks. However, it is not very informative without Accuracy, Precision, and Recall, especially when the classes are almost balanced and it is hard to tell which class is harder to predict.\n",
    "\n",
    "* Accuracy measures the fraction of the total sample that is correctly identified.\n",
    "* Precision measures that out of all the examples predicted as positive, how many are actually positive.\n",
    "* Recall measures that out of all the actual positives, how many examples were correctly classified as positive by the model.\n",
    "* F1-Score is the harmonic mean of Precision and Recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Pre-trained Bert Model: **BERTweet**\n",
    "\n",
    "BERTweet is the first public large-scale language model pre-trained for English Tweets. BERTweet is trained based on the RoBERTa pre-training procedure. The corpus used to pre-train BERTweet consists of 850M English Tweets (16B word tokens ~ 80GB), containing 845M Tweets streamed from 01/2012 to 08/2019 and 5M Tweets related to the COVID-19 pandemic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class config:\n",
    "    BERT_PATH = \"vinai/bertweet-base\"\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    VALID_BATCH_SIZE = 8\n",
    "    EPOCHS = 7\n",
    "    MODEL_PATH = 'model.bin'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.BERT_PATH, do_lower_case=True)\n",
    "bert_model = AutoModel.from_pretrained(config.BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 - Complete the below code\n",
    "\n",
    "class BERTweetClassifier(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BERTweetClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "\n",
    "        # TODO: add a dropout layer with p=0.3\n",
    "        self.bert_drop = None\n",
    "\n",
    "        # TODO: add a linear layer for our classification task\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        # TODO: get the model's output\n",
    "        bert_output = None\n",
    "\n",
    "        # TODO: apply the model's dropout layer to the pooler output\n",
    "        reguralized_output = None\n",
    "\n",
    "        # TODO: call the classifier\n",
    "        output = None\n",
    "        return output\n",
    "\n",
    "model = BERTweetClassifier(bert_model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prepare Dataset:\n",
    "\n",
    "Input text is tokenized using our BERT's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6 - Tokenize the input text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Train & Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7 - Complete the below code\n",
    "\n",
    "def train_model(data_loader, model, optimizer, epoch, device, scheduler):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for data in tqdm(data_loader):\n",
    "        ids = data[\"ids\"]\n",
    "        token_type_ids = data[\"token_type_ids\"]\n",
    "        mask = data[\"mask\"]\n",
    "        targets = data[\"target\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # TODO: get the model's output and calculate the loss\n",
    "        predictions = None\n",
    "        loss = None\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss / len(data_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(data_loader, model, device):\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            ids = data[\"ids\"]\n",
    "            token_type_ids = data[\"token_type_ids\"]\n",
    "            mask = data[\"mask\"]\n",
    "            targets = data[\"target\"]\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # TODO: get the model's output\n",
    "            outputs = None\n",
    "\n",
    "            # TODO: calculate the probabilities of the model's output (between 0 and 1)\n",
    "            probabilities = None\n",
    "\n",
    "            probabilities = probabilities.numpy().tolist()\n",
    "            all_predictions.extend(probabilities)\n",
    "\n",
    "\n",
    "            targets = targets.cpu().detach()\n",
    "            targets = targets.numpy().tolist()\n",
    "            all_targets.extend(targets)\n",
    "\n",
    "    return all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Fine-tune Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8 - Define the model's optimizer and learning-rate scheduler\n",
    "learning_rate = None\n",
    "optimizer = None\n",
    "\n",
    "\n",
    "\n",
    "# TODO: use get_linear_schedule_with_warmup for the transformers lib\n",
    "num_train_steps = None\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9 - Complete the code of the training loop\n",
    "\n",
    "best_f1 = 0\n",
    "val_metrics = []\n",
    "print(f'Total no of epochs = {config.EPOCHS}')\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    with tqdm(range(len(train_dataloader)), leave=False, position=1, desc=' Steps') as p_bar:\n",
    "\n",
    "        # TODO: train the model for 1 epochs\n",
    "\n",
    "\n",
    "        # TODO: evaluate the model on the test data\n",
    "        outputs, targets = None\n",
    "\n",
    "        # TODO: measure Accuracy, Precision, Recall, and F1-score metrics\n",
    "        accuracy, recall, precision, f1 = None, None, None, None\n",
    "\n",
    "\n",
    "        print(f\"\\n epoch:{epoch} | Validation Accuracy: {accuracy} | Recall: {recall} | Precision: {precision} | F1 Score: {f1}\")\n",
    "\n",
    "        val_metrics.append({'accuracy': accuracy, 'recall': recall, 'precision': precision, 'f1': f1})\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            # TODO: save the best model based on the performance on f1, and update the best f1-score\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visualize Model Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10 - Visualize training metrics (A, P, R, F1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluate on Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 11 - you have to reach an f1-score of at least 85% for full marks.\n",
    "\n",
    "# TODO: load the best model\n",
    "best_model = None\n",
    "\n",
    "# TODO: evaluate the model's performance on the test data and print it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رتبه‌بندی نویسندگان (15 نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>  \n",
    "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
    "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_algorithm(papers, n):\n",
    "    \"\"\"\n",
    "        Implementing the HITS algorithm to score authors based on their papers and co-authors.\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        papers: A list of paper dictionaries with the following keys:\n",
    "                \"id\": A unique ID for the paper\n",
    "                \"title\": The title of the paper\n",
    "                \"abstract\": The abstract of the paper\n",
    "                \"date\": The year in which the paper was published\n",
    "                \"authors\": A list of the names of the authors of the paper\n",
    "                \"related_topics\": A list of IDs for related topics (optional)\n",
    "                \"citation_count\": The number of times the paper has been cited (optional)\n",
    "                \"reference_count\": The number of references in the paper (optional)\n",
    "                \"references\": A list of IDs for papers that are cited in the paper (optional)\n",
    "        n: An integer representing the number of top authors to return.\n",
    "\n",
    "        Returns\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        List \n",
    "        list of the top n authors based on their hub scores.\n",
    "    \"\"\"\n",
    "    # Create a graph of authors and papers (all of the authors and papers represented as nodes, and all of the authors who wrote each paper connected to the corresponding paper node by an edge)\n",
    "    G = None\n",
    "\n",
    "    # Run the HITS algorithm\n",
    "    hubs, authorities = None\n",
    "\n",
    "    # Create a list of top n authors based on their hub scores\n",
    "    top_authors = None\n",
    "    return top_authors\n",
    "\n",
    "\n",
    "# call the hit_algorithm function\n",
    "top_authors = hits_algorithm(papers, 2)\n",
    "\n",
    "# print the top authors\n",
    "print(top_authors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
