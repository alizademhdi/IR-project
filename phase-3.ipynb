{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر بیگی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>تمرین سوم</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۴ دی <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">\n",
    "<div dir='rtl'>\n",
    "\n",
    "<b> نام و نام‌‌خانوادگی: </b>\n",
    "<!-- YOUR NAME HERE -->\n",
    "\n",
    "<b> شماره دانشجویی: </b>\n",
    "<!-- YOUR STUDENT ID HERE -->\n",
    "\n",
    "<b> لینک colab: </b>\n",
    "\n",
    "<!-- UPLOAD YOUR NOTEBOOK TO GOOGLE COLAB AND MAKE SURE TO RUN ALL OF ITS CELLS -->\n",
    "</div>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1> \n",
    "مقدمه\n",
    "</h1>\n",
    "<p>\n",
    "در این تمرین قصد داریم به مباحث زیر بپردازیم:\n",
    "<li> embedding </li>\n",
    "<li> کلاسه‌بندی متن </li>\n",
    "<li> کاهش ابعاد </li>\n",
    "<li> خوشه‌بندی متن و هرس‌کردن خوشه‌ها</li>\n",
    "\n",
    "دیتاست این تمرین از دیتاست‌های kaggle انتخاب شده‌است و لینک آن در بخش اول تمرین در اختیار شما قرار داده شده است.\n",
    "\n",
    "کتابخانه‌های مورد نظرتان را هم می‌توانید در اولین سل نوت‌بوک فراخوانی کنید. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import torch\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import edit_distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import fasttext\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mhdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>1.\n",
    "دریافت و آماده‌سازی دیتاست\n",
    "</h1>\n",
    "<p>\n",
    "دیتاست استفاده شده در این تمرین، مجموعه‌ای عناوین، خلاصه، و ژانر چندین کتاب است. \n",
    "این دیتاست در kaggle موجود است. \n",
    "ابتدا این دیتاست را با استفاده از kaggle api دریافت کنید و سپس آن را لود کنید.\n",
    "<br/>\n",
    "لینک دیتاست: https://www.kaggle.com/datasets/athu1105/book-genre-prediction\n",
    "<br>\n",
    "<i>در صورتی که با خطای 443 مواجه شدید، یا از پراکسی استفاده کنید یا از کولب.</i>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/phase-3/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Drowned Wednesday</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Drowned Wednesday is the first Trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>As the book opens, Jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Eyes of the Overworld</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Cugel is easily persuaded by the merchant Fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Magic's Promise</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>The book opens with Herald-Mage Vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Taran Wanderer</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Taran and Gurgi have returned to Caer Dallben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                      title    genre  \\\n",
       "0      0          Drowned Wednesday  fantasy   \n",
       "1      1              The Lost Hero  fantasy   \n",
       "2      2  The Eyes of the Overworld  fantasy   \n",
       "3      3            Magic's Promise  fantasy   \n",
       "4      4             Taran Wanderer  fantasy   \n",
       "\n",
       "                                             summary  \n",
       "0   Drowned Wednesday is the first Trustee among ...  \n",
       "1   As the book opens, Jason awakens on a school ...  \n",
       "2   Cugel is easily persuaded by the merchant Fia...  \n",
       "3   The book opens with Herald-Mage Vanyel return...  \n",
       "4   Taran and Gurgi have returned to Caer Dallben...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<p>\n",
    "حال در این مرحله، به پیش‌پردازش متن می‌پردازیم. این پیش‌پردازش باید روی هر دو ستون title و summary اعمال شود.\n",
    "برای پیش‌پردازش نیازی نیست که هرکدام از اعمال پیش‌پردازش را خودتان مانند تمارین قبل پیاده کنید. برای پیاده‌سازی تابع زیر می‌توانید از کتابخانه‌های معمول برای این کار بهره ببرید.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_domain = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, minimum_length=1, stopword_removal=True, stopwords_domain=stopwords_domain, lower_case=True,\n",
    "                       punctuation_removal=True):\n",
    "    \"\"\"\n",
    "    preprocess text by removing stopwords, punctuations, and converting to lowercase, and also filter based on a min length\n",
    "    for stopwords use nltk.corpus.stopwords.words('english')\n",
    "    for punctuations use string.punctuation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        text to be preprocessed\n",
    "    minimum_length: int\n",
    "        minimum length of the token\n",
    "    stopword_removal: bool\n",
    "        whether to remove stopwords\n",
    "    stopwords_domain: list\n",
    "        list of stopwords to be removed base on domain\n",
    "    lower_case: bool\n",
    "        whether to convert to lowercase\n",
    "    punctuation_removal: bool\n",
    "        whether to remove punctuations\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_tokens = word_tokenize(text)\n",
    "\n",
    "    if stopword_removal:\n",
    "        stopwords_domain = [x.lower() for x in stopwords_domain]\n",
    "        normalized_tokens = [word for word in normalized_tokens if word.lower() not in stopwords_domain]\n",
    "\n",
    "    if punctuation_removal:\n",
    "        normalized_tokens = [word for word in normalized_tokens if word not in string.punctuation]\n",
    "\n",
    "    if lower_case:\n",
    "        normalized_tokens = [word.lower() for word in normalized_tokens if len(word) > minimum_length]\n",
    "\n",
    "    else:\n",
    "        normalized_tokens = [word for word in normalized_tokens if len(word) > minimum_length]\n",
    "\n",
    "    return normalized_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"summary_preprocessed\"] = df[\"summary\"].apply(lambda x: preprocess_text(x))\n",
    "df[\"title_preprocessed\"] = df[\"title\"].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در نهایت بعد از اپلای کردن پیش‌پردازش روی هر دو ستون، دو ستون پیش‌پردازش شده را با هم ادغام کنید و آن را در یک آرایه به نام X قرار دهید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title_preprocessed'] + df['summary_preprocessed']\n",
    "X_text = [' '.join(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"lost hero book opens jason awakens school bus unable remember anything past sitting next piper mclean leo valdez call name say girlfriend best friend respectively three part class field trip grand canyon arrive classmate dylan turns venti storm spirit attacks trio trip leader coach gleeson hedge ensuing fight jason surprises everyone including one coins turns sword uses battle storm spirits coach hedge reveals satyr fight taken captive fleeing spirit battle flying chariot arrives rescue trio one people annabeth upset discovers missing boyfriend percy jackson expected annabeth seeking percy told vision goddess hera look `` guy one shoe '' turns jason shoe destroyed fight jason piper leo told demigods taken back camp half-blood meet greek demigod children like leo revealed son hephaestus piper daughter aphrodite jason son zeus though hera tells champion jason later discovers full brother zeus 's demigod daughter thalia grace hunter artemis shortly arrive three given quest rescue hera captured set soon discover enemies working orders gaea overthrow gods quest encounter thalia hunters looking percy thalia jason reunite first since jason captured age two way aeolus 's castle jason leo piper become separated thalia promises meet wolf house last place thalia seen jason meeting nearly apprehended aeolus gaea 's orders trio manage escape thanks mellie aeolus former assistant end san francisco thanks result dream piper aphrodite landing san francisco trio rush mt.diablo fight giant enceladus kidnapped piper 's father manage kill giant save piper 's father rush wolf house free hera although heroes hunters save hera king giants porphyrion rises fully disappears hole earth jason 's memory starts returning remembers hero roman counterpart camp half-blood somewhere near san francisco son jupiter zeus 's roman aspect realizes hera also known juno switched percy jackson roman camp memory life hopes two camps would ultimately work together fight giants defeat goddess gaea\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>1-1.\n",
    "Embedding\n",
    "</h2>\n",
    "\n",
    "در این قسمت از fasttext کمک می‌گیریم تا به یک embedding اولیه برای هر کتاب برسیم.\n",
    "    با استفاده از داده‌هایی که داریم یک مدل fasttext آموزش دهید که برای هر توکن یک امبدینگ ۱۰۰تایی بدهد.\n",
    "    در مرحله‌ی بعد میانگین وزن دار امبدینگ های fasttext\n",
    "        توکن‌های ورودی (چکیده + عنوان)\n",
    "    را بر اساس tfidif آن‌ها محاسبه کنید و به امبدینگ نهایی متن برسید.\n",
    "    <br>\n",
    "    در واقع به عبارت ساده‌تر بر اساس میانگین وزن‌دار که وزن‌های ما tfidf توکن‌ها می‌باشد به امبدینگ نهایی متن بر اساس fasttext می‌رسیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText:\n",
    "\n",
    "    def __init__(self, preprocessor=None, method='skipgram'):\n",
    "        self.method = method\n",
    "        self.model = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def train(self, texts):\n",
    "        \"\"\"\n",
    "        train the fasttext model and save it into self.model\n",
    "        Parameters\n",
    "        ----------\n",
    "        texts: list of list of str\n",
    "        \"\"\"\n",
    "        file_name = './data/phase-3/data_cleaned.txt'\n",
    "        with open(file_name, 'w') as f:\n",
    "            for item in texts:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "            f.close()\n",
    "\n",
    "        self.model = fasttext.train_unsupervised(file_name, model=self.method)\n",
    "\n",
    "    def get_query_embedding(self, query, tf_idf_vectorizer):\n",
    "        \"\"\"\n",
    "        get the embedding of the query. You can use the tf_idf_vectorizer to get the weights of the words in the query. preprocess the query using self.preprocessor if it is not None\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str\n",
    "        tf_idf_vectorizer: TfidfVectorizer\n",
    "        Returns embedding of the query\n",
    "        \"\"\"\n",
    "        embedded_query = np.zeros(100)\n",
    "        query = self.preprocessor(query)\n",
    "        tf_idf_scores = tf_idf_vectorizer.transform([' '.join(query)]).toarray()\n",
    "        for word in query:\n",
    "            if word in tf_idf_vectorizer.vocabulary_.keys():\n",
    "                word_indice = tf_idf_vectorizer.vocabulary_[word]\n",
    "                embedded_query += tf_idf_scores[0][word_indice] * self.model.get_word_vector(word)\n",
    "\n",
    "        return embedded_query\n",
    "\n",
    "    def save_FastText_model(self, path='./models/FastText_model.bin'):\n",
    "        self.model.save_model(path)\n",
    "\n",
    "    def load_FastText_model(self, path=\"./models/FastText_model.bin\"):\n",
    "        self.model = fasttext.load_model(path)\n",
    "\n",
    "    def prepare(self, dataset, mode, save=False):\n",
    "        if mode == 'train':\n",
    "            self.train(dataset)\n",
    "        if mode == 'load':\n",
    "            self.load_FastText_model()\n",
    "        if save:\n",
    "            self.save_FastText_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  20282\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   65583 lr:  0.000002 avg.loss:  2.271513 ETA:   0h 0m 0s100.1% words/sec/thread:   64968 lr: -0.000030 avg.loss:  2.271576 ETA:   0h 0m 0sWarning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Progress: 100.0% words/sec/thread:   64967 lr:  0.000000 avg.loss:  2.271576 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "FastText_model = FastText(preprocessor=preprocess_text)\n",
    "FastText_model.prepare(X_text, mode='train', save=True)\n",
    "FastText_model.prepare(X, mode='load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.880240261554718, 'societies'),\n",
       " (0.8100507259368896, 'agriculture'),\n",
       " (0.8066205978393555, 'equality'),\n",
       " (0.8012783527374268, 'lifestyle'),\n",
       " (0.7936832308769226, 'sociology'),\n",
       " (0.7921492457389832, 'agricultural'),\n",
       " (0.7878493666648865, 'fertility'),\n",
       " (0.786555290222168, 'variety'),\n",
       " (0.782918393611908, 'equals'),\n",
       " (0.7827007174491882, 'popularity')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"society\"\n",
    "FastText_model.model.get_nearest_neighbors(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این مرحله آرایه X را روی TFIDF فیت می‌کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit_vectorizer(self, data):\n",
    "        \"\"\"\n",
    "        fit the vectorizer on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: list of list of str\n",
    "        \"\"\"\n",
    "        self.vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_model = TF_IDF()\n",
    "TF_IDF_model.fit_vectorizer(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07685816  0.15103993 -0.31325506 -0.31091463 -0.11035508 -0.06025137\n",
      " -0.33890378 -0.20461031  0.19143126 -0.01584841  0.10076773  0.30836532\n",
      "  0.07478167  0.04833817  0.35057064  0.16702574 -0.2122911  -0.23538411\n",
      " -0.17856634 -0.29422837 -0.13177812  0.2476757   0.44564785 -0.65641227\n",
      " -0.02192403 -0.2110163   0.57409605  0.04755806 -0.08134577  0.25422455\n",
      "  0.12808537  0.20377593 -0.74035645 -0.00785394  0.52432304 -0.30830564\n",
      " -0.12534711 -0.33635902 -0.08596513  0.13402439 -0.14908559 -0.17038143\n",
      " -0.40552511 -0.16403412 -0.03879646 -0.03073727  0.3090703   0.02828454\n",
      "  0.05911007  0.08717421 -0.53820047 -0.01005109 -0.55456436  0.06239751\n",
      " -0.20141677 -0.41153832 -0.03252132 -0.20446468 -0.29354015  0.53638878\n",
      "  0.37253731  0.15439685 -0.55014616  0.28978732 -0.40171458  0.21939173\n",
      " -0.26290529 -0.42341182  0.036148   -0.47445785  0.05072556  0.26519767\n",
      "  0.21300658  0.0514566  -0.02343663  0.35361504 -0.31836375 -0.46709597\n",
      " -0.19775754  0.22024759  0.10076064  0.06863415 -0.60588253  0.22531782\n",
      " -0.13005456 -0.06317496 -0.43592575  0.15865298  0.16316791 -0.18423356\n",
      " -0.32316869 -0.28832689  0.05356884 -0.13114314  0.07680592 -0.25242832\n",
      " -0.19321145  0.23069587  0.09420681  0.04231292]\n"
     ]
    }
   ],
   "source": [
    "text = \"society of the snow\"\n",
    "embedded_text = FastText_model.get_query_embedding(text, TF_IDF_model.vectorizer)\n",
    "print(embedded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در نهایت، تمامی entry های درون X را به صورت امبدینگ دربیاورید و آن را در X ذخیره کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4657 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4657/4657 [00:09<00:00, 473.31it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "for text in tqdm(X_text):\n",
    "    embeddings.append(FastText_model.get_query_embedding(text, TF_IDF_model.vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h2>1-2. \n",
    "آماده‌سازی داده تمرین و تست\n",
    "</h2>\n",
    "<p>\n",
    "در این بخش ابتدا می‌خواهیم تا داده‌ای که می‌خواهیم بر اساس آن کلاسه‌بندی کتاب‌ها را انجام دهیم، که همان ژانر کتاب‌ها است را انکود کنیم.\n",
    "<br>\n",
    "سپس با جداسازی داده آموزش و تست، به آموزش مدل‌های کلاسه‌بند می‌پردازیم.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['genre'].unique()\n",
    "label_to_id = {}\n",
    "id_to_label = {}\n",
    "for i, label in enumerate(labels):\n",
    "    label_to_id[label] = i\n",
    "    id_to_label[i] = label\n",
    "\n",
    "labels = df['genre'].apply(lambda x: label_to_id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h1>2. \n",
    "کلاسه‌بندی متن\n",
    "</h1>\n",
    "در این بخش می‌خواهیم تا با استفاده از داده‌هایی که داریم، سه مدل کلاسه‌بندی متن را آموزش دهیم و عملکرد آن‌ها را با هم مقایسه کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>2-1. \n",
    "SVM و Naive Bayes\n",
    "</h2>\n",
    "با استفاده از توابع آماده در sklearn، کلاسه‌بند SVM و Naive Bayes را روی داده آموزش تمرین دهید. سپس، ژانر داده تست را با استفاده از مدل آموزش داده شده پیش‌بینی کنید و آن را در آرایه‌های NB_prediction و SVM_prediction ذخیره کنید.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB = GaussianNB()\n",
    "NB.fit(x_train, y_train)\n",
    "\n",
    "SVM = svm.SVC()\n",
    "SVM.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_prediction = NB.predict(x_test)\n",
    "SVM_prediction = SVM.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>2-2.  \n",
    "Rocchio\n",
    "</h2>\n",
    "می‌خواهیم روش Rocchio را از\n",
    "پایه پیاده‌سازی کنیم.\n",
    "با توجه به شواهد نوشته شده هر تابع را کامل کنید و با آموزش مدل روی داده‌های train\n",
    "لیبل‌های داده‌های test\n",
    "را پیش بینی کنید و آن را در rocchio_prediction ذخیره کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocchioTextClassifier:\n",
    "    def __init__(self, preprocessor=None):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.centroids = None\n",
    "\n",
    "    def calculate_centroids(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates the centroids of each class in the dataset. A centroid is defined as the mean vector of all the feature vectors in a class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            An array contaning the feature vectors of each sentence.\n",
    "\n",
    "        y : np.ndarray\n",
    "            An array containing the class labels for each feature vector in X.\n",
    "\n",
    "        Sets self.centeroids as a dictionary where keys are unique class labels from 'y', and values are the calculated centroids (mean vectors) for each class.\n",
    "        \"\"\"\n",
    "        self.centroids = {}\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def predict_label(self, x):\n",
    "        \"\"\"\n",
    "        Classifies a new instance by finding the class whose centroid is closest to the new instance's vector.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            An array of a new instance to be classified.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The predicted label for the input.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "        return label\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Classifies the array X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            An array of new instances to be classified.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The predicted label for each input.\n",
    "        \"\"\"\n",
    "        return np.array([self.predict_label(x) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your Rocchio implementation on X_train and Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict on X_test using the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>2-3. \n",
    "ارزیابی\n",
    "</h2>\n",
    "برای هر سه روش پیاده‌سازی شده، معیار‌های ارزیابی زیر را بررسی کنید.\n",
    "<br>\n",
    "f1 score, accuracy, precision, recall\n",
    "<br>\n",
    "سپس، نتایج به دست آمده را با هم در چهار نمودار مقایسه کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run evaluation metrics on the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot evaulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Draw confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "کدام مدل از همه دقیق‌تر عمل کرد؟ نتیجه‌گیری و تحلیل خود از نتایج ارزیابی را گزارش دهید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">#TODO: Write your answer in here.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h1>3. \n",
    "کاهش ابعاد و خوشه‌بندی متن\n",
    "</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>3-1. \n",
    "کاهش ابعاد\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h3>3-1-1. \n",
    "PCA\n",
    "</h3>\n",
    "یکی از روش‌های کاهش ابعاد، PCA است. با استفاده از پیاده‌سازی آن در کتابخانه sklearn، ابعاد ویژگی‌های X را کاهش دهید.\n",
    "<br>\n",
    "سپس با استفاده از explained_variance_ratio_ در الگوریتم PCA  نشان دهید که با وجود یک ترشولد 90 درصد تا چه میزان میتوان ابعاد ویژگی ها را کم تر کرد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimension(embedding, n_components):\n",
    "    \"\"\"\n",
    "    Performs dimensional reduction using PCA with n components left behind\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : List\n",
    "        A list of embeddings of documents\n",
    "\n",
    "    n_components: int\n",
    "        Number of components to keep\n",
    "\n",
    "    Returns a list of reduced embeddings\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این قسمت می‌توانید برای شهود بهتر، نمودار رسم کنید و همچنین برای ساده‌تر شدن کار، از pipeline‌های sklearn بهره ببرید. \n",
    "<br>\n",
    "<i> انجام این کار‌ها صرفا توصیه است و اجباری نیست. </i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Findout the most reduced dimension which has 90% cutoff explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h3>3-1-2. \n",
    "TSNE\n",
    "</h3>\n",
    "<br>\n",
    "     برای اینکه بتوانید در مراحل بعدی، نتایج خوشه‌بندی را مشاهده کنید، در این قسمت به پیاده‌سازی تابع کاهش بعد بردارهای جاسازی با استفاده از روش T-SNE می‌پردازید.\n",
    "برای اینکار تابع convert_to_2d_tsne را پیاده‌سازی می‌کنید که لیستی از بردارهای جاسازی را به عنوان ورودی دریافت می‌کند و در خروجی، لیستی از بردارهای جاسازی کاهش بعد داده شده به دو بعد را تولید می‌کند. برای پیاده سازی این تابع می‌توانید از کتابخانه‌های آماده استفاده کنید.\n",
    "<br>\n",
    "توجه کنید که از بردارهای خروجی این قسمت <u>صرفا برای رسم نمودار</u> استفاده می‌کنید و تمامی مراحلی که در ادامه طی می‌کنید (به جز رسم نمودار)، باید با استفاده از بردارهای کاهش بعد داده <u>نشده</u> انجام شوند.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sklearn.manifold import TSNE\n",
    "def convert_to_2d_tsne(emb_vecs):\n",
    "    \"\"\"\n",
    "    Converts each raw embedding vector to 2d vector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_vecs : List\n",
    "        A list of vectors\n",
    "\n",
    "    Returns a list of 2d vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Convert each input vector to 2d vector\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following code\n",
    "# X_2d = convert_to_2d_tsne(np.array(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>3-2. \n",
    "خوشه‌بندی\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h3>3-2-1. \n",
    "K-Means\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این قسمت، شما ابتدا الگوریتم خوشه‌بندی K-means را \n",
    "<u><b>از پایه</b></u>\n",
    " پیاده‌سازی می‌کنید.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "\n",
    "def cluster_kmeans(emb_vecs, n_clusters):\n",
    "    \"\"\"\n",
    "    Clusters input vectors using K-means method\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_vecs : List\n",
    "        A list of vectors\n",
    "\n",
    "    n_clusters: int\n",
    "        Number of clusters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Two lists: 1) A list containing cluster centers 2) A list containing cluster index for each input vector\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement K-means method from scratch\n",
    "    # You can't use sklearn.cluster.KMeans for clustering\n",
    "    # implement kmeans clustering here\n",
    "    emb_matrix = None\n",
    "\n",
    "    # Randomly initialize centroids\n",
    "    initial_centroids = None\n",
    "\n",
    "    for _ in range(None):  # You can adjust the number of iterations\n",
    "        # Compute distances between data points and centroids\n",
    "        distances = None\n",
    "\n",
    "        # Assign each point to the closest centroid\n",
    "        cluster_indices = None\n",
    "\n",
    "        # Update centroids\n",
    "        new_centroids = []\n",
    "\n",
    "\n",
    "    return initial_centroids.tolist(), cluster_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    " با استفاده از K-Means خوشه‌های اسناد را ایجاد کنید. الگوریتم را با استفاده از چند مقدار مختلف تعداد خوشه‌ها (k) اجرا کنید. در هربار اجرا، با استفاده از تعدادی از اسناد موجود در هر خوشه، موضوع آن خوشه را تعیین کرده و خوشه‌بندی حاصله را با استفاده از بردار‌های دوبعدی قسمت قبل، رسم کنید. با اینکار، پیاده‌سازی خود و همچنین کارایی این الگوریتم در خوشه‌بندی اسناد و قرار دادن اسناد مشابه در خوشه‌های یکسان را بررسی کنید.\n",
    "<br>\n",
    " نمودار silhouette score برای مقدار‌های مختلف k را رسم کرده و silhouette analysis برای انتخاب k مناسب انجام دهید. \n",
    " همچنین با استفاده از داده‌های دارای برچسب، مقدار purity به ازای k را رسم کرده و مقدار purity برای k نهایی را گزارش کنید.\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the purity score for the given cluster assignments and ground truth classes\n",
    "\n",
    "    y_true: list\n",
    "        ground truth labels for each document\n",
    "\n",
    "    y_pred: list\n",
    "        predicted labels for each document\n",
    "\n",
    "    Returns a purity score between 0.0 and 1.0 (higher is better)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate silhouette score and purity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot silhouette score for different value of k (at least 5 different k values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot purity for different value of k (at least 5 different k values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "با استفاده از نمودارهای رسم شده توضیح دهید بهترین k برای انتخاب در داده ما با استفاده از الگوریتم K-Means چیست؟\n",
    "چرا؟\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">#TODO: Write your answer in here.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h3>3-2-2. \n",
    "Hierarchical clustering\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "تکنیک خوشه‌بندی سلسله مراتبی یکی از تکنیک‌های خوشه‌بندی در یادگیری ماشین است. در این قسمت شما می‌توانید از لایببری scipy یا هر لایبرری دیگری در پایتون استفاده کنید تا داده‌ها را به صورت سلسله‌مراتبی خوشه‌بندی کنید. سپس می‌توانید خوشه‌ها را با matplotlib مشاهده کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform hierarchical clustering on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot dendrogram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
