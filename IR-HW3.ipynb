{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر بیگی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>تمرین سوم</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۴ دی <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">\n",
    "<div dir='rtl'>\n",
    "\n",
    "<b> نام و نام‌‌خانوادگی: </b>\n",
    "<!-- YOUR NAME HERE -->\n",
    "\n",
    "<b> شماره دانشجویی: </b>\n",
    "<!-- YOUR STUDENT ID HERE -->\n",
    "\n",
    "<b> لینک colab: </b>\n",
    "\n",
    "<!-- UPLOAD YOUR NOTEBOOK TO GOOGLE COLAB AND MAKE SURE TO RUN ALL OF ITS CELLS -->\n",
    "</div>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1> \n",
    "مقدمه\n",
    "</h1>\n",
    "<p>\n",
    "در این تمرین قصد داریم به مباحث زیر بپردازیم:\n",
    "<li> embedding </li>\n",
    "<li> کلاسه‌بندی متن </li>\n",
    "<li> کاهش ابعاد </li>\n",
    "<li> خوشه‌بندی متن و هرس‌کردن خوشه‌ها</li>\n",
    "\n",
    "دیتاست این تمرین از دیتاست‌های kaggle انتخاب شده‌است و لینک آن در بخش اول تمرین در اختیار شما قرار داده شده است.\n",
    "\n",
    "کتابخانه‌های مورد نظرتان را هم می‌توانید در اولین سل نوت‌بوک فراخوانی کنید. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import fasttext\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>1.\n",
    "دریافت و آماده‌سازی دیتاست\n",
    "</h1>\n",
    "<p>\n",
    "دیتاست استفاده شده در این تمرین، مجموعه‌ای عناوین، خلاصه، و ژانر چندین کتاب است. \n",
    "این دیتاست در kaggle موجود است. \n",
    "ابتدا این دیتاست را با استفاده از kaggle api دریافت کنید و سپس آن را لود کنید.\n",
    "<br/>\n",
    "لینک دیتاست: https://www.kaggle.com/datasets/athu1105/book-genre-prediction\n",
    "<br>\n",
    "<i>در صورتی که با خطای 443 مواجه شدید، یا از پراکسی استفاده کنید یا از کولب.</i>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download Dataset (https://www.kaggle.com/datasets/athu1105/book-genre-prediction) Using Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load Dataset into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform data cleaning and eda (you can add cells here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<p>\n",
    "حال در این مرحله، به پیش‌پردازش متن می‌پردازیم. این پیش‌پردازش باید روی هر دو ستون title و summary اعمال شود.\n",
    "برای پیش‌پردازش نیازی نیست که هرکدام از اعمال پیش‌پردازش را خودتان مانند تمارین قبل پیاده کنید. برای پیاده‌سازی تابع زیر می‌توانید از کتابخانه‌های معمول برای این کار بهره ببرید.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, minimum_length=1, stopword_removal=True, stopwords_domain=[], lower_case=True,\n",
    "                       punctuation_removal=True):\n",
    "    \"\"\"\n",
    "    preprocess text by removing stopwords, punctuations, and converting to lowercase, and also filter based on a min length\n",
    "    for stopwords use nltk.corpus.stopwords.words('english')\n",
    "    for punctuations use string.punctuation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        text to be preprocessed\n",
    "    minimum_length: int\n",
    "        minimum length of the token\n",
    "    stopword_removal: bool\n",
    "        whether to remove stopwords\n",
    "    stopwords_domain: list\n",
    "        list of stopwords to be removed base on domain\n",
    "    lower_case: bool\n",
    "        whether to convert to lowercase\n",
    "    punctuation_removal: bool\n",
    "        whether to remove punctuations\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Tokenize text\n",
    "    \n",
    "    # TODO: Preprocess tokenized text\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply preprocess text on both title and summary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در نهایت بعد از اپلای کردن پیش‌پردازش روی هر دو ستون، دو ستون پیش‌پردازش شده را با هم ادغام کنید و آن را در یک آرایه به نام X قرار دهید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Merge preprocessed columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>1-1.\n",
    "Embedding\n",
    "</h2>\n",
    "\n",
    "در این قسمت از fasttext کمک می‌گیریم تا به یک embedding اولیه برای هر کتاب برسیم.\n",
    "    با استفاده از داده‌هایی که داریم یک مدل fasttext آموزش دهید که برای هر توکن یک امبدینگ ۱۰۰تایی بدهد.\n",
    "    در مرحله‌ی بعد میانگین وزن دار امبدینگ های fasttext\n",
    "        توکن‌های ورودی (چکیده + عنوان)\n",
    "    را بر اساس tfidif آن‌ها محاسبه کنید و به امبدینگ نهایی متن برسید.\n",
    "    <br>\n",
    "    در واقع به عبارت ساده‌تر بر اساس میانگین وزن‌دار که وزن‌های ما tfidf توکن‌ها می‌باشد به امبدینگ نهایی متن بر اساس fasttext می‌رسیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText:\n",
    "\n",
    "    def __init__(self, preprocessor=None, method='skipgram'):\n",
    "        self.method = method\n",
    "        self.model = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def train(self, texts):\n",
    "        \"\"\"\n",
    "        train the fasttext model and save it into self.model\n",
    "        Parameters\n",
    "        ----------\n",
    "        texts: list of list of str\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # TODO\n",
    "\n",
    "    def get_query_embedding(self, query, tf_idf_vectorizer):\n",
    "        \"\"\"\n",
    "        get the embedding of the query. You can use the tf_idf_vectorizer to get the weights of the words in the query. preprocess the query using self.preprocessor if it is not None\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str\n",
    "        tf_idf_vectorizer: TfidfVectorizer\n",
    "        Returns embedding of the query\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # TODO\n",
    "\n",
    "    def save_FastText_model(self, path='FastText_model.bin'):\n",
    "        self.model.save_model(path)\n",
    "  \n",
    "    def load_FastText_model(self, path=\"FastText_model.bin\"):\n",
    "        self.model = fasttext.load_model(path)\n",
    "\n",
    "    def prepare(self, dataset, mode, save=False):\n",
    "        if mode == 'train':\n",
    "            self.train(dataset)\n",
    "        if mode == 'load':\n",
    "            self.load_FastText_model()\n",
    "        if save:\n",
    "            self.save_FastText_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train fasttext on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get nearest neighbors of an arbitrary word using fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این مرحله آرایه X را روی TFIDF فیت می‌کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit_vectorizer(self, data):\n",
    "        \"\"\"\n",
    "        fit the vectorizer on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: list of list of str\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit TFIDF Vectorizer on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get Embedding of an arbitrary query using your prior implemention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در نهایت، تمامی entry های درون X را به صورت امبدینگ دربیاورید و آن را در X ذخیره کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get embedding on each document in X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h2>1-2. \n",
    "آماده‌سازی داده تمرین و تست\n",
    "</h2>\n",
    "<p>\n",
    "در این بخش ابتدا می‌خواهیم تا داده‌ای که می‌خواهیم بر اساس آن کلاسه‌بندی کتاب‌ها را انجام دهیم، که همان ژانر کتاب‌ها است را انکود کنیم.\n",
    "<br>\n",
    "سپس با جداسازی داده آموزش و تست، به آموزش مدل‌های کلاسه‌بند می‌پردازیم.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode genres with values between 0 and n_classes-1 and save it in array Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into train and test (test size = 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h1>2. \n",
    "کلاسه‌بندی متن\n",
    "</h1>\n",
    "در این بخش می‌خواهیم تا با استفاده از داده‌هایی که داریم، سه مدل کلاسه‌بندی متن را آموزش دهیم و عملکرد آن‌ها را با هم مقایسه کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>2-1. \n",
    "SVM و Naive Bayes\n",
    "</h2>\n",
    "با استفاده از توابع آماده در sklearn، کلاسه‌بند SVM و Naive Bayes را روی داده آموزش تمرین دهید. سپس، ژانر داده تست را با استفاده از مدل آموزش داده شده پیش‌بینی کنید و آن را در آرایه‌های NB_prediction و SVM_prediction ذخیره کنید.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train Sklearn's SVM and Naive Bayes implementation on X_train and Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict on X_test using the trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>2-2.  \n",
    "Rocchio\n",
    "</h2>\n",
    "می‌خواهیم روش Rocchio را از\n",
    "پایه پیاده‌سازی کنیم.\n",
    "با توجه به شواهد نوشته شده هر تابع را کامل کنید و با آموزش مدل روی داده‌های train\n",
    "لیبل‌های داده‌های test\n",
    "را پیش بینی کنید و آن را در rocchio_prediction ذخیره کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocchioTextClassifier:\n",
    "    def __init__(self, preprocessor=None):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.centroids = None\n",
    "    \n",
    "    def calculate_centroids(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates the centroids of each class in the dataset. A centroid is defined as the mean vector of all the feature vectors in a class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            An array contaning the feature vectors of each sentence.\n",
    "\n",
    "        y : np.ndarray\n",
    "            An array containing the class labels for each feature vector in X.\n",
    "\n",
    "        Sets self.centeroids as a dictionary where keys are unique class labels from 'y', and values are the calculated centroids (mean vectors) for each class.\n",
    "        \"\"\"\n",
    "        self.centroids = {}\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "    def predict_label(self, x):\n",
    "        \"\"\"\n",
    "        Classifies a new instance by finding the class whose centroid is closest to the new instance's vector.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            An array of a new instance to be classified.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The predicted label for the input.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "        return label\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Classifies the array X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            An array of new instances to be classified.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The predicted label for each input.\n",
    "        \"\"\"\n",
    "        return np.array([self.predict_label(x) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your Rocchio implementation on X_train and Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict on X_test using the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>2-3. \n",
    "ارزیابی\n",
    "</h2>\n",
    "برای هر سه روش پیاده‌سازی شده، معیار‌های ارزیابی زیر را بررسی کنید.\n",
    "<br>\n",
    "f1 score, accuracy, precision, recall\n",
    "<br>\n",
    "سپس، نتایج به دست آمده را با هم در چهار نمودار مقایسه کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run evaluation metrics on the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot evaulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Draw confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "کدام مدل از همه دقیق‌تر عمل کرد؟ نتیجه‌گیری و تحلیل خود از نتایج ارزیابی را گزارش دهید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">#TODO: Write your answer in here.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h1>3. \n",
    "کاهش ابعاد و خوشه‌بندی متن\n",
    "</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>3-1. \n",
    "کاهش ابعاد\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h3>3-1-1. \n",
    "PCA\n",
    "</h3>\n",
    "یکی از روش‌های کاهش ابعاد، PCA است. با استفاده از پیاده‌سازی آن در کتابخانه sklearn، ابعاد ویژگی‌های X را کاهش دهید.\n",
    "<br>\n",
    "سپس با استفاده از explained_variance_ratio_ در الگوریتم PCA  نشان دهید که با وجود یک ترشولد 90 درصد تا چه میزان میتوان ابعاد ویژگی ها را کم تر کرد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimension(embedding, n_components):\n",
    "    \"\"\"\n",
    "    Performs dimensional reduction using PCA with n components left behind\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : List\n",
    "        A list of embeddings of documents\n",
    "    \n",
    "    n_components: int\n",
    "        Number of components to keep\n",
    "\n",
    "    Returns a list of reduced embeddings\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این قسمت می‌توانید برای شهود بهتر، نمودار رسم کنید و همچنین برای ساده‌تر شدن کار، از pipeline‌های sklearn بهره ببرید. \n",
    "<br>\n",
    "<i> انجام این کار‌ها صرفا توصیه است و اجباری نیست. </i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Findout the most reduced dimension which has 90% cutoff explained variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h3>3-1-2. \n",
    "TSNE\n",
    "</h3>\n",
    "<br>\n",
    "     برای اینکه بتوانید در مراحل بعدی، نتایج خوشه‌بندی را مشاهده کنید، در این قسمت به پیاده‌سازی تابع کاهش بعد بردارهای جاسازی با استفاده از روش T-SNE می‌پردازید.\n",
    "برای اینکار تابع convert_to_2d_tsne را پیاده‌سازی می‌کنید که لیستی از بردارهای جاسازی را به عنوان ورودی دریافت می‌کند و در خروجی، لیستی از بردارهای جاسازی کاهش بعد داده شده به دو بعد را تولید می‌کند. برای پیاده سازی این تابع می‌توانید از کتابخانه‌های آماده استفاده کنید.\n",
    "<br>\n",
    "توجه کنید که از بردارهای خروجی این قسمت <u>صرفا برای رسم نمودار</u> استفاده می‌کنید و تمامی مراحلی که در ادامه طی می‌کنید (به جز رسم نمودار)، باید با استفاده از بردارهای کاهش بعد داده <u>نشده</u> انجام شوند.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sklearn.manifold import TSNE\n",
    "def convert_to_2d_tsne(emb_vecs):\n",
    "    \"\"\"\n",
    "    Converts each raw embedding vector to 2d vector \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_vecs : List\n",
    "        A list of vectors\n",
    "\n",
    "    Returns a list of 2d vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Convert each input vector to 2d vector \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment the following code\n",
    "# X_2d = convert_to_2d_tsne(np.array(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>3-2. \n",
    "خوشه‌بندی\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h3>3-2-1. \n",
    "K-Means\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این قسمت، شما ابتدا الگوریتم خوشه‌بندی K-means را \n",
    "<u><b>از پایه</b></u>\n",
    " پیاده‌سازی می‌کنید.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "\n",
    "def cluster_kmeans(emb_vecs, n_clusters):\n",
    "    \"\"\"\n",
    "    Clusters input vectors using K-means method\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_vecs : List\n",
    "        A list of vectors\n",
    "    \n",
    "    n_clusters: int\n",
    "        Number of clusters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Two lists: 1) A list containing cluster centers 2) A list containing cluster index for each input vector\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement K-means method from scratch\n",
    "    # You can't use sklearn.cluster.KMeans for clustering\n",
    "    # implement kmeans clustering here\n",
    "    emb_matrix = None\n",
    "\n",
    "    # Randomly initialize centroids\n",
    "    initial_centroids = None\n",
    "\n",
    "    for _ in range(None):  # You can adjust the number of iterations\n",
    "        # Compute distances between data points and centroids\n",
    "        distances = None\n",
    "\n",
    "        # Assign each point to the closest centroid\n",
    "        cluster_indices = None\n",
    "\n",
    "        # Update centroids\n",
    "        new_centroids = []\n",
    "        \n",
    "\n",
    "    return initial_centroids.tolist(), cluster_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    " با استفاده از K-Means خوشه‌های اسناد را ایجاد کنید. الگوریتم را با استفاده از چند مقدار مختلف تعداد خوشه‌ها (k) اجرا کنید. در هربار اجرا، با استفاده از تعدادی از اسناد موجود در هر خوشه، موضوع آن خوشه را تعیین کرده و خوشه‌بندی حاصله را با استفاده از بردار‌های دوبعدی قسمت قبل، رسم کنید. با اینکار، پیاده‌سازی خود و همچنین کارایی این الگوریتم در خوشه‌بندی اسناد و قرار دادن اسناد مشابه در خوشه‌های یکسان را بررسی کنید.\n",
    "<br>\n",
    " نمودار silhouette score برای مقدار‌های مختلف k را رسم کرده و silhouette analysis برای انتخاب k مناسب انجام دهید. \n",
    " همچنین با استفاده از داده‌های دارای برچسب، مقدار purity به ازای k را رسم کرده و مقدار purity برای k نهایی را گزارش کنید.\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the purity score for the given cluster assignments and ground truth classes\n",
    "    \n",
    "    y_true: list\n",
    "        ground truth labels for each document\n",
    "    \n",
    "    y_pred: list\n",
    "        predicted labels for each document\n",
    "        \n",
    "    Returns a purity score between 0.0 and 1.0 (higher is better)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate silhouette score and purity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot silhouette score for different value of k (at least 5 different k values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot purity for different value of k (at least 5 different k values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "با استفاده از نمودارهای رسم شده توضیح دهید بهترین k برای انتخاب در داده ما با استفاده از الگوریتم K-Means چیست؟\n",
    "چرا؟\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">#TODO: Write your answer in here.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<h3>3-2-2. \n",
    "Hierarchical clustering\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "تکنیک خوشه‌بندی سلسله مراتبی یکی از تکنیک‌های خوشه‌بندی در یادگیری ماشین است. در این قسمت شما می‌توانید از لایببری scipy یا هر لایبرری دیگری در پایتون استفاده کنید تا داده‌ها را به صورت سلسله‌مراتبی خوشه‌بندی کنید. سپس می‌توانید خوشه‌ها را با matplotlib مشاهده کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform hierarchical clustering on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot dendrogram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
